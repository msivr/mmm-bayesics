{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(mmm_evaluation)=\n",
    "# Model Evaluation in PyMC-Marketing\n",
    "\n",
    "This notebook demonstrates how to evaluate Marketing Mix Models using PyMC-Marketing's evaluation metrics and functions. We'll cover:\n",
    "\n",
    "1. Standard evaluation metrics (RMSE, MAE, MAPE)\n",
    "2. Normalized metrics (NRMSE, NMAE)\n",
    "3. Calculating and visualizing metric distributions and summaries of those distributions\n",
    "4. Creating evaluation plots (prior vs posterior plots)\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    root_mean_squared_error,\n",
    ")\n",
    "\n",
    "from pymc_marketing.mmm import (\n",
    "    MMM,\n",
    "    GeometricAdstock,\n",
    "    LogisticSaturation,\n",
    ")\n",
    "from pymc_marketing.mmm.evaluation import (\n",
    "    calculate_metric_distributions,\n",
    "    compute_summary_metrics,\n",
    "    summarize_metric_distributions,\n",
    ")\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 7]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed: int = sum(map(ord, \"mmm-evaluation\"))\n",
    "rng: np.random.Generator = np.random.default_rng(seed=seed)\n",
    "hdi_prob: float = 0.89  # change this to whatever HDI you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Demo Model\n",
    "\n",
    "Let's first create a simple MMM model using the example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example data\n",
    "data_url = \"https://raw.githubusercontent.com/pymc-labs/pymc-marketing/main/data/mmm_example.csv\"\n",
    "data = pd.read_csv(data_url, parse_dates=[\"date_week\"])\n",
    "\n",
    "X = data.drop(\"y\", axis=1)\n",
    "y = data[\"y\"]\n",
    "\n",
    "# Create and fit the model\n",
    "mmm = MMM(\n",
    "    adstock=GeometricAdstock(l_max=8),\n",
    "    saturation=LogisticSaturation(),\n",
    "    date_column=\"date_week\",\n",
    "    channel_columns=[\"x1\", \"x2\"],\n",
    "    control_columns=[\n",
    "        \"event_1\",\n",
    "        \"event_2\",\n",
    "        \"t\",\n",
    "    ],\n",
    "    yearly_seasonality=2,\n",
    ")\n",
    "\n",
    "fit_kwargs = {\n",
    "    \"tune\": 1_500,\n",
    "    \"chains\": 4,\n",
    "    \"draws\": 2_000,\n",
    "    \"nuts_sampler\": \"numpyro\",\n",
    "    \"target_accept\": 0.92,\n",
    "    \"random_seed\": rng,\n",
    "}\n",
    "_ = mmm.fit(X, y, **fit_kwargs)\n",
    "\n",
    "# Generate posterior predictive samples\n",
    "posterior_preds = mmm.sample_posterior_predictive(X, random_seed=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Evaluation Metrics\n",
    "\n",
    "PyMC-Marketing provides several metrics for evaluating your models:\n",
    "\n",
    "1. Standard metrics from scikit-learn:\n",
    "   - RMSE (Root Mean Squared Error)\n",
    "   - MAE (Mean Absolute Error)\n",
    "   - MAPE (Mean Absolute Percentage Error)\n",
    "\n",
    "2. Bayesian R-Squared (from `arviz.az.r2_score`)\n",
    "\n",
    "3. Normalized metrics:\n",
    "   - NRMSE (Normalized Root Mean Squared Error), such as is used by Robyn\n",
    "   - NMAE (Normalized Mean Absolute Error)\n",
    "\n",
    "Let's calculate these metrics for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all posterior samples\n",
    "results = compute_summary_metrics(\n",
    "    y_true=mmm.y,\n",
    "    y_pred=posterior_preds.y.to_numpy(),\n",
    "    metrics_to_calculate=[\n",
    "        \"r_squared\",\n",
    "        \"rmse\",\n",
    "        \"nrmse\",\n",
    "        \"mae\",\n",
    "        \"nmae\",\n",
    "        \"mape\",\n",
    "    ],\n",
    "    hdi_prob=hdi_prob,\n",
    ")\n",
    "\n",
    "# Print results in a formatted way\n",
    "for metric, stats in results.items():\n",
    "    print(f\"\\n{metric.upper()}:\")\n",
    "    for stat, value in stats.items():\n",
    "        print(f\"  {stat}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compute_summary_metrics` actually combines the steps of two other functions:\n",
    "\n",
    "1. `calculate_metric_distributions`\n",
    "2. `summarize_metric_distributions`\n",
    "\n",
    "The metric distributions (unsummarised) can sometimes be useful on their own, e.g. if you'd like to visualise the distribution of a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distributions for multiple metrics\n",
    "metric_distributions = calculate_metric_distributions(\n",
    "    y_true=mmm.y,\n",
    "    y_pred=posterior_preds.y.to_numpy(),\n",
    "    metrics_to_calculate=[\"rmse\", \"mae\", \"r_squared\"],\n",
    ")\n",
    "\n",
    "# Summarize the distributions\n",
    "summaries = summarize_metric_distributions(metric_distributions, hdi_prob=0.89)\n",
    "\n",
    "# Create a nice display of the summaries\n",
    "for metric, summary in summaries.items():\n",
    "    print(f\"\\n{metric.upper()} Summary:\")\n",
    "    print(f\"  Mean: {summary['mean']:.4f}\")\n",
    "    print(f\"  Median: {summary['median']:.4f}\")\n",
    "    print(f\"  Standard Deviation: {summary['std']:.4f}\")\n",
    "    print(\n",
    "        f\"  89% HDI: [{summary['89%_hdi_lower']:.4f}, {summary['89%_hdi_upper']:.4f}]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the distribution of R-squared\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "az.plot_dist(metric_distributions[\"r_squared\"], color=\"C0\", ax=ax)\n",
    "ax.axvline(\n",
    "    summaries[\"r_squared\"][\"mean\"],\n",
    "    color=\"C3\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Mean: {metric_distributions['r_squared'].mean():.4f}\",\n",
    ")\n",
    "ax.set_title(\"Distribution of R-squared across posterior samples\")\n",
    "ax.set_xlabel(\"R-squared\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Metric Distributions in Bayesian Models\n",
    "\n",
    "In Bayesian modeling, we tend to work with distributions rather than point estimates. This is particularly important for model evaluation metrics because:\n",
    "\n",
    "1. **E[f(x)] is not guaranteed to be f(E[x])**: This means calculating metrics on mean predictions can give different (and potentially misleading) results compared to calculating the distribution of metrics across posterior samples.\n",
    "\n",
    "2. **Uncertainty Quantification**: Having distributions of metrics allows us to understand the uncertainty in our model's performance.\n",
    "\n",
    "Let's demonstrate this with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong way: Calculate metrics using mean predictions\n",
    "mean_predictions = posterior_preds.y.mean(axis=1)\n",
    "naive_rmse = root_mean_squared_error(mmm.y, mean_predictions)\n",
    "\n",
    "# Correct way: Calculate distribution of metrics\n",
    "metric_distributions = calculate_metric_distributions(\n",
    "    y_true=mmm.y, y_pred=posterior_preds.y, metrics_to_calculate=[\"rmse\"]\n",
    ")\n",
    "\n",
    "proper_rmse_mean = metric_distributions[\"rmse\"].mean()\n",
    "\n",
    "print(f\"RMSE calculated on mean predictions: {naive_rmse:.4f}\")\n",
    "print(f\"Mean of RMSE distribution: {proper_rmse_mean:.4f}\")\n",
    "\n",
    "# Visualize the RMSE distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "az.plot_dist(metric_distributions[\"rmse\"], color=\"C0\", ax=ax)\n",
    "ax.axvline(naive_rmse, color=\"C3\", linestyle=\"--\", label=\"Metric on mean predictions\")\n",
    "ax.axvline(\n",
    "    proper_rmse_mean, color=\"C2\", linestyle=\"--\", label=\"Mean of metric distribution\"\n",
    ")\n",
    "ax.set_title(\"Distribution of RMSE across posterior samples\")\n",
    "ax.set_xlim(0, 500)\n",
    "ax.set_xlabel(\"RMSE\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Prior vs Posterior Distributions\n",
    "\n",
    "We can also visualize how our prior beliefs compare to the posterior distributions using the `plot_prior_vs_posterior` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, sample from the prior\n",
    "prior_preds = mmm.sample_prior_predictive(X, random_seed=rng)\n",
    "\n",
    "# Plot prior vs posterior for adstock parameter\n",
    "fig = mmm.plot_prior_vs_posterior(\n",
    "    var_name=\"adstock_alpha\",\n",
    "    alphabetical_sort=True,  # Sort channels alphabetically\n",
    ")\n",
    "\n",
    "# Plot prior vs posterior for saturation parameter\n",
    "fig = mmm.plot_prior_vs_posterior(\n",
    "    var_name=\"saturation_beta\",\n",
    "    alphabetical_sort=False,  # Sort by difference between prior and posterior means\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These visualizations help us understand:\n",
    "\n",
    "1. How much we learned from the data (difference between prior and posterior)\n",
    "2. The uncertainty in our parameter estimates (width of the distributions)\n",
    "3. Whether our priors were reasonable (by comparing prior and posterior ranges)\n",
    "\n",
    "The `plot_prior_vs_posterior` method allows us to sort channels either alphabetically or by the magnitude of change from prior to posterior, helping identify which channels had the strongest updates from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Calculate various evaluation metrics for your MMM including normalized versions (NRMSE, NMAE), as both summaries and distributions\n",
    "2. Visualize metric distributions for a chosen evaluation metric\n",
    "3. Compare prior vs posterior distributions for different metrics\n",
    "\n",
    "These tools help us understand model performance and uncertainty in our predictions, which is crucial for making informed marketing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -p pymc_marketing,pytensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
