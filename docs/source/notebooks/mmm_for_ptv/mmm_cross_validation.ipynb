{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7fc7062-d3ee-4c23-9c4f-883d26695887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "(mmm_time_slice_cross_validation)=\n",
    "# Time-Slice-Cross-Validation and Parameter Stability\n",
    "\n",
    "Perform time-slice cross validation for a media mix model. This is an important step to evaluate the stability and quality of the model. We not only look into out of sample predictions but also the stability of the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f4c276e-5308-4170-941c-80337fe276ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a9ced15-c5f8-49a8-87a7-796877ceef68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import numpyro\n",
    "import pprint\n",
    "from itertools import combinations\n",
    "import matplotlib.dates as mdates\n",
    "import ray\n",
    "import time\n",
    "import mlflow\n",
    "import math\n",
    "from scipy.stats import beta\n",
    "import pymc as pm\n",
    "\n",
    "\n",
    "from pymc_marketing.metrics import crps\n",
    "from pymc_marketing.mmm import (\n",
    "    MMM,\n",
    "    GeometricAdstock,\n",
    "    LogisticSaturation,\n",
    ")\n",
    "from pymc_marketing.mmm.utils import apply_sklearn_transformer_across_dim\n",
    "from pymc_marketing.prior import Prior\n",
    "\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 7]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "# dropdown widget\n",
    "dbutils.widgets.dropdown(\"quality_type\", \"QDAU\", [\"QDAU\", \"DAU\"])\n",
    "dbutils.widgets.dropdown(\"audience\", \"new_winback\", [\"new_winback\", \"returning\"])\n",
    "\n",
    "# databricks job parameters\n",
    "dbutils.widgets.dropdown(\"intercept_prior\", \"dist=TruncatedNormal_mu=0.4_sigma=0.05_lower=0.3_upper=0.6\", [\"dist=TruncatedNormal_mu=0.4_sigma=0.05_lower=0.3_upper=0.6\", \"dist=TruncatedNormal_mu=0.45_sigma=0.05_lower=0.3_upper=0.6\", \"dist=TruncatedNormal_mu=0.5_sigma=0.05_lower=0.4_upper=0.7\", \"dist=TruncatedNormal_mu=0.2_sigma=0.1_lower=0.1\",\"dist=TruncatedNormal_mu=0.3_sigma=0.1_lower=0.1\", \"dist=TruncatedNormal_mu=0.4_sigma=0.1_lower=0.1\", \"dist=TruncatedNormal_mu=0.5_sigma=0.1_lower=0.1\", \"dist=HalfNormal_sigma=1\"])\n",
    "dbutils.widgets.dropdown(\"adstock_alpha_prior\", 'uniform', ['spend_imps_ratio', 'contrib_imps_ratio', 'short_long_memory', 'uniform'])\n",
    "dbutils.widgets.dropdown(\"gamma_control_prior\", \"dist=Normal_mu=0_sigma=.05_dims=control\", [\"dist=Normal_mu=0_sigma=.025_dims=control\", \"dist=Normal_mu=0_sigma=.03_dims=control\", \"dist=Normal_mu=0_sigma=.05_dims=control\", \"dist=Normal_mu=0_sigma=.1_dims=control\"])\n",
    "dbutils.widgets.dropdown(\"saturation_beta_prior_sigma\", \"prior_imps_share_sigma\", [\"prior_hybrid_share_sigma\", \"prior_imps_share_sigma\", \"prior_spend_share_sigma\", \"prior_contribution_share_sigma\", \"default\", \"manual\"])\n",
    "\n",
    "dbutils.widgets.text(\"sample_tuneups\", \"750\")\n",
    "tuneups = int(dbutils.widgets.get(\"sample_tuneups\"))\n",
    "\n",
    "dbutils.widgets.text(\"sample_draws\", \"750\")\n",
    "draws = int(dbutils.widgets.get(\"sample_draws\"))\n",
    "\n",
    "dbutils.widgets.dropdown(\"trend_type\", \"annual\", [\"month\", \"annual\"])\n",
    "trend_type = (dbutils.widgets.get(\"trend_type\"))\n",
    "\n",
    "dbutils.widgets.text(\"macs_app_saturation_prior\", \"\")\n",
    "\n",
    "macs_app_saturation_prior = dbutils.widgets.get(\"macs_app_saturation_prior\")\n",
    "intercept_prior = dbutils.widgets.get(\"intercept_prior\")\n",
    "adstock_alpha_prior = dbutils.widgets.get(\"adstock_alpha_prior\")\n",
    "gamma_control_prior = dbutils.widgets.get(\"gamma_control_prior\")\n",
    "saturation_beta_prior_sigma = dbutils.widgets.get(\"saturation_beta_prior_sigma\")\n",
    "audience = dbutils.widgets.get(\"audience\")\n",
    "quality_type = dbutils.widgets.get(\"quality_type\")\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c061f92-8717-4026-a24c-bc44c673cb52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Initiate mlflow logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06280bc0-b1a8-412f-80a6-85c65329318c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"quality_type\", quality_type)\n",
    "mlflow.log_param(\"audience\", audience)\n",
    "mlflow.log_param(\"intercept_prior\", intercept_prior)\n",
    "mlflow.log_param(\"adstock_alpha_prior\", adstock_alpha_prior)\n",
    "mlflow.log_param(\"gamma_control_prior\", gamma_control_prior)\n",
    "mlflow.log_param(\"saturation_beta_prior_sigma\", saturation_beta_prior_sigma)\n",
    "mlflow.log_param(\"trend_type\", trend_type)\n",
    "mlflow.log_param(\"sample_tuneups\", tuneups)\n",
    "mlflow.log_param(\"sample_draws\", draws)\n",
    "\n",
    "\n",
    "run = mlflow.active_run()\n",
    "if run:\n",
    "    print(\"Active run name:\", run.info.run_name)\n",
    "else:\n",
    "    print(\"⚠️ No active MLflow run.\")\n",
    "\n",
    "# Add a description of the experiment\n",
    "description = \"Short vs. long adstock memory and short saturation for macs app\"\n",
    "mlflow.set_tag(\"mlflow.note.content\", description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da666e92-db7a-4779-b530-6f3751c11d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "From the widgets or job parameters, we can customize the hyperpriors space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c4dfe75-57d6-43b4-8b50-4a1c94bd70a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_prior_config_string(config_str: str) -> Prior:\n",
    "    \"\"\"\n",
    "    Parse a string like 'dist=Normal_mu=0.5_sigma=0.1' into a Prior object.\n",
    "    Supports flat (non-nested) prior definitions.\n",
    "    \"\"\"\n",
    "    if not config_str:\n",
    "        return \"\"\n",
    "\n",
    "    parts = config_str.strip().split(\"_\")\n",
    "    config = {}\n",
    "\n",
    "    for part in parts:\n",
    "        if \"=\" not in part:\n",
    "            raise ValueError(f\"Invalid part '{part}': expected key=value format\")\n",
    "        key, value = part.split(\"=\", 1)\n",
    "        config[key] = value\n",
    "\n",
    "    if \"dist\" not in config:\n",
    "        raise ValueError(\"Missing 'dist' key in prior definition.\")\n",
    "\n",
    "    dist_name = config.pop(\"dist\")\n",
    "    # Convert numeric strings to float\n",
    "    kwargs = {\n",
    "        k: float(v) if v.replace(\".\", \"\", 1).isdigit() or \"e\" in v.lower() else v\n",
    "        for k, v in config.items()\n",
    "    }\n",
    "\n",
    "    return Prior(dist_name, **kwargs)\n",
    "\n",
    "\n",
    "model_config_param = {\n",
    "    \"intercept\": parse_prior_config_string(intercept_prior),\n",
    "    \"gamma_control\": parse_prior_config_string(gamma_control_prior),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0957e8a1-9109-4f4c-89e1-0e50d180227e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "seed: int = sum(map(ord, \"mmm\"))\n",
    "rng: np.random.Generator = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fc7b0a6-78cb-43f7-9ce9-00066168c128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db2922b1-5d14-455d-b21d-37a0208d7e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Read data of daily active users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26b4d2de-6477-42c2-9c7f-62dfaab0b03d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import daily active users\n",
    "dau_df = spark.sql(\"SELECT * FROM datascience.shua.mmm_dau\").toPandas()\n",
    "\n",
    "# Lowercase all column names\n",
    "dau_df.columns = [col.lower() for col in dau_df.columns]\n",
    "\n",
    "# Replace 'winback' with 'new' in the start_type column\n",
    "dau_df[\"start_type\"] = dau_df[\"start_type\"].replace(\"WINBACK\", \"NEW\")\n",
    "\n",
    "# Group by date, start_type, and quality_flag, summing daily_users\n",
    "dau_df = (\n",
    "    dau_df.groupby([\"status_date\", \"start_type\"], as_index=False)[\"daily_users\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"status_date\": \"day_dt\"})\n",
    ")\n",
    "\n",
    "# pivot by start_type\n",
    "dau_df = pd.pivot_table(dau_df, index=\"day_dt\", columns=\"start_type\", values=\"daily_users\").reset_index()\n",
    "\n",
    "# rename columns\n",
    "dau_df.columns = [\"day_dt\", \"y_new_winback_dau\", \"y_returning_dau\"]\n",
    "\n",
    "dau_df.columns = [col.lower() for col in dau_df.columns]\n",
    "dau_df['day_dt'] = pd.to_datetime(dau_df[\"day_dt\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "783d5cd0-9bcf-4592-8f1b-79aef6ca4b42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Read mmm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ffebca-90ae-429a-8eff-1d6dc49b0ca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = spark.sql(\"select * from datascience.shua.mmm_data_wide_042425\").toPandas()\n",
    "raw_df[\"day_dt\"] = pd.to_datetime(raw_df[\"day_dt\"], errors=\"coerce\")\n",
    "# join with dau_df\n",
    "raw_df = pd.merge(\n",
    "    raw_df,\n",
    "    dau_df,\n",
    "    on=\"day_dt\",\n",
    "    how=\"left\",\n",
    ")\n",
    "# rename a column\n",
    "raw_df = raw_df.rename(columns={\"y_new_winback\": \"y_new_winback_qdau\", \"y_returning\": \"y_returning_qdau\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21eb98a7-05e9-4a18-ab26-f80ca7051384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define important events/anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9296237f-4331-4d13-a147-03c97204eaa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Map the event dictionary of notable events\n",
    "# look for holidays that start on weekends and could have a holiday effect on the following Monday\n",
    "events_dict = {\n",
    "    \"2023-01-01\": \"ny23_2023\",\n",
    "    \"2023-01-16\": \"mlk_2023\",\n",
    "    \"2023-02-12\": \"superbowl_2023\",\n",
    "    \"2023-02-13\": \"day_after_superbowl_2023\",\n",
    "    \"2023-02-20\": \"pres_2023\",\n",
    "    \"2023-03-24\": \"anomaly_3_24_23\",  # roku stitcher bug start\n",
    "    \"2023-03-25\": \"anomaly_3_25_23\",\n",
    "    \"2023-03-26\": \"anomaly_3_26_23\",\n",
    "    \"2023-03-27\": \"anomaly_3_27_23\",\n",
    "    \"2023-03-28\": \"anomaly_3_28_23\",  # roku stitcher bug end\n",
    "    \"2023-05-29\": \"mem_2023\",\n",
    "    \"2023-06-19\": \"juneteenth_2023\",\n",
    "    \"2023-07-04\": \"ind_2023\",\n",
    "    \"2023-09-04\": \"lab_2023\",\n",
    "    \"2023-11-23\": \"thanks_2023\",\n",
    "    \"2023-11-24\": \"blk_fri_2023\",\n",
    "    \"2023-12-17\": \"anomaly_12_17_23\",  # bug\n",
    "    \"2023-12-20\": \"anomaly_12_20_2023\",  # bug\n",
    "    \"2023-12-21\": \"anomaly_12_21_2023\",  # bug\n",
    "    \"2023-12-23\": \"day_b4_xmas_eve_2023\",\n",
    "    \"2023-12-24\": \"xmas_eve_2023\",\n",
    "    \"2023-12-25\": \"xmas_2023\",\n",
    "    \"2023-12-26\": \"xmas_day_after_2023\",\n",
    "    \"2023-12-30\": \"day_b4_nye_2023\",\n",
    "    \"2023-12-31\": \"nye_2023\",\n",
    "    \"2024-01-01\": \"ny24_2024\",\n",
    "    \"2024-01-15\": \"mlk_2024\",\n",
    "    \"2024-02-11\": \"anomaly_2_11_2024\",  # bug\n",
    "    \"2024-02-19\": \"pres_2024\",\n",
    "    \"2024-03-12\": \"anomaly_3_11_2024\",  # bug\n",
    "    \"2024-03-12\": \"anomaly_3_12_2024\",  # bug\n",
    "    \"2024-03-13\": \"anomaly_3_13_2024\",  # bug\n",
    "    \"2024-03-14\": \"anomaly_3_14_2024\",  # bug\n",
    "    \"2024-03-15\": \"anomaly_3_15_2024\",  # bug\n",
    "    \"2024-05-27\": \"mem_2024\",\n",
    "    \"2024-06-19\": \"juneteenth_2024\",\n",
    "    \"2024-07-04\": \"ind_2024\",\n",
    "    \"2024-09-02\": \"lab_2024\",\n",
    "    \"2024-11-28\": \"thanks_2024\",\n",
    "    \"2024-11-29\": \"blk_fri_2024\",\n",
    "    \"2024-12-24\": \"xmas_eve_2024\",\n",
    "    \"2024-12-25\": \"xmas_2024\",\n",
    "    \"2024-12-31\": \"nye_2024\",\n",
    "    \"2024-02-11\": \"superbowl_2024\",\n",
    "    \"2024-02-12\": \"day_after_superbowl_2024\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Map the event names to the 'event' column\n",
    "raw_df['event'] = pd.to_datetime(raw_df['day_dt'], errors='coerce').dt.strftime('%Y-%m-%d').map(events_dict)\n",
    "\n",
    "# One-hot encode events\n",
    "event_dummies = pd.get_dummies(raw_df[\"event\"], prefix=\"event_\", prefix_sep=\"\")\n",
    "event_dummies = (event_dummies >= 1).astype(int)\n",
    "daily_df = pd.concat([raw_df, event_dummies], axis=1).sort_values(\"day_dt\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# One-hot encode day of week\n",
    "dow_dummies = pd.get_dummies(daily_df[\"day_dt\"].dt.dayofweek, prefix=\"dow_\", prefix_sep=\"\")\n",
    "dow_dummies = (dow_dummies >= 1).astype(int)\n",
    "daily_df = pd.concat([daily_df, dow_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "518cbbca-cbb1-4771-844a-e08d3249ea73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Plot of y_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f5d449c-88d8-4d6d-be0a-c2915996e4a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(20, 10), sharex=True)\n",
    "\n",
    "# Define variables to plot\n",
    "metrics = [(\"y_new_winback_dau\", \"New Winback Daily\"), (\"y_new_winback_qdau\", \"New Winback Quality Daily\"), (\"y_returning_dau\", \"Returning Daily\"), (\"y_returning_qdau\", \"Returning Quality Daily\")]\n",
    "\n",
    "for ax, (col, title) in zip(axes, metrics):\n",
    "    sns.lineplot(data=daily_df, x=\"day_dt\", y=col, color=\"black\", ax=ax)\n",
    "    event_days = daily_df[pd.notna(daily_df[\"event\"])]\n",
    "    ax.plot(event_days[\"day_dt\"], event_days[col], \"ro\", markersize=6, label=\"Event\")\n",
    "    ax.set(title=title, xlabel=\"Month\", ylabel=col)\n",
    "    ax.legend()\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc3cc2ce-2787-46d4-8323-f07f7ba4f896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Add trend control vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32e67dc4-68c6-4dfc-b2af-d6b0cda45190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add annual trend\n",
    "\n",
    "# Extract year\n",
    "daily_df[\"year\"] = daily_df[\"day_dt\"].dt.year\n",
    "\n",
    "# Use groupby cumcount for per-year linear trend\n",
    "daily_df[\"annual_trend_2023\"] = daily_df.groupby(\"year\").cumcount().where(daily_df[\"year\"] == 2023, 0)\n",
    "daily_df[\"annual_trend_2024\"] = daily_df.groupby(\"year\").cumcount().where(daily_df[\"year\"] == 2024, 0)\n",
    "\n",
    "# segmented monthly trend\n",
    "daily_df[\"month\"] = daily_df[\"day_dt\"].dt.to_period(\"M\")\n",
    "\n",
    "# Normalize day of month to [0, 1] scale per month\n",
    "daily_df[\"day_frac\"] = (\n",
    "    daily_df.groupby(\"month\")[\"day_of_month\"]\n",
    "    .transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    ")\n",
    "\n",
    "# One-hot encode month\n",
    "month_dummies = pd.get_dummies(daily_df[\"month\"].astype(str), prefix=\"month\", drop_first=False)\n",
    "\n",
    "# Multiply each dummy by normalized day_frac\n",
    "for col in month_dummies.columns:\n",
    "    daily_df[f\"trend_{col}\"] = month_dummies[col] * daily_df[\"day_frac\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3efa850-d994-4951-8fb6-9f3f5e694db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Specify Variables in the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c84cd69-8bb8-45cc-9307-3ad67580584b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. control variables\n",
    "# We just keep the holidays columns\n",
    "control_columns = [col for col in daily_df.columns if col.startswith(\"event_\") or col.startswith(\"dow_\") or col.startswith(f\"trend_{trend_type}\")]\n",
    "print(\"Control columns:\", control_columns)\n",
    "\n",
    "# 2. media variables\n",
    "# Impressions only for now\n",
    "media_activity = \"imps\" # must be \"imps\" or \"spend\"\n",
    "channel_columns_raw = sorted(\n",
    "    [\n",
    "        col\n",
    "        for col in daily_df.columns\n",
    "        if (\"channel_\" in col) & (f\"_{media_activity}\" in col)\n",
    "    ]\n",
    ")\n",
    "\n",
    "imps_channel_mapping = {\n",
    "    \"channel_app_imps\": \"MACS App\",\n",
    "    \"channel_hmi_audio_imps\": \"HMI Audio\",\n",
    "    \"channel_hmi_display_imps\": \"HMI Display\",\n",
    "    \"channel_hmi_linear_imps\": \"HMI Linear\",\n",
    "    \"channel_hmi_ooh_imps\": \"HMI OOH\",\n",
    "    \"channel_hmi_video_imps\": \"HMI Digital Video\",\n",
    "    \"channel_influencer_imps\": \"Influencer\",\n",
    "    \"channel_macs_programmatic_imps\": \"MACS Programmatic\",\n",
    "    \"channel_macs_social_imps\": \"MACS Social\",\n",
    "    \"channel_oo_display_imps\": \"OO Display\",\n",
    "    \"channel_oo_linear_imps\": \"OO Linear\",\n",
    "    \"channel_oo_video_imps\": \"OO Video\",\n",
    "    \"channel_paid_search_imps\": \"Paid Search\",\n",
    "    \"channel_partnerships_imps\": \"Partnerships\",\n",
    "    \"channel_podcast_one_imps\": \"Podcast One Audio\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "spend_channel_mapping = {\n",
    "    \"channel_app_spend\": \"MACS App\",\n",
    "    \"channel_hmi_audio_spend\": \"HMI Audio\",\n",
    "    \"channel_hmi_display_spend\": \"HMI Display\",\n",
    "    \"channel_hmi_linear_spend\": \"HMI Linear\",\n",
    "    \"channel_hmi_ooh_spend\": \"HMI OOH\",\n",
    "    \"channel_hmi_video_spend\": \"HMI Digital Video\",\n",
    "    \"channel_influencer_spend\": \"Influencer\",\n",
    "    \"channel_macs_programmatic_spend\": \"MACS Programmatic\",\n",
    "    \"channel_macs_social_spend\": \"MACS Social\",\n",
    "    \"channel_oo_display_spend\": \"OO Display\",\n",
    "    \"channel_oo_linear_spend\": \"OO Linear\",\n",
    "    \"channel_oo_video_spend\": \"OO Video\",\n",
    "    \"channel_paid_search_spend\": \"Paid Search\",\n",
    "    \"channel_partnerships_spend\": \"Partnerships\",\n",
    "    \"channel_podcast_one_spend\": \"Podcast One Audio\",\n",
    "}\n",
    "\n",
    "if media_activity == \"imps\":\n",
    "    channel_mapping = imps_channel_mapping\n",
    "    channel_columns = sorted(list(imps_channel_mapping.keys()))\n",
    "    \n",
    "else:\n",
    "    channel_mapping = spend_channel_mapping\n",
    "    channel_columns = sorted(list(spend_channel_mapping.keys()))\n",
    "    \n",
    "channel_names = sorted(list(imps_channel_mapping.values()))\n",
    "sorted_spend_channel_names =  {k: spend_channel_mapping[k] for k in sorted(spend_channel_mapping, key=spend_channel_mapping.get)}\n",
    "\n",
    "print(\"Channel columns:\", channel_names)\n",
    "print(\"Spend columns:\", list(sorted_spend_channel_names.keys()))\n",
    "\n",
    "\n",
    "\n",
    "# 3. y variable\n",
    "y_var = f\"y_{audience}_{quality_type}\".lower()\n",
    "print(\"y_var:\", y_var)\n",
    "\n",
    "data_df = daily_df[[\"day_dt\", y_var, *channel_columns, *control_columns, *sorted_spend_channel_names]]\n",
    "data_df = data_df.rename(columns=channel_mapping)\n",
    "\n",
    "# reorder the channel columns in data_df only\n",
    "sorted_channel_columns = sorted(channel_mapping.values())\n",
    "data_df = data_df[['day_dt', y_var, *sorted_channel_columns, *control_columns, *sorted_spend_channel_names]]\n",
    "\n",
    "# 4. Date column\n",
    "data_df[\"day_dt\"] = pd.to_datetime(data_df[\"day_dt\"])\n",
    "date_column = \"day_dt\"\n",
    "data_df.columns\n",
    "\n",
    "# remove event_0 column\n",
    "if \"event_0\" in data_df.columns:\n",
    "    data_df = data_df.drop(columns=[\"event_0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06a4db6a-c20f-42f1-8211-a174177bc389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Specify Time-Slice-Cross-Validation Strategy\n",
    "\n",
    "The main idea of the time-slice cross validation process is to fit the model on a time slice of the data and then evaluate it on the next time slice. We repeat this process for each time slice of the data. As we want to simulate a production-like environment where we enlarge our training data over time, we make the time-slice size grow over time.\n",
    "\n",
    "Following the strategy of the example notebook, we use the media share of each channel to set the prior standard deviation of the beta parameters. We need to compute this share for EACH training time slice independently.\n",
    "\n",
    "```{admonition} Data Leakage\n",
    ":class: warning\n",
    "\n",
    "It is very important to avoid data leakage when performing time-slice cross validation. This means that the model should not see any training data from the future. This also includes any data pre-processing steps!\n",
    "\n",
    "For example, as mentioned above, we need to compute the media share for each training time slice independently if we want to avoid data leakage. Other sources of data leakage include using a global feature for thr trend component. In our case, we simply use an increasing variable `t` so we are safe as we just increase it by one for each time slice.\n",
    "```\n",
    "\n",
    "We wrap the main steps of the training procedure in a set of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c259dc-db77-44ca-bb37-2de55c62af8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get shared channel names, ensuring consistent ordering\n",
    "channels = sorted(channel_names)\n",
    "\n",
    "# Get imps_shares and spend_shares\n",
    "imps_shares = (\n",
    "    data_df.melt(value_vars=channels, var_name=\"channel\", value_name=\"imps\")\n",
    "    .groupby(\"channel\", as_index=False)\n",
    "    .agg({\"imps\": \"sum\"})\n",
    "    .sort_values(by=\"channel\")\n",
    "    .assign(imps_share=lambda x: x[\"imps\"] / x[\"imps\"].sum())[\"imps_share\"]\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "spend_shares = (\n",
    "    daily_df.melt(value_vars=list(spend_channel_mapping.keys()), var_name=\"channel\", value_name=\"spend\")\n",
    "    .groupby(\"channel\", as_index=False)\n",
    "    .agg({\"spend\": \"sum\"})\n",
    "    .sort_values(by=\"channel\")\n",
    "    .assign(spend_share=lambda x: x[\"spend\"] / x[\"spend\"].sum())[\"spend_share\"]\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "# Bar plot settings\n",
    "x = np.arange(len(channels))  # label locations\n",
    "width = 0.4  # bar width\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width / 2, imps_shares, width, label=\"Imps Share\")\n",
    "plt.bar(x + width / 2, spend_shares, width, label=\"Spend Share\")\n",
    "\n",
    "# Add labels\n",
    "for i in range(len(channels)):\n",
    "    plt.text(x[i] - width / 2, imps_shares[i] + 0.01, str(round(imps_shares[i], 2)), ha=\"center\", va=\"bottom\", size=10)\n",
    "    plt.text(x[i] + width / 2, spend_shares[i] + 0.01, str(round(spend_shares[i], 2)), ha=\"center\", va=\"bottom\", size=10)\n",
    "\n",
    "plt.xticks(x, channels, rotation=45)\n",
    "plt.ylabel(\"Share\")\n",
    "plt.title(\"Impressions vs Spend Share by Channel\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df5fcc40-aeb9-4390-a9c9-7eff6fd35908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBDYWxjdWxhdGUgd2VpZ2h0ZWQgc2NvcmUKbXVsdGlwbGljYXRpdmVfc2NvcmUgPSBpbXBzX3NoYXJlcyArIHNwZW5kX3NoYXJlcwpub3JtYWxpemVkX3Njb3JlID0gKG11bHRpcGxpY2F0aXZlX3Njb3JlIC8gbXVsdGlwbGljYXRpdmVfc2NvcmUuc3VtKCkpCgojIFByaW50IGFzIGEgdGFibGUKaW1wb3J0IHBhbmRhcyBhcyBwZAoKc2NvcmVfZGYgPSBwZC5EYXRhRnJhbWUoewogICAgImNoYW5uZWwiOiBjaGFubmVscywKICAgICJpbXBzX3NoYXJlIjogaW1wc19zaGFyZXMsCiAgICAic3BlbmRfc2hhcmUiOiBzcGVuZF9zaGFyZXMsCiAgICAiaHlicmlkX3Njb3JlIjogbm9ybWFsaXplZF9zY29yZSwKfSkKCmRpc3BsYXkoc2NvcmVfZGYpCgpoeWJyaWRfc2hhcmVfbGlzdCA9IFtdCgpmb3Igc2hhcmUgaW4gbGlzdChzY29yZV9kZlsnaHlicmlkX3Njb3JlJ10pOgogIGlmIHNoYXJlIDwgLjAwNToKICAgICAgc2hhcmUgPSAuMDA1CiAgaHlicmlkX3NoYXJlX2xpc3QuYXBwZW5kKHNoYXJlKQpoeWJyaWRfc2hhcmVfbGlzdA==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewd9f35d8\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewd9f35d8\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewd9f35d8\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewd9f35d8) SELECT `channel`,SUM(`hybrid_score`) `column_5630db7b1126` FROM q GROUP BY `channel`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewd9f35d8\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "channel",
             "id": "column_5630db7b1124"
            },
            "y": [
             {
              "column": "hybrid_score",
              "id": "column_5630db7b1126",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_5630db7b1126": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "378bfbee-3147-4a3b-9f46-59303ef48567",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 15.7109375,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "channel",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "channel",
           "type": "column"
          },
          {
           "alias": "column_5630db7b1126",
           "args": [
            {
             "column": "hybrid_score",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate weighted score\n",
    "multiplicative_score = imps_shares + spend_shares\n",
    "normalized_score = (multiplicative_score / multiplicative_score.sum())\n",
    "\n",
    "# Print as a table\n",
    "import pandas as pd\n",
    "\n",
    "score_df = pd.DataFrame({\n",
    "    \"channel\": channels,\n",
    "    \"imps_share\": imps_shares,\n",
    "    \"spend_share\": spend_shares,\n",
    "    \"hybrid_score\": normalized_score,\n",
    "})\n",
    "\n",
    "display(score_df)\n",
    "\n",
    "hybrid_share_list = []\n",
    "\n",
    "for share in list(score_df['hybrid_score']):\n",
    "  if share < .005:\n",
    "      share = .005\n",
    "  hybrid_share_list.append(share)\n",
    "hybrid_share_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c03b2c7-a5d6-44ff-8612-3d311af0275a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Adstock Alpha Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2282079-886f-4db5-ba8f-6f3e0c2d30e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def suggest_adstock_alpha_prior(\n",
    "    contrib_share,\n",
    "    imps_share,\n",
    "    min_alpha=1,\n",
    "    max_alpha=5,\n",
    "    efficiency_means_short_memory=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Suggests a Beta prior for adstock_alpha based on contribution/impression efficiency.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    contrib_share : float\n",
    "        Fractional contribution share of the channel (0 to 1).\n",
    "    imps_share : float\n",
    "        Fractional impression share of the channel (0 to 1).\n",
    "    min_alpha : float\n",
    "        Lower bound for Beta(alpha, beta).\n",
    "    max_alpha : float\n",
    "        Upper bound for Beta(alpha, beta).\n",
    "    efficiency_means_short_memory : bool\n",
    "        If True, high efficiency implies short memory (low alpha).\n",
    "        If False, high efficiency implies long memory (high alpha).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Prior\n",
    "        A Beta distribution for adstock_alpha.\n",
    "    float\n",
    "        The efficiency ratio used in scaling.\n",
    "    \"\"\"\n",
    "    if imps_share == 0:\n",
    "        return Prior(\"Beta\", alpha=1, beta=1), np.inf  # Uninformative\n",
    "\n",
    "    # Define effectiveness ratio\n",
    "    efficiency = contrib_share / imps_share\n",
    "    scaled = np.clip(efficiency, 0, 5)\n",
    "\n",
    "    # Smooth transformation: scale α depending on the desired interpretation\n",
    "    scaled_value = (np.tanh(scaled) + 1) / 2  # range: [0, 1]\n",
    "\n",
    "    if efficiency_means_short_memory:\n",
    "        # High efficiency → short memory → lower alpha\n",
    "        scaled_alpha = max_alpha - (max_alpha - min_alpha) * scaled_value\n",
    "    else:\n",
    "        # High efficiency → long memory → higher alpha\n",
    "        scaled_alpha = min_alpha + (max_alpha - min_alpha) * scaled_value\n",
    "\n",
    "    beta_param = max(1, max_alpha + min_alpha - scaled_alpha)\n",
    "\n",
    "    return {\"prior\": Prior(\"Beta\", alpha=round(scaled_alpha, 2), beta=round(beta_param, 2)),\n",
    "            \"alpha\": scaled_alpha,\n",
    "            \"beta\": beta_param,\n",
    "            \"ratio\": efficiency}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d553a55c-79ef-4832-938f-1460c4df4565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "returning_channel_contributions_12_1_23 = {\n",
    "                    \"HMI Audio\": 0.0075,\n",
    "                    \"HMI Digital Video\": 0.0231,\n",
    "                    \"HMI Display\": 0.0549,\n",
    "                    \"HMI Linear\": 0.0223,\n",
    "                    \"HMI OOH\": 0.0622,\n",
    "                    \"Influencer\": 0.0003,\n",
    "                    \"MACS App\": 0.0511,\n",
    "                    \"MACS Programmatic\": 0.0342,\n",
    "                    \"MACS Social\": 0.0636,\n",
    "                    \"OO Display\": 0.0043,\n",
    "                    \"OO Linear\": 0.0904,\n",
    "                    \"OO Video\": 0.0960,\n",
    "                    \"Paid Search\": 0.1401,\n",
    "                    \"Partnerships\": 0.2875,\n",
    "                    \"Podcast One Audio\": 0.0803,\n",
    "                }\n",
    "if adstock_alpha_prior == \"spend_imps_ratio\":\n",
    "    channel_data = {\n",
    "        channel: {\"contrib\": contrib, \"imps\": imps}\n",
    "        for channel, contrib, imps in zip(channels, spend_shares, imps_shares)\n",
    "    }\n",
    "    adstock_alpha_prior_dict = {\n",
    "    channel: suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"prior\"]\n",
    "    for channel, v in channel_data.items()\n",
    "    }\n",
    "    efficiency_prior_dict = {\n",
    "    channel: suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"ratio\"]\n",
    "    for channel, v in channel_data.items()\n",
    "    }\n",
    "\n",
    "    alpha_prior_list = [\n",
    "        suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"alpha\"]\n",
    "        for channel, v in channel_data.items()\n",
    "    ]\n",
    "\n",
    "    beta_prior_list = [\n",
    "        suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"beta\"]\n",
    "        for channel, v in channel_data.items()\n",
    "    ]\n",
    "    pprint.pprint(adstock_alpha_prior_dict)\n",
    "    pprint.pprint(efficiency_prior_dict)\n",
    "    pprint.pprint(alpha_prior_list)\n",
    "    pprint.pprint(beta_prior_list)\n",
    "\n",
    "elif adstock_alpha_prior == \"contrib_imps_ratio\":\n",
    "    channel_data = {\n",
    "        channel: {\"contrib\": contrib, \"imps\": imps}\n",
    "        for channel, contrib, imps in zip(channels, list(returning_channel_contributions_12_1_23.values()), imps_shares)\n",
    "    }\n",
    "    adstock_alpha_prior_dict = {\n",
    "    channel: suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"prior\"]\n",
    "    for channel, v in channel_data.items()\n",
    "    }\n",
    "    efficiency_prior_dict = {\n",
    "    channel: suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"ratio\"]\n",
    "    for channel, v in channel_data.items()\n",
    "    }\n",
    "\n",
    "    alpha_prior_list = [\n",
    "        suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"alpha\"]\n",
    "        for channel, v in channel_data.items()\n",
    "    ]\n",
    "\n",
    "    beta_prior_list = [\n",
    "        suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"beta\"]\n",
    "        for channel, v in channel_data.items()\n",
    "    ]\n",
    "    pprint.pprint(adstock_alpha_prior_dict)\n",
    "    pprint.pprint(efficiency_prior_dict)\n",
    "    pprint.pprint(alpha_prior_list)\n",
    "    pprint.pprint(beta_prior_list)\n",
    "\n",
    "elif adstock_alpha_prior == \"uniform\":\n",
    "\n",
    "    adstock_alpha_prior_dict = {\n",
    "    channel: Prior(\"Beta\", alpha=1, beta=1, dims=\"channel\")\n",
    "    for channel in channels\n",
    "    }\n",
    "\n",
    "    pprint.pprint(adstock_alpha_prior_dict)\n",
    "\n",
    "elif adstock_alpha_prior == \"short_long_memory\":\n",
    "    adstock_alpha_prior_dict = {\n",
    "    \"Paid Search\": Prior(\"Beta\", alpha=1.5, beta=8),               # Mean ≈ 0.16\n",
    "    \"HMI Display\": Prior(\"Beta\", alpha=2, beta=6),                 # Mean = 0.25\n",
    "    \"MACS App\": Prior(\"Beta\", alpha=2, beta=6),                    # Mean = 0.25\n",
    "    \"MACS Programmatic\": Prior(\"Beta\", alpha=2, beta=6),           # Mean = 0.25\n",
    "    \"OO Display\": Prior(\"Beta\", alpha=2, beta=6),                  # Mean = 0.25\n",
    "    \"Influencer\": Prior(\"Beta\", alpha=2, beta=5),                  # Mean ≈ 0.29\n",
    "    \"MACS Social\": Prior(\"Beta\", alpha=2, beta=5),                 # Mean ≈ 0.29\n",
    "    \"HMI Audio\": Prior(\"Beta\", alpha=3.5, beta=3.5),               # Mean = 0.50\n",
    "    \"HMI Digital Video\": Prior(\"Beta\", alpha=4, beta=4),           # Mean = 0.50\n",
    "    \"OO Video\": Prior(\"Beta\", alpha=4, beta=4),                    # Mean = 0.50\n",
    "    \"Podcast One Audio\": Prior(\"Beta\", alpha=4, beta=3),           # Mean ≈ 0.57\n",
    "    \"HMI OOH\": Prior(\"Beta\", alpha=5, beta=3),                     # Mean ≈ 0.625\n",
    "    \"Partnerships\": Prior(\"Beta\", alpha=5, beta=3),                # Mean ≈ 0.625\n",
    "    \"HMI Linear\": Prior(\"Beta\", alpha=6, beta=2),                  # Mean = 0.75\n",
    "    \"OO Linear\": Prior(\"Beta\", alpha=6, beta=2),                   # Mean = 0.75\n",
    "}\n",
    "    \n",
    "    alpha_prior_list = [prior.parameters[\"alpha\"] for prior in adstock_alpha_prior_dict.values()]\n",
    "    beta_prior_list = [prior.parameters[\"beta\"] for prior in adstock_alpha_prior_dict.values()]\n",
    "    print(\"Alphas:\", alpha_prior_list)\n",
    "    print(\"Betas:\", beta_prior_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628ff396-e585-4318-b91b-b8e35abb08ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_adstock_alpha_priors_facet(prior_dict, channels_to_plot=None, n_cols=4):\n",
    "    \"\"\"\n",
    "    Plots Beta distributions for adstock_alpha priors per channel using facet grid layout.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prior_dict : dict\n",
    "        Dictionary of {channel: Prior(\"Beta\", alpha=..., beta=...)}\n",
    "    channels_to_plot : list of str, optional\n",
    "        Subset of channels to visualize. If None, plots all.\n",
    "    n_cols : int\n",
    "        Number of columns in the facet grid.\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, 1, 500)\n",
    "    selected_channels = channels_to_plot or list(prior_dict.keys())\n",
    "    n_channels = len(selected_channels)\n",
    "    n_rows = math.ceil(n_channels / n_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 3 * n_rows), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, channel in enumerate(selected_channels):\n",
    "        prior = prior_dict[channel]\n",
    "        a = prior.parameters.get(\"alpha\")\n",
    "        b = prior.parameters.get(\"beta\")\n",
    "\n",
    "        y = beta.pdf(x, a, b)\n",
    "        ax = axes[i]\n",
    "        ax.plot(x, y, label=f\"α={a:.2f}, β={b:.2f}\")\n",
    "        ax.set_title(channel)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "    # Hide unused subplots if any\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"Beta Priors for Adstock α by Channel\", fontsize=16)\n",
    "    fig.supxlabel(\"adstock_alpha\")\n",
    "    fig.supylabel(\"Density\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_adstock_alpha_priors_facet(adstock_alpha_prior_dict)\n",
    "\n",
    "# Save and log\n",
    "fig.savefig(\"Adstock Alpha Prior Distribution By Channel.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"Adstock Alpha Prior Distribution By Channel.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0aa61152-08ef-48db-8a8c-43bca6254423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Saturation Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d5aaa6a-473d-4c4e-8bbe-3b99bdca33f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Shared default prior\n",
    "default_saturation = {\n",
    "    \"lam\": Prior(\"Gamma\", alpha=3, beta=1), # s curve\n",
    "    \"beta\": Prior(\"HalfNormal\", sigma=2), # scale/impact\n",
    "}\n",
    "\n",
    "# MACS App: early and fast saturation (quick decay)\n",
    "if macs_app_saturation_prior != \"\":\n",
    "    macs_app_prior = macs_app_saturation_prior\n",
    "else:\n",
    "    macs_app_prior = {\n",
    "                        \"lam\": Prior(\"Gamma\", alpha=5, beta=1),    # mean = 0.2 → steeper start\n",
    "                        \"beta\": Prior(\"HalfNormal\", sigma=.5)      # mean ≈ 4 → much steeper curve\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Build final saturation priors per channel\n",
    "saturation_priors = {\n",
    "    ch: (macs_app_prior if ch == \"MACS App\" else default_saturation)\n",
    "    for ch in channel_names\n",
    "}\n",
    "\n",
    "# Extract lists\n",
    "lam_alpha_priors = []\n",
    "lam_beta_priors = []\n",
    "beta_sigma_priors = []\n",
    "\n",
    "for ch in channel_names:\n",
    "    lam_prior = saturation_priors[ch][\"lam\"]\n",
    "    beta_prior = saturation_priors[ch][\"beta\"]\n",
    "\n",
    "    if lam_prior.distribution == \"Gamma\":\n",
    "        lam_alpha_priors.append(lam_prior.parameters[\"alpha\"])\n",
    "        lam_beta_priors.append(lam_prior.parameters[\"beta\"])\n",
    "    else:\n",
    "        lam_alpha_priors.append(None)\n",
    "        lam_beta_priors.append(None)\n",
    "\n",
    "    if beta_prior.distribution == \"HalfNormal\":\n",
    "        beta_sigma_priors.append(beta_prior.parameters[\"sigma\"])\n",
    "    else:\n",
    "        beta_sigma_priors.append(None)\n",
    "\n",
    "# Print or return the lists\n",
    "print(\"lam_alpha_priors:\", lam_alpha_priors)\n",
    "print(\"lam_beta_priors:\", lam_beta_priors)\n",
    "print(\"beta_sigma_priors:\", beta_sigma_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "920f1722-4036-492c-93f7-3a6e6f272777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pymc_marketing.mmm import LogisticSaturation\n",
    "\n",
    "def plot_logistic_saturation_for_channel(channel_name, saturation_priors, seed=0):\n",
    "    \"\"\"\n",
    "    Plot the logistic saturation curve for a specific channel using pymc-marketing's plot_curve().\n",
    "    \n",
    "    Args:\n",
    "        channel_name (str): The name of the channel to plot.\n",
    "        saturation_priors (dict): Dict mapping channel -> {\"lam\": Prior(...), \"beta\": Prior(...)}.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    if channel_name not in saturation_priors:\n",
    "        raise ValueError(f\"Channel '{channel_name}' not found in saturation_priors.\")\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    adstock = LogisticSaturation(priors=saturation_priors[channel_name])\n",
    "    prior = adstock.sample_prior(random_seed=rng)\n",
    "    curve = adstock.sample_curve(prior)\n",
    "\n",
    "    adstock.plot_curve(curve, sample_kwargs={\"rng\": rng}, subplot_kwargs={\"figsize\": (5,4)})\n",
    "    plt.title(f\"Logistic Saturation Curve: {channel_name}\", fontsize=9)\n",
    "    plt.xlabel(\"Normalized Media Input\")\n",
    "    plt.ylabel(\"Saturation Output\")\n",
    "    plt.grid(True, linewidth=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_logistic_saturation_for_channel(\"MACS App\", saturation_priors)\n",
    "plot_logistic_saturation_for_channel(\"Partnerships\", saturation_priors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6853620c-1a2d-4ad5-9d8f-b5ea8edfcc88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.log_param(\"macs_app_prior\", macs_app_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9f720be-fbe4-4458-a616-fda09479bce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15836dc3-95ee-43c5-bfeb-d9379f4cbd9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prior Specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd5078f3-e1d1-45e4-b5e0-89f728a066f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67cae917-24d9-4a6c-9d0a-72216c325eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For each fold, we \n",
    "- compute prior sigma for saturation beta based on media activity shares or the last known media contribution of training set (i.e. 12/1/2023 below).\n",
    "- filter out zero columns\n",
    "- reset the model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4050d2b-52ab-419f-9d9f-6521b8953bf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_sigma_from_media_activity(\n",
    "    X: pd.DataFrame, \n",
    "    media_activity: str = \"imps\"\n",
    ") -> list[float]:\n",
    "    \"\"\"Compute the prior standard deviation of the beta parameters from the activity share of each channel.\"\"\"\n",
    "\n",
    "    if media_activity == \"imps\":\n",
    "        media_cols = list(channel_names)\n",
    "    elif media_activity == \"spend\":\n",
    "        media_cols = list(sorted_spend_channel_names)\n",
    "\n",
    "    n_channels = len(media_cols)\n",
    "    total_activity_per_channel = data_df[media_cols].sum(axis=0)\n",
    "    activity_share = total_activity_per_channel / total_activity_per_channel.sum()\n",
    "    prior_sigma = n_channels * activity_share.to_numpy()\n",
    "    return prior_sigma.tolist()\n",
    "\n",
    "\n",
    "\n",
    "def get_mmm(X: pd.DataFrame, channel_names: list[str], control_columns: list[str], date_column: str) -> MMM:\n",
    "    \"\"\"Specify the model.\"\"\"\n",
    "\n",
    "    # specify the prior alpha,beta for adstock_alpha beta prior distribution\n",
    "    if adstock_alpha_prior in ['spend_imps_ratio', 'contrib_imps_ratio', 'short_long_memory']:\n",
    "        prior_alpha = alpha_prior_list\n",
    "        prior_beta = beta_prior_list\n",
    "    elif adstock_alpha_prior == 'uniform':\n",
    "        # uniform beta distribution\n",
    "        prior_alpha = [1 for _ in channels]\n",
    "        prior_beta = [1 for _ in channels]\n",
    "\n",
    "    # specify the prior sigma for saturation beta\n",
    "    if saturation_beta_prior_sigma == \"prior_imps_share_sigma\":\n",
    "            prior_sigma = compute_sigma_from_media_activity(data_df, media_activity=\"imps\")\n",
    "    elif saturation_beta_prior_sigma == \"prior_spend_share_sigma\":\n",
    "            prior_sigma = compute_sigma_from_media_activity(data_df, media_activity=\"spend\")\n",
    "    elif saturation_beta_prior_sigma == \"prior_hybrid_share_sigma\":\n",
    "            prior_sigma = hybrid_share_list\n",
    "    elif saturation_beta_prior_sigma == \"prior_contribution_share_sigma\":\n",
    "            if audience == 'returning':\n",
    "                returning_channel_contributions_12_1_23 = {\n",
    "                    \"HMI Audio\": 0.0075,\n",
    "                    \"HMI Digital Video\": 0.0231,\n",
    "                    \"HMI Display\": 0.0549,\n",
    "                    \"HMI Linear\": 0.0223,\n",
    "                    \"HMI OOH\": 0.0622,\n",
    "                    \"Influencer\": 0.0003,\n",
    "                    \"MACS App\": 0.0511,\n",
    "                    \"MACS Programmatic\": 0.0342,\n",
    "                    \"MACS Social\": 0.0636,\n",
    "                    \"OO Display\": 0.0043,\n",
    "                    \"OO Linear\": 0.0904,\n",
    "                    \"OO Video\": 0.0960,\n",
    "                    \"Paid Search\": 0.1401,\n",
    "                    \"Partnerships\": 0.2875,\n",
    "                    \"Podcast One Audio\": 0.0803,\n",
    "                }\n",
    "                prior_sigma = [v * len(returning_channel_contributions_12_1_23) for v in returning_channel_contributions_12_1_23.values()]\n",
    "            elif audience == 'new_winback':\n",
    "                new_winback_channel_contributions_12_1_23 = {\n",
    "                    \"HMI Audio\": 0.0122,\n",
    "                    \"HMI Digital Video\": 0.0237,\n",
    "                    \"HMI Display\": 0.0595,\n",
    "                    \"HMI Linear\": 0.0327,\n",
    "                    \"HMI OOH\": 0.0717,\n",
    "                    \"Influencer\": 0.0002,\n",
    "                    \"MACS App\": 0.0497,\n",
    "                    \"MACS Programmatic\": 0.0281,\n",
    "                    \"MACS Social\": 0.0541,\n",
    "                    \"OO Display\": 0.0067,\n",
    "                    \"OO Linear\": 0.0828,\n",
    "                    \"OO Video\": 0.1046,\n",
    "                    \"Paid Search\": 0.1387,\n",
    "                    \"Partnerships\": 0.2712,\n",
    "                    \"Podcast One Audio\": 0.0439,\n",
    "                }\n",
    "                prior_sigma = [v * len(new_winback_channel_contributions_12_1_23) for v in new_winback_channel_contributions_12_1_23.values()]\n",
    "\n",
    "    if saturation_beta_prior_sigma == \"default\":\n",
    "        model_config = model_config_param | {\"saturation_beta\": Prior(\"HalfNormal\", sigma=2, dims=\"channel\"), \n",
    "                                             \"saturation_lambda\": Prior(\"Gamma\", alpha=3, beta=1),\n",
    "                                            \"adstock_alpha\": Prior(\"Beta\", alpha=prior_alpha, beta=prior_beta, dims=\"channel\")}\n",
    "    elif saturation_beta_prior_sigma == \"manual\":\n",
    "        model_config = model_config_param | {\"saturation_beta\": Prior(\"HalfNormal\", sigma=beta_sigma_priors, dims=\"channel\"), \n",
    "                                             \"saturation_lambda\": Prior(\"Gamma\", alpha=lam_alpha_priors, beta=lam_beta_priors, dims=\"channel\"),\n",
    "                                             \"adstock_alpha\": Prior(\"Beta\", alpha=prior_alpha, beta=prior_beta, dims=\"channel\")}\n",
    "    else:\n",
    "        model_config = model_config_param | {\"saturation_beta\": Prior(\"HalfNormal\", sigma=prior_sigma, dims=\"channel\"),\n",
    "                                             \"saturation_lambda\": Prior(\"Gamma\", alpha=3, beta=1),\n",
    "                                            \"adstock_alpha\": Prior(\"Beta\", alpha=prior_alpha, beta=prior_beta, dims=\"channel\")}\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "    print(f\"Running prior_sigma using {saturation_beta_prior_sigma}\")\n",
    "    print(f\"Running prior_alpha and prior_beta of adstock using {adstock_alpha_prior}\")\n",
    "\n",
    "    # Filter out control columns that are all zero for this fold\n",
    "    nonzero_control_columns = [col for col in control_columns if X[col].sum() > 0]\n",
    "\n",
    "    \n",
    "        \n",
    "    ###################\n",
    "\n",
    "    return MMM(\n",
    "        adstock=GeometricAdstock(l_max=64),\n",
    "        saturation=LogisticSaturation(),\n",
    "        date_column=date_column,\n",
    "        channel_columns=channel_names, # use channel names \n",
    "        control_columns=nonzero_control_columns,\n",
    "        model_config=model_config,\n",
    "        time_varying_media=True,\n",
    "        time_varying_intercept=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_mmm(\n",
    "    mmm: MMM, X: pd.DataFrame, y: pd.Series, random_seed: np.random.Generator\n",
    ") -> MMM:\n",
    "    \"\"\"Fit the model.\"\"\"\n",
    "    fit_kwargs = {\n",
    "        \"tune\": tuneups,\n",
    "        \"chains\": 4,\n",
    "        \"draws\": draws,\n",
    "        \"nuts_sampler\": \"numpyro\",\n",
    "        \"random_seed\": random_seed,\n",
    "        \"prior_predictive\": True,\n",
    "        \"log_likelihood\": True\n",
    "    }\n",
    "    _ = mmm.fit(X, y, progressbar=True, **fit_kwargs)\n",
    "    _ = mmm.sample_posterior_predictive(\n",
    "        X, extend_idata=True, combined=True, progressbar=True, random_seed=random_seed\n",
    "    )\n",
    "    return mmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca49936-1073-4d32-b61c-efe1c2cd97a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d07481a-5705-462b-96db-3525c871a0a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For the sake of convenience, we define a data container to store the results of the time-slice cross validation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40a7473-91ae-4cae-964f-9886e1182c8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TimeSliceCrossValidationResult:\n",
    "    \"\"\"Container for the results of the time slice cross validation.\"\"\"\n",
    "\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: pd.Series\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: pd.Series\n",
    "    mmm: MMM\n",
    "    y_pred_test: pd.Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f44610e-8d6b-4a74-8b6e-6608f120985f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Finally, we define the main function that performs the time-slice cross validation step by calling the functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af25155-e9d6-4b0b-9093-16b2bf4d44a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def time_slice_cross_validation_step(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    random_seed: np.random.Generator,\n",
    ") -> TimeSliceCrossValidationResult:\n",
    "    \"\"\"Time-slice cross validation step.\n",
    "\n",
    "    We fit the model on the training data and generate predictions for the test data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: pd.DataFrame\n",
    "        Training data.\n",
    "    y_train: pd.Series\n",
    "        Training target.\n",
    "    X_test: pd.DataFrame\n",
    "        Test data.\n",
    "    y_test: pd.Series\n",
    "        Test target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TimeSliceCrossValidationResult\n",
    "        Results of the time slice cross validation step.\n",
    "    \"\"\"\n",
    "    mmm = get_mmm(X_train, channel_names=channel_names, control_columns=control_columns, date_column=date_column)\n",
    "    \n",
    "        # Pretty-print model configuration\n",
    "    print(\"\\n🔧 Model Configuration:\")\n",
    "    try:\n",
    "        config: Any = mmm.model_config  # or mmm.__dict__ if custom\n",
    "        print(pprint.pformat(config, sort_dicts=False))\n",
    "    except AttributeError:\n",
    "        print(\"No model_config attribute found in mmm object.\")\n",
    "        \n",
    "    # Alternative using f-string\n",
    "    print(f\"Configurations for mmm: {pprint.pformat(mmm.model_config)}\")\n",
    "    mmm = fit_mmm(mmm, X_train, y_train, random_seed)\n",
    "\n",
    "    y_pred_test = mmm.sample_posterior_predictive(\n",
    "        X_test,\n",
    "        include_last_observations=True,\n",
    "        original_scale=True,\n",
    "        extend_idata=False,\n",
    "        progressbar=False,\n",
    "        random_seed=random_seed,\n",
    "    )\n",
    "\n",
    "    return TimeSliceCrossValidationResult(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        mmm=mmm,\n",
    "        y_pred_test=y_pred_test,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf6e0ed9-9378-4ac0-96ee-4dfd6f2360e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Prior distribution Checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc67c8f9-cb28-4390-b6b7-69fde4a68ae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prior_dist_check_cv(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    random_seed: np.random.Generator):\n",
    "\n",
    "    \"\"\"\n",
    "    Plot the prior distribution check\n",
    "    \"\"\"\n",
    "\n",
    "    mmm = get_mmm(X_train, channel_names=channel_names, control_columns=control_columns, date_column=date_column)\n",
    "    prior_predictive = mmm.sample_prior_predictive(X_train, y_train, samples=4_000, extend_idata=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 3.5))  # shortened height\n",
    "\n",
    "    fig = mmm.plot_prior_predictive(original_scale=True, ax=ax)\n",
    "    # set y-axis to be in the range of y_var\n",
    "    ax.legend(loc=\"upper right\", bbox_to_anchor=(0.5, -0.2), ncol=4)\n",
    "    ax.set_title(\"Prior Predictive Sampling - Training Data\")\n",
    "    ax.set(xlabel=\"date\", ylabel=y_var)\n",
    "\n",
    "    # Save and log to MLflow\n",
    "    fig.savefig(f\"Prior Predictive Check - Training Data.png\", bbox_inches=\"tight\")\n",
    "    mlflow.log_artifact(f\"Prior Predictive Check - Training Data.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca34a39a-97c9-4a02-88a7-c2715cf345f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are now ready to run the time-slice cross validation loop 💪!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c919816d-632c-4298-895e-85f986c4ba52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Time-Slice-Cross-Validation Loop\n",
    "\n",
    "Depending on the business requirements, we need to decide the initial number of observations to use for fitting the model (`n_init`) and the forecast horizon (`forecast_horizon`). For this example, we use the first year's observations to fit the model and then predict the next 12 observations (3 months)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4a32e2-bd6b-48d9-89d5-1e2fdde39423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split the data into X and Y\n",
    "\n",
    "x_columns = [date_column] + channel_names + control_columns\n",
    "X = data_df.loc[:, x_columns]\n",
    "y = data_df[y_var]\n",
    "\n",
    "# Define your forecast horizons per month\n",
    "n_init = 365\n",
    "forecast_horizons = {\n",
    "    \"2024-01\": 31,\n",
    "    \"2024-02\": 29,\n",
    "    \"2024-03\": 31,\n",
    "}\n",
    "months = sorted(list(forecast_horizons.keys()))\n",
    "n_iterations = len(months)\n",
    "\n",
    "mlflow.log_param(\"forecast_horizons\", forecast_horizons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc4119e6-8c1f-4989-aa1b-3284e69fdfa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prior Distribution Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f79b282f-ba00-4142-a04f-6f9f12fc6c61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can now generate prior predictive samples to see how the model behaves under the prior specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a9f586b-9142-4fa1-a9a3-359cd5d32e01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "prior_check_results = []\n",
    "split_start = n_init\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    month_str = months[i]\n",
    "    forecast_horizon = forecast_horizons[month_str]\n",
    "    step_size = forecast_horizon\n",
    "\n",
    "    train_test_split = split_start\n",
    "\n",
    "    if train_test_split + forecast_horizon > len(y):\n",
    "        print(f\"⚠️ Skipping iteration {i} ({month_str}) — not enough data\")\n",
    "        continue\n",
    "\n",
    "    X_train = X.iloc[:train_test_split].copy()\n",
    "    y_train = y.iloc[:train_test_split].copy()\n",
    "    X_test = X.iloc[train_test_split : train_test_split + forecast_horizon].copy()\n",
    "    y_test = y.iloc[train_test_split : train_test_split + forecast_horizon].copy()\n",
    "\n",
    "\n",
    "    print(f\"\\n📦 Fold {i} — {month_str}\")\n",
    "    print(f\"Train: {X_train[date_column].min()} to {X_train[date_column].max()}\")\n",
    "    print(f\"Test:  {X_test[date_column].min()} to {X_test[date_column].max()}\")\n",
    "\n",
    "    result = prior_dist_check_cv(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        random_seed=rng,\n",
    "    )\n",
    "\n",
    "\n",
    "    prior_check_results.append(result)\n",
    "    split_start += step_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e987becb-3595-403c-a23a-187ab47a0f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's run it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6bdc28f-d902-4fd3-a48f-21ce189a4660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize Ray\n",
    "if not ray.is_initialized():\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Ray remote function for CV step\n",
    "@ray.remote\n",
    "def run_time_slice_cv(X_train, y_train, X_test, y_test, rng, max_retries=2):\n",
    "    import traceback\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            return time_slice_cross_validation_step(\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                random_seed=rng,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                print(f\"❌ FAILED: {e}\")\n",
    "                traceback.print_exc()\n",
    "                return None\n",
    "\n",
    "# Prepare tasks\n",
    "futures = []\n",
    "split_start = n_init\n",
    "split_indices = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    month_str = months[i]\n",
    "    forecast_horizon = forecast_horizons[month_str]\n",
    "    step_size = forecast_horizon\n",
    "\n",
    "    train_test_split = split_start\n",
    "\n",
    "    if train_test_split + forecast_horizon > len(y):\n",
    "        print(f\"⚠️ Skipping iteration {i} ({month_str}) — not enough data\")\n",
    "        continue\n",
    "\n",
    "    X_train = X.iloc[:train_test_split].copy()\n",
    "    y_train = y.iloc[:train_test_split].copy()\n",
    "    X_test = X.iloc[train_test_split : train_test_split + forecast_horizon].copy()\n",
    "    y_test = y.iloc[train_test_split : train_test_split + forecast_horizon].copy()\n",
    "\n",
    "    print(f\"Forecast horizon: {forecast_horizon} days\")\n",
    "    print(f\"Iteration {i} ({month_str})\")\n",
    "    # print training period\n",
    "    print(f\"Training period: {X_train[date_column].min()} to {X_train[date_column].max()}\")\n",
    "    # print testing period\n",
    "    print(f\"Testing period: {X_test[date_column].min()} to {X_test[date_column].max()}\")\n",
    "\n",
    "\n",
    "    future = run_time_slice_cv.remote(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        rng=rng,\n",
    "    )\n",
    "    futures.append(future)\n",
    "    split_indices.append((i, month_str))\n",
    "\n",
    "    split_start += step_size\n",
    "\n",
    "# Collect results with tqdm\n",
    "results = []\n",
    "for i, future in tqdm(enumerate(futures), total=len(futures), desc=\"TimeSlice CV\"):\n",
    "    result = ray.get(future)\n",
    "    if result is not None:\n",
    "        results.append(result)\n",
    "    else:\n",
    "        print(f\"⚠️ Skipped result at iteration {split_indices[i][0]}: {split_indices[i][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c769466d-60b7-4c41-a6f7-a978fa8bda22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Diagnostics\n",
    "\n",
    "First, we evaluate whether we have any divergences in the model (we can extend the analysis more more model diagnostics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "341e8dd1-25d1-457e-ad8f-9b26f7d26ed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "divergences = sum([result.mmm.idata[\"sample_stats\"][\"diverging\"].sum().item() for result in results])\n",
    "mlflow.log_metric(\"divergences\", divergences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbc267f6-f381-42be-bcf4-0a4a0b416a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Measure rhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "940114de-98c6-4859-8c23-6caac38c9e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TO DO: Log rhat to mlflow\n",
    "all_rhat = pd.DataFrame()\n",
    "\n",
    "for month, result in zip(months, results):\n",
    "    idata = result.mmm.idata\n",
    "\n",
    "    summary = az.summary(\n",
    "        data=idata,\n",
    "        var_names=[\n",
    "            \"intercept\",\n",
    "            \"y_sigma\",\n",
    "            \"saturation_beta\",\n",
    "            \"saturation_lam\",\n",
    "            \"adstock_alpha\",\n",
    "            \"gamma_control\",\n",
    "        ],\n",
    "        kind=\"diagnostics\"\n",
    "    )\n",
    "\n",
    "    # aggregate summary describe into a dataframe\n",
    "    summary_df = summary.describe().transpose().reset_index()\n",
    "    summary_df[\"month\"] = month\n",
    "    rhat_df = summary_df.loc[summary_df['index'] == 'r_hat']\n",
    "    # concat dataframes\n",
    "    all_rhat = pd.concat([all_rhat, rhat_df], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# store the mean r_hat per month into a dictionary\n",
    "rhat_per_month = all_rhat.groupby('month')['mean'].mean().round(2).to_dict()\n",
    "\n",
    "rhat_per_month\n",
    "\n",
    "# log to mlflow\n",
    "mlflow.log_param(\"r_hat\", rhat_per_month)\n",
    "\n",
    "#display(all_rhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9f9bd6c-89d7-4b19-bb8c-c93a3a1959ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lets visualize the posterior distributions of all model parameters from the first cross-validation fold of a PyMC-Marketing MMM model by plotting one histogram per parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca41f30-77a6-466b-92a1-0e6d29cdaa23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First result only\n",
    "month = months[0]\n",
    "result = results[0]\n",
    "idata = result.mmm.idata\n",
    "    \n",
    "\n",
    "# define param_names\n",
    "param_names = list(idata.posterior.data_vars)\n",
    "# define n_params\n",
    "n_params = len(param_names)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_params, figsize=(15, 3 * n_params), sharex=False)\n",
    "\n",
    "if n_params == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "for i, param in enumerate(param_names):\n",
    "        values = idata.posterior[param].values.flatten()\n",
    "        axes[i].hist(values, bins=50, density=True)\n",
    "        axes[i].set_title(param)\n",
    "        axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save and log\n",
    "fig.savefig(\"parameter_posterior_distribution_fold0.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"parameter_posterior_distribution_fold0.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a45c8a1b-01f2-4111-9b34-aa2071c9f8e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Model Trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ffd5492-2f46-41fc-b82d-5871049ace61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot trace and capture Axes array\n",
    "_ = az.plot_trace(\n",
    "    data=results[0].mmm.fit_result,\n",
    "    var_names=[\n",
    "        \"intercept\",\n",
    "        \"y_sigma\",\n",
    "        \"saturation_beta\",\n",
    "        \"saturation_lam\",\n",
    "        \"adstock_alpha\",\n",
    "        \"gamma_control\",\n",
    "    ],\n",
    "    compact=True,\n",
    "    backend_kwargs={\"figsize\": (12, 10), \"layout\": \"constrained\"},\n",
    ")\n",
    "\n",
    "# Get the current figure object after plot\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(\"Model Trace - Fold 0\", fontsize=16)\n",
    "\n",
    "# Save and log\n",
    "fig.savefig(\"model_trace_fold_0.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"model_trace_fold_0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff1925e-87a9-4789-b57d-10becadfe929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot trace and capture Axes array\n",
    "_ = az.plot_trace(\n",
    "    data=results[-1].mmm.fit_result,\n",
    "    var_names=[\n",
    "        \"intercept\",\n",
    "        \"y_sigma\",\n",
    "        \"saturation_beta\",\n",
    "        \"saturation_lam\",\n",
    "        \"adstock_alpha\",\n",
    "        \"gamma_control\",\n",
    "    ],\n",
    "    compact=True,\n",
    "    backend_kwargs={\"figsize\": (12, 10), \"layout\": \"constrained\"},\n",
    ")\n",
    "\n",
    "# Get the current figure object after plot\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(\"Model Trace - Last Fold\", fontsize=16)\n",
    "\n",
    "# Save and log\n",
    "fig.savefig(\"model_trace_last_fold.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"model_trace_last_fold.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db8dd938-d92a-44f6-a99a-901a59ff05fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Prior vs. Posterior Influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49aa4f9a-d71a-41ed-8f11-e34c0f8e1b60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "idata = results[0].mmm.idata\n",
    "print(idata.groups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d97dc101-95c6-4f66-9218-98c2f6496490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "az.plot_dist_comparison(\n",
    "    data=results[1].mmm.idata,\n",
    "    var_names=[\n",
    "        \"intercept\",\n",
    "        \"y_sigma\",\n",
    "        \"saturation_beta\",\n",
    "        \"saturation_lam\",\n",
    "        \"adstock_alpha\",\n",
    "        \"gamma_control\",\n",
    "    ],\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31b20159-53ff-4fc3-9381-e0ad39966ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate Parameter Stability\n",
    "\n",
    "Next, we look at the stability of the model parameters. For a good model, these should not change abruptly over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01e0ec9f-9a5f-4c1c-83d0-701a39fbab8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8f52de2-657d-484b-bb8c-c17fcbf20f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "records_df = pd.DataFrame()\n",
    "\n",
    "for month, result in zip(months, results):\n",
    "    idata = result.mmm.idata\n",
    "    intercept = idata.posterior[\"intercept\"]  # Could be scalar or time-varying\n",
    "\n",
    "    # Stack chain and draw into one dimension\n",
    "    intercept_stacked = intercept.stack(sample=(\"chain\", \"draw\"))\n",
    "\n",
    "    # Check if intercept is time-varying (has more than 1 dim after stacking)\n",
    "    non_sample_dims = [dim for dim in intercept_stacked.dims if dim != \"sample\"]\n",
    "\n",
    "    if non_sample_dims:  # Time-varying intercept\n",
    "        time_dim = non_sample_dims[0]\n",
    "        n_days = intercept_stacked.sizes[time_dim]\n",
    "\n",
    "        # Get the date range based on X_train\n",
    "        start_date = pd.to_datetime(result.X_train[date_column].min())\n",
    "        time_index = pd.date_range(start=start_date, periods=n_days, freq=\"D\")\n",
    "\n",
    "        # Reshape to DataFrame: [time x sample]\n",
    "        intercept_df = pd.DataFrame(\n",
    "            intercept_stacked.transpose(time_dim, \"sample\").values,\n",
    "            index=time_index,\n",
    "            columns=intercept_stacked.coords[\"sample\"].values,\n",
    "        )\n",
    "        intercept_df = intercept_df.rename_axis(\"day_dt\").reset_index()\n",
    "        intercept_df[\"iteration\"] = month\n",
    "\n",
    "        # Melt to long format\n",
    "        intercept_long = intercept_df.melt(\n",
    "            id_vars=[\"day_dt\", \"iteration\"], \n",
    "            var_name=\"sample_id\", \n",
    "            value_name=\"intercept\"\n",
    "        )\n",
    "\n",
    "        # Monthly summaries\n",
    "        intercept_long[\"month_of_day_dt\"] = intercept_long[\"day_dt\"].dt.to_period(\"M\")\n",
    "        grouped = intercept_long.groupby(\"month_of_day_dt\")[\"intercept\"]\n",
    "\n",
    "        monthly_summary = grouped.agg(mean=\"mean\", median=\"median\", std=\"std\").reset_index()\n",
    "        monthly_summary[\"lower_ci\"] = grouped.quantile(0.025).values\n",
    "        monthly_summary[\"upper_ci\"] = grouped.quantile(0.975).values\n",
    "        monthly_summary[\"iteration\"] = month\n",
    "\n",
    "        records_df = pd.concat([records_df, monthly_summary], ignore_index=True)\n",
    "\n",
    "    else:  # Scalar intercept\n",
    "        intercept_flat = intercept_stacked.values\n",
    "\n",
    "        summary = {\n",
    "            \"iteration\": month,\n",
    "            \"month_of_day_dt\": pd.Period(result.X_train[date_column].min(), freq=\"M\"),\n",
    "            \"mean\": intercept_flat.mean(),\n",
    "            \"median\": np.median(intercept_flat),\n",
    "            \"std\": intercept_flat.std(),\n",
    "            \"lower_ci\": np.quantile(intercept_flat, 0.025),\n",
    "            \"upper_ci\": np.quantile(intercept_flat, 0.975),\n",
    "        }\n",
    "\n",
    "        records_df = pd.concat([records_df, pd.DataFrame([summary])], ignore_index=True)\n",
    "\n",
    "# Convert month to string for plotting\n",
    "records_df[\"month_of_day_dt\"] = records_df[\"iteration\"].astype(str)\n",
    "\n",
    "display(records_df)\n",
    "\n",
    "# plot the intercepts\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "\n",
    "az.plot_forest(\n",
    "    data=[result.mmm.idata[\"posterior\"] for result in results],\n",
    "    model_names=[f\"{i}\" for i in months],\n",
    "    var_names=[\"intercept\"],\n",
    "    combined=True,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.suptitle(\"Intercept\", fontsize=18, fontweight=\"bold\", y=1.06)\n",
    "fig.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"Intercept Stability.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"Intercept Stability.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e97e857e-5f5a-4eb0-911f-7437ab105df5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Gamma Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46a5df15-5d42-4de5-b87c-a511fc4da0f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 18))\n",
    "\n",
    "az.plot_forest(\n",
    "    data=[result.mmm.idata[\"posterior\"] for result in results],\n",
    "    model_names=[f\"{i}\" for i in months],\n",
    "    var_names=[\"gamma_control\"],\n",
    "    combined=True,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.suptitle(\"Non-Media Coefficients (Gamma Control)\", fontsize=18, fontweight=\"bold\", y=1.06);\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"Non-Media Coefficients Stability.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"Non-Media Coefficients Stability.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c49428ea-b8ee-473c-9951-9fa4cacbebeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Adstock Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40de847-af38-48c5-900c-dcb6ad8852f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "az.plot_forest(\n",
    "    data=[result.mmm.idata[\"posterior\"] for result in results],\n",
    "    model_names=[f\"{i}\" for i in months],\n",
    "    var_names=[\"adstock_alpha\"],\n",
    "    combined=True,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.suptitle(\"Adstock Alpha\", fontsize=18, fontweight=\"bold\", y=1.06);\n",
    "# label x-axis as \"Adstock Alpha\"\n",
    "ax.set_xlabel(\"Alpha (Low=faster decay, High = slower decay)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"Geo Adstock Alpha Stability.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"Geo Adstock Alpha Stability.png\")\n",
    "\n",
    "# double check whether this is theta \n",
    "#A higher ( \\alpha ) results in a slower decay of media effects, leading to higher cumulative adstock values.\n",
    "#A lower ( \\alpha ) results in a faster decay, leading to lower cumulative adstock values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f7983b5-fc47-4107-87f3-9ad30927c8d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Saturation Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edbc6c2d-ef9c-49b3-a5cf-2a18797050bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "az.plot_forest(\n",
    "    data=[result.mmm.idata[\"posterior\"] for result in results],\n",
    "    model_names=[f\"{i}\" for i in months],\n",
    "    var_names=[\"saturation_beta\"],\n",
    "    combined=True,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.suptitle(\"Saturation Beta\", fontsize=18, fontweight=\"bold\", y=1.06);\n",
    "\n",
    "ax.set_xlabel(\"Beta (Low = fast saturation, High = slow saturation)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"Saturation Beta Stability.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"Saturation Beta Stability.png\")\n",
    "\n",
    "\n",
    "#High ( \\beta ): Media effectiveness drops off quickly after reaching the saturation point, indicating that additional spend or impressions beyond a certain level have little incremental effect.\n",
    "# Low ( \\beta ): Media effectiveness decreases more gradually, allowing for a smoother decline in returns as spend or impressions increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8744611d-88c8-40f9-95ac-d53278eb5747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Saturation Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d0b426b-2213-4dba-92c5-8945133bff91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "az.plot_forest(\n",
    "    data=[result.mmm.idata[\"posterior\"] for result in results],\n",
    "    model_names=[f\"{i}\" for i in months],\n",
    "    var_names=[\"saturation_lam\"],\n",
    "    combined=True,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.suptitle(\"Saturation Lambda\", fontsize=18, fontweight=\"bold\", y=1.06);\n",
    "# label x-axis as \"Saturation Lambda\"\n",
    "ax.set_xlabel(\"Lambda (Low = fast saturation, High = slow saturation)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# If ( \\lambda ) is small, the media channel saturates quickly, meaning additional spend or impressions beyond a certain point will have little incremental effect.\n",
    "# If ( \\lambda ) is large, the media channel can sustain higher levels of spend or impressions before experiencing diminishing returns.\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"Saturation Lambda Stability.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"Saturation Lambda Stability.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e9bfcfe-a12b-4631-b09b-c52586444959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The parameters seem to be stable over time. This implies that the estimates ROAS will not change abruptly over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d33de820-22dc-4077-ad8c-73b1a7129815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's look at contribution of each channel to the total spend for each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bfc6291-377a-4205-88cd-0dc29ab9958a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate Out of Sample Predictions\n",
    "\n",
    "Finally, we evaluate the out of sample predictions. To begin with, we can simply plot the posterior predictive distributions for each iteration for both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff1ac059-98b3-4f35-b299-2e6d088532b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=n_iterations,\n",
    "    ncols=1,\n",
    "    figsize=(20, 12),\n",
    "    sharex=True,\n",
    "    sharey=False,\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    ax = axes[i]\n",
    "    result.mmm.plot_posterior_predictive(original_scale=True, ax=ax)\n",
    "\n",
    "    hdi_prob = 0.94\n",
    "    test_hdi = az.hdi(result.y_pred_test[\"y\"].to_numpy().T, hdi_prob=hdi_prob)\n",
    "\n",
    "    ax.fill_between(\n",
    "        result.X_test[date_column],\n",
    "        test_hdi[:, 0],\n",
    "        test_hdi[:, 1],\n",
    "        color=\"C1\",\n",
    "        label=f\"{hdi_prob:.0%} HDI (test)\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax.plot(X[date_column], y, marker=\"o\", color=\"black\")\n",
    "    ax.axvline(result.X_test[date_column].iloc[0], color=\"C2\", linestyle=\"--\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "axes[-1].set(xlim=(X[date_column].iloc[n_init - 9], None))\n",
    "fig.suptitle(\"Posterior Predictive Check\", fontsize=18, fontweight=\"bold\", y=1.02);\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"Posterior Predictive Check.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"Posterior Predictive Check.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac9fe553-b9ee-4e0d-8132-3df037212f6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Overall, the out of sample predictions look very good 🚀!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9909155-75d1-4861-8b0a-7f0434d2380c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can quantify the model performance using the Continuous Ranked Probability Score (CRPS).\n",
    "\n",
    "> *“The CRPS — Continuous Ranked Probability Score — is a score function that compares a single ground truth value to a Cumulative Distribution Function. It can be used as a metric to evaluate a model’s performance when the target variable is continuous and the model predicts the target’s distribution; Examples include Bayesian Regression or Bayesian Time Series models.”*\n",
    "\n",
    "\n",
    "For a nice explanation of the CRPS, check out this [blog post](https://towardsdatascience.com/crps-a-scoring-function-for-bayesian-machine-learning-models-dd55a7a337a8).\n",
    "\n",
    "In PyMC Marketing, we provide the function {func}`crps <pymc_marketing.metrics.crps>` to compute this metric. We can use it to compute the CRPS score for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be9fe39-6092-4514-8987-8ed5d6265469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crps_results_train: list[float] = [\n",
    "    crps(\n",
    "        y_true=result.y_train.to_numpy(),\n",
    "        y_pred=az.extract(\n",
    "            # Scale the predictions back to the original scale\n",
    "            apply_sklearn_transformer_across_dim(\n",
    "                data=result.mmm.idata.posterior_predictive[\"y\"],\n",
    "                func=result.mmm.get_target_transformer().inverse_transform,\n",
    "                dim_name=\"date\",\n",
    "            )\n",
    "        )[\"y\"]\n",
    "        .to_numpy()\n",
    "        .T,\n",
    "    )\n",
    "    for result in results\n",
    "]\n",
    "\n",
    "\n",
    "crps_results_test: list[float] = [\n",
    "    crps(\n",
    "        y_true=result.y_test.to_numpy(),\n",
    "        y_pred=result.y_pred_test[\"y\"].to_numpy().T,\n",
    "    )\n",
    "    for result in results\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=2, ncols=1, figsize=(12, 7), sharex=True, sharey=False, layout=\"constrained\"\n",
    ")\n",
    "\n",
    "ax[0].plot(crps_results_train, marker=\"o\", color=\"C0\", label=\"train\")\n",
    "ax[0].set(ylabel=\"CRPS\", title=\"Train CRPS\")\n",
    "ax[1].plot(crps_results_test, marker=\"o\", color=\"C1\", label=\"test\")\n",
    "ax[1].set(xlabel=\"Iteration\", ylabel=\"CRPS\", title=\"Test CRPS\")\n",
    "fig.suptitle(\"CRPS for each iteration\", fontsize=18, fontweight=\"bold\", y=1.05);\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"CRPS.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"CRPS.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3da80d04-d011-4896-9544-42e0d1dd337e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Component Contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24eab786-0312-44cd-bb50-72816b221cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_contributions = []\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    contrib = result.mmm.compute_mean_contributions_over_time(original_scale=True).copy()\n",
    "\n",
    "    # Use the index as day\n",
    "    contrib[\"day_dt\"] = pd.to_datetime(contrib.index)\n",
    "    contrib[\"fold\"] = i + 1\n",
    "\n",
    "    all_contributions.append(contrib)\n",
    "\n",
    "df_all_contributions = pd.concat(all_contributions, ignore_index=True)\n",
    "# make day_dt the index\n",
    "df_all_contributions = df_all_contributions.set_index(\"day_dt\", drop=False)\n",
    "\n",
    "# melt df\n",
    "melt_contrib = df_all_contributions.melt(id_vars=[\"day_dt\", \"fold\"], var_name=\"channel\", value_name=\"mean_contribution\")\n",
    "\n",
    "# aggregate by day_dt, and channel\n",
    "mean_contrib = melt_contrib.groupby([\"day_dt\", \"channel\"])[\"mean_contribution\"].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e78b2cf-2b0b-456c-a0e0-d16508651894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(mean_contrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb103f2-11eb-4645-ade3-1f676dba3d7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_grouped_contribution_share(\n",
    "    df,\n",
    "    channel_groups,\n",
    "    format=\"long\",\n",
    "    date_col=\"day_dt\",\n",
    "    freq=\"D\",\n",
    "    percentage_share=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes grouped contribution values or percentage shares over time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with columns ['date', 'channel', 'mean_contribution'].\n",
    "    channel_groups : dict\n",
    "        Dictionary of group_name -> list of channel names.\n",
    "    format : str\n",
    "        Output format: 'long' or 'wide'.\n",
    "    date_col : str\n",
    "        Column name for date.\n",
    "    freq : str\n",
    "        Resampling frequency: 'D' = daily, 'W' = weekly, 'M' = monthly, etc.\n",
    "    percentage_share : bool\n",
    "        If True, return percentage share; if False, return original values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Long or wide format DataFrame with contributions or percent shares by group.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # Aggregate contributions per date and channel\n",
    "    grouped = df.groupby([date_col, \"channel\"], as_index=False).agg({\"mean_contribution\": \"mean\"})\n",
    "\n",
    "    # Pivot to wide format\n",
    "    df_pivot = grouped.pivot(index=date_col, columns=\"channel\", values=\"mean_contribution\").fillna(0)\n",
    "\n",
    "    # Resample by frequency\n",
    "    df_resampled = df_pivot.resample(freq).mean()\n",
    "\n",
    "    # Sum contributions within each group\n",
    "    group_contributions = {}\n",
    "    for group_name, channels in channel_groups.items():\n",
    "        valid_channels = [c for c in channels if c in df_resampled.columns]\n",
    "        group_contributions[group_name] = df_resampled[valid_channels].sum(axis=1)\n",
    "\n",
    "    # Build DataFrame of group contributions\n",
    "    df_grouped = pd.DataFrame(group_contributions)\n",
    "    df_grouped[date_col] = df_resampled.index\n",
    "\n",
    "    if percentage_share:\n",
    "        df_grouped_share = df_grouped.set_index(date_col).div(df_grouped.set_index(date_col).sum(axis=1), axis=0) * 100\n",
    "        df_grouped_share = df_grouped_share.reset_index()\n",
    "        value_col = \"percent_share\"\n",
    "        df_out = df_grouped_share\n",
    "    else:\n",
    "        df_out = df_grouped\n",
    "        value_col = \"contribution\"\n",
    "\n",
    "    if format == \"long\":\n",
    "        df_long = df_out.melt(id_vars=[date_col], var_name=\"group\", value_name=value_col)\n",
    "        return df_long.reset_index(drop=True)\n",
    "\n",
    "    elif format == \"wide\":\n",
    "        return df_out.reset_index(drop=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid format. Choose 'long' or 'wide'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cf1ba52-c4b4-4b06-bb5a-9b51e63f6261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "channel_groupings = {\n",
    "        \"Intercept\": ['intercept'],\n",
    "        \"Day of Week Seasonality\": [col for col in control_columns if col.startswith(\"dow_\")],\n",
    "        \"Trend\": [col for col in control_columns if col.startswith('trend_')],\n",
    "        \"Events\": [col for col in control_columns if col.startswith(\"event_\")],\n",
    "        \"Media\": channel_names\n",
    "    }\n",
    "\n",
    "df_percent_share = compute_grouped_contribution_share(mean_contrib, channel_groupings, format=\"wide\", date_col = \"day_dt\", freq=\"M\", percentage_share=True)\n",
    "\n",
    "\n",
    "# Save your DataFrame to a temporary file\n",
    "df_percent_share.to_csv(\"high_level_contribution_percent.csv\", index=False)\n",
    "\n",
    "# Log it as an artifact in the current MLflow run\n",
    "mlflow.log_artifact(\"high_level_contribution_percent.csv\")\n",
    "\n",
    "display(df_percent_share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "744d6c4d-21fc-4222-9339-9162b37512a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "channel_groupings = {\n",
    "        \"Intercept\": ['intercept'],\n",
    "        \"Day of Week Seasonality\": [col for col in control_columns if col.startswith(\"dow_\")],\n",
    "        \"Trend\": [col for col in control_columns if col.startswith('trend_')],\n",
    "        \"Events\": [col for col in control_columns if col.startswith(\"event_\")],\n",
    "        \"Media\": channel_names\n",
    "    }\n",
    "\n",
    "high_level_contrib = compute_grouped_contribution_share(mean_contrib, channel_groupings, format=\"wide\", date_col = \"day_dt\", freq=\"D\", percentage_share=False)\n",
    "\n",
    "\n",
    "# Save your DataFrame to a temporary file\n",
    "high_level_contrib.to_csv(\"high_level_contributions.csv\", index=False)\n",
    "\n",
    "# Log it as an artifact in the current MLflow run\n",
    "mlflow.log_artifact(\"high_level_contributions.csv\")\n",
    "\n",
    "display(high_level_contrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eea14164-f279-47a6-881b-705aad9ab9ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "channel_groupings = {\n",
    "    \"Base\": (\n",
    "        ['intercept']\n",
    "        + [col for col in control_columns if col.startswith(\"dow_\")]\n",
    "        + [col for col in control_columns if col.startswith(f\"trend_\")]\n",
    "        + [col for col in control_columns if col.startswith(\"event_\")]\n",
    "    ),\n",
    "    \"Partnerships\": [\"Partnerships\"],\n",
    "    \"Paid Search\": [\"Paid Search\"],\n",
    "    \"OO Video\": [\"OO Video\"],\n",
    "    \"OO Linear\": [\"OO Linear\"],\n",
    "    \"Podcast One Audio\": [\"Podcast One Audio\"],\n",
    "    \"MACS Social\": [\"MACS Social\"],\n",
    "    \"HMI OOH\": [\"HMI OOH\"],\n",
    "    \"HMI Display\": [\"HMI Display\"],\n",
    "    \"MACS App\": [\"MACS App\"],\n",
    "    \"MACS Programmatic\": [\"MACS Programmatic\"],\n",
    "    \"HMI Digital Video\": [\"HMI Digital Video\"],\n",
    "    \"HMI Linear\": [\"HMI Linear\"],\n",
    "    \"HMI Audio\": [\"HMI Audio\"],\n",
    "    \"OO Display\": [\"OO Display\"],\n",
    "    \"Influencer\": [\"Influencer\"],\n",
    "}\n",
    "\n",
    "media_percent_share = compute_grouped_contribution_share(mean_contrib, channel_groupings, format=\"wide\", date_col = \"day_dt\", freq=\"M\")\n",
    "\n",
    "# Save your DataFrame to a temporary file\n",
    "media_percent_share.to_csv(\"media_contributions.csv\", index=False)\n",
    "\n",
    "# Log it as an artifact in the current MLflow run\n",
    "mlflow.log_artifact(\"media_contributions.csv\")\n",
    "\n",
    "display(media_percent_share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4869bc-47cb-4705-89bf-4d16c20b8b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "exclude_cols = [\"Base\", \"day_dt\"]\n",
    "contribution_cols = [col for col in media_percent_share.columns if col not in exclude_cols]\n",
    "\n",
    "# Compute mean contributions and get top 3\n",
    "top3 = media_percent_share[contribution_cols].mean().sort_values(ascending=False).head(3)\n",
    "print(top3)\n",
    "\n",
    "\n",
    "# Log each as MLflow param with rank\n",
    "\n",
    "for i, (channel, contrib) in enumerate(top3.items(), start=1):\n",
    "    rank_key = f\"Rank {i} channel\"\n",
    "    value = {channel: round(contrib, 2)}\n",
    "    mlflow.log_param(rank_key, value)  # convert dict to JSON string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38e7d3e3-c72e-4350-b21f-6f46d40605d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06d45bc8-7178-4c2e-802f-371e535be51e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_area_contributions(df, date_col=\"day_dt\", title=\"Component Contributions Over Time\"):\n",
    "    \"\"\"\n",
    "    Plots an area chart of component contributions over time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with datetime column and float columns per component.\n",
    "    date_col : str\n",
    "        Name of the datetime column.\n",
    "    title : str\n",
    "        Title of the plot.\n",
    "    \"\"\"\n",
    "    # Sort by date to ensure correct plotting order\n",
    "    df = df.sort_values(date_col)\n",
    "\n",
    "    # Set datetime column as index\n",
    "    df_plot = df.set_index(date_col)\n",
    "\n",
    "    # Plot area chart\n",
    "    ax = df_plot.plot.area(figsize=(14, 6), alpha=0.8)\n",
    "\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylabel(\"Contribution\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.grid(True, which=\"major\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_area_contributions(media_percent_share, date_col=\"day_dt\", title=\"Monthly Component Contribution Shares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6b5dad-fd4e-479d-adfe-5fbace593907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for any negative coefficients, replace with 0\n",
    "high_level_contrib_positive = high_level_contrib.copy()\n",
    "high_level_contrib_positive['Trend'] = high_level_contrib['Trend'].clip(lower=0)\n",
    "high_level_contrib_positive['Events'] = high_level_contrib['Events'].clip(lower=0)\n",
    "high_level_contrib_positive['Day of Week Seasonality'] = high_level_contrib['Day of Week Seasonality'].clip(lower=0)\n",
    "\n",
    "plot_area_contributions(high_level_contrib_positive, title=\"Monthly Component Contributions\")\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"CRPS.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"CRPS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa124d2c-d256-4724-ac0c-e3ea87286796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_share = media_percent_share[\"Base\"].loc[media_percent_share[\"day_dt\"].dt.year == 2024].mean()\n",
    "media_share = 100 - base_share\n",
    "\n",
    "print(\"Base Contribution Share:\", base_share)\n",
    "print(\"Media Contribution Share:\", media_share)\n",
    "\n",
    "mlflow.log_metrics({\"base_share\": round(base_share,2), \"media_share\": round(media_share,2)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb8e2127-ebff-4b6e-ab7e-b5c86cef09b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5c177c-488e-4a80-8fe4-cef23ffcf229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_out_sample_metrics_cv(results, metric=\"mape\"):\n",
    "    \"\"\"\n",
    "    Computes MAPE (Mean Absolute Percentage Error) across all folds of a\n",
    "    cross-validated PyMC-Marketing MMM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : list of TimeSliceCrossValidationResult\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        MAPE across all folds.\n",
    "    \"\"\"\n",
    "\n",
    "    if metric == \"mape\":\n",
    "      all_y_true = []\n",
    "      all_y_pred = []\n",
    "\n",
    "      for result in results:\n",
    "            y_true = result.y_test.values.flatten()\n",
    "            y_pred_samples = result.y_pred_test[\"y\"].to_numpy()  # shape: (T, S)\n",
    "\n",
    "            y_pred_mean = y_pred_samples.T.mean(axis=0)\n",
    "\n",
    "            all_y_true.extend(y_true)\n",
    "            all_y_pred.extend(y_pred_mean)\n",
    "\n",
    "      # Convert to numpy arrays\n",
    "      all_y_true = np.array(all_y_true)\n",
    "      all_y_pred = np.array(all_y_pred)\n",
    "\n",
    "      mape = np.mean(np.abs((all_y_true - all_y_pred) / all_y_true)) * 100\n",
    "      return mape\n",
    "    elif metric == \"coverage\":\n",
    "      total = 0\n",
    "      covered = 0\n",
    "\n",
    "      for result in results:\n",
    "          y_true = result.y_test.values.flatten()\n",
    "\n",
    "          # shape: (T, S), need (S, T) for az.hdi\n",
    "          y_pred_samples = result.y_pred_test[\"y\"].to_numpy().T  # shape: (samples, time)\n",
    "          hdi = az.hdi(y_pred_samples, hdi_prob=hdi_prob)  # shape: (T, 2)\n",
    "\n",
    "          lower, upper = hdi[:, 0], hdi[:, 1]\n",
    "          is_covered = (y_true >= lower) & (y_true <= upper)\n",
    "\n",
    "          covered += is_covered.sum()\n",
    "          total += len(y_true)\n",
    "\n",
    "      coverage_rate = covered / total\n",
    "      return coverage_rate\n",
    "\n",
    "    \n",
    "    else:\n",
    "      raise ValueError(\"metric must be either mape, coverage, or pseudo_r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f0b1fa-d635-4498-a768-467588322342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mape = compute_out_sample_metrics_cv(results, metric=\"mape\")\n",
    "coverage = compute_out_sample_metrics_cv(results, metric=\"coverage\")\n",
    "\n",
    "print(\"MAPE for out-of-sample:\", mape)\n",
    "print(\"coverage for out-of-sample:\", coverage)\n",
    "\n",
    "mlflow.log_metric(\"mape\", round(mape,2))\n",
    "mlflow.log_metric(\"coverage\", round(coverage,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c99cba08-5b84-4a66-829a-6780cc3acf6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Posterior fit per fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19e7b07-c76e-43c1-965b-588f5dab3760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for month, result in zip(months,results):\n",
    "\n",
    "  fig = result.mmm.plot_posterior_predictive(original_scale=True, figsize=(10, 3))\n",
    "  # Get Axes and set x-axis limits\n",
    "  ax = fig.axes[0]  # assumes there's one main plot\n",
    "  ax.set_xlim([pd.to_datetime('2023-12'), pd.to_datetime(\"2024-04\")])\n",
    "  ax.set_title(f\"Posterior Predictions - {month}\")\n",
    "  print(month, result.X_test[\"day_dt\"].min(), result.X_test[\"day_dt\"].max())\n",
    "\n",
    "  # Save and log to MLflow\n",
    "  fig.savefig(f\"Posterior Predictions Over Time - {month}\", bbox_inches=\"tight\")\n",
    "  mlflow.log_artifact(f\"Posterior Predictions Over Time - {month}.png\")\n",
    "\n",
    "  # TO DO: troubleshoot why the last month isn't showing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da32bc8d-7244-4424-8e72-5b512b4d7a0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mape_over_time(results):\n",
    "    \"\"\"\n",
    "    Plot MAPE over time across all test folds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : list of TimeSliceCrossValidationResult\n",
    "    \"\"\"\n",
    "    all_fold_mape = []\n",
    "\n",
    "    for result in results:\n",
    "        y_true = result.y_test.values.flatten()\n",
    "        y_pred = result.y_pred_test[\"y\"].to_numpy().T.mean(axis=0)\n",
    "        dates = result.X_test[\"day_dt\"].values\n",
    "\n",
    "        # Avoid div-by-zero\n",
    "        mask = y_true != 0\n",
    "        y_true = y_true[mask]\n",
    "        y_pred = y_pred[mask]\n",
    "        dates = dates[mask]\n",
    "\n",
    "        mape = np.abs((y_true - y_pred) / y_true)\n",
    "\n",
    "        fold_df = pd.DataFrame({\n",
    "            \"day_dt\": pd.to_datetime(dates),\n",
    "            \"mape\": mape\n",
    "        })\n",
    "        all_fold_mape.append(fold_df)\n",
    "\n",
    "    # Combine all folds and average if needed\n",
    "    mape_df = pd.concat(all_fold_mape)\n",
    "    mape_daily = mape_df.groupby(\"day_dt\").mean().reset_index()\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(mape_daily[\"day_dt\"], mape_daily[\"mape\"] * 100, marker=\"o\")\n",
    "    ax.set_ylabel(\"MAPE (%)\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_title(\"MAPE Over Time Across Folds\")\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and log to MLflow\n",
    "    fig.savefig(\"MAPE Over Time Across Folds - Out Sample\", bbox_inches=\"tight\")\n",
    "    mlflow.log_artifact(\"MAPE Over Time Across Folds - Out Sample.png\")\n",
    "\n",
    "plot_mape_over_time(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65505a1c-d4fa-4043-b7fd-0997f23d5a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2b4134f-1482-4cfe-8715-cd5d740de549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92fde0f3-94a3-423b-bab3-9457cd3cc7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Bayesian R^2"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "audience",
      "width": 194
     },
     {
      "breakBefore": false,
      "name": "quality_type",
      "width": 194
     },
     {
      "breakBefore": false,
      "name": "intercept_prior",
      "width": 403
     },
     {
      "breakBefore": false,
      "name": "gamma_control_prior",
      "width": 403
     },
     {
      "breakBefore": false,
      "name": "adstock_alpha_prior",
      "width": 298
     },
     {
      "breakBefore": false,
      "name": "saturation_beta_prior_sigma",
      "width": 298
     },
     {
      "breakBefore": false,
      "name": "sample_tuneups",
      "width": 194
     },
     {
      "breakBefore": false,
      "name": "sample_draws",
      "width": 194
     }
    ]
   },
   "notebookName": "mmm_cross_validation",
   "widgets": {
    "adstock_alpha_prior": {
     "currentValue": "short_long_memory",
     "nuid": "5bf1af6a-e2b3-4652-be6c-570ce793f2e6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "uniform",
      "label": null,
      "name": "adstock_alpha_prior",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "spend_imps_ratio",
        "contrib_imps_ratio",
        "short_long_memory",
        "uniform"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "uniform",
      "label": null,
      "name": "adstock_alpha_prior",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "spend_imps_ratio",
        "contrib_imps_ratio",
        "short_long_memory",
        "uniform"
       ]
      }
     }
    },
    "audience": {
     "currentValue": "returning",
     "nuid": "02f45e64-cdc0-4a37-909d-7939857efbe3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "new_winback",
      "label": null,
      "name": "audience",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "new_winback",
        "returning"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "new_winback",
      "label": null,
      "name": "audience",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "new_winback",
        "returning"
       ]
      }
     }
    },
    "gamma_control_prior": {
     "currentValue": "dist=Normal_mu=0_sigma=.05_dims=control",
     "nuid": "e1da57f3-9992-4b97-80b2-45515f31e9c4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dist=Normal_mu=0_sigma=.05_dims=control",
      "label": null,
      "name": "gamma_control_prior",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dist=Normal_mu=0_sigma=.025_dims=control",
        "dist=Normal_mu=0_sigma=.03_dims=control",
        "dist=Normal_mu=0_sigma=.05_dims=control",
        "dist=Normal_mu=0_sigma=.1_dims=control"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dist=Normal_mu=0_sigma=.05_dims=control",
      "label": null,
      "name": "gamma_control_prior",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dist=Normal_mu=0_sigma=.025_dims=control",
        "dist=Normal_mu=0_sigma=.03_dims=control",
        "dist=Normal_mu=0_sigma=.05_dims=control",
        "dist=Normal_mu=0_sigma=.1_dims=control"
       ]
      }
     }
    },
    "intercept_prior": {
     "currentValue": "dist=TruncatedNormal_mu=0.45_sigma=0.05_lower=0.3_upper=0.6",
     "nuid": "8470323a-473a-4fb5-9706-fb81218cae86",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dist=TruncatedNormal_mu=0.4_sigma=0.05_lower=0.3_upper=0.6",
      "label": null,
      "name": "intercept_prior",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dist=TruncatedNormal_mu=0.4_sigma=0.05_lower=0.3_upper=0.6",
        "dist=TruncatedNormal_mu=0.45_sigma=0.05_lower=0.3_upper=0.6",
        "dist=TruncatedNormal_mu=0.5_sigma=0.05_lower=0.4_upper=0.7",
        "dist=TruncatedNormal_mu=0.2_sigma=0.1_lower=0.1",
        "dist=TruncatedNormal_mu=0.3_sigma=0.1_lower=0.1",
        "dist=TruncatedNormal_mu=0.4_sigma=0.1_lower=0.1",
        "dist=TruncatedNormal_mu=0.5_sigma=0.1_lower=0.1",
        "dist=HalfNormal_sigma=1"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dist=TruncatedNormal_mu=0.4_sigma=0.05_lower=0.3_upper=0.6",
      "label": null,
      "name": "intercept_prior",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dist=TruncatedNormal_mu=0.4_sigma=0.05_lower=0.3_upper=0.6",
        "dist=TruncatedNormal_mu=0.45_sigma=0.05_lower=0.3_upper=0.6",
        "dist=TruncatedNormal_mu=0.5_sigma=0.05_lower=0.4_upper=0.7",
        "dist=TruncatedNormal_mu=0.2_sigma=0.1_lower=0.1",
        "dist=TruncatedNormal_mu=0.3_sigma=0.1_lower=0.1",
        "dist=TruncatedNormal_mu=0.4_sigma=0.1_lower=0.1",
        "dist=TruncatedNormal_mu=0.5_sigma=0.1_lower=0.1",
        "dist=HalfNormal_sigma=1"
       ]
      }
     }
    },
    "macs_app_saturation_prior": {
     "currentValue": "",
     "nuid": "1ba6e566-a46d-48b7-b393-70135627f995",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "macs_app_saturation_prior",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "macs_app_saturation_prior",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "quality_type": {
     "currentValue": "DAU",
     "nuid": "b902097a-aced-4b0a-8d6c-0d156206f6cb",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "QDAU",
      "label": null,
      "name": "quality_type",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "QDAU",
        "DAU"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "QDAU",
      "label": null,
      "name": "quality_type",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "QDAU",
        "DAU"
       ]
      }
     }
    },
    "sample_draws": {
     "currentValue": "250",
     "nuid": "5426be00-0749-4a8b-8ae6-9c0858ce63ec",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "750",
      "label": null,
      "name": "sample_draws",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "750",
      "label": null,
      "name": "sample_draws",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "sample_tuneups": {
     "currentValue": "250",
     "nuid": "a65a02aa-735c-4d48-ba36-4c7f6c684f25",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "750",
      "label": null,
      "name": "sample_tuneups",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "750",
      "label": null,
      "name": "sample_tuneups",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "saturation_beta_prior_sigma": {
     "currentValue": "manual",
     "nuid": "44a963f3-e04b-4a87-88d5-5e856dccd5a6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "prior_imps_share_sigma",
      "label": null,
      "name": "saturation_beta_prior_sigma",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "prior_hybrid_share_sigma",
        "prior_imps_share_sigma",
        "prior_spend_share_sigma",
        "prior_contribution_share_sigma",
        "default",
        "manual"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "prior_imps_share_sigma",
      "label": null,
      "name": "saturation_beta_prior_sigma",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "prior_hybrid_share_sigma",
        "prior_imps_share_sigma",
        "prior_spend_share_sigma",
        "prior_contribution_share_sigma",
        "default",
        "manual"
       ]
      }
     }
    },
    "trend_type": {
     "currentValue": "month",
     "nuid": "f6fabaf3-7e94-4804-ac86-479baa42aeab",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "annual",
      "label": null,
      "name": "trend_type",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "month",
        "annual"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "annual",
      "label": null,
      "name": "trend_type",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "month",
        "annual"
       ]
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
