{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7fc7062-d3ee-4c23-9c4f-883d26695887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "(mmm_time_slice_cross_validation)=\n",
    "# Time-Slice-Cross-Validation and Parameter Stability\n",
    "\n",
    "Perform time-slice cross validation for a media mix model. This is an important step to evaluate the stability and quality of the model. We not only look into out of sample predictions but also the stability of the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f4c276e-5308-4170-941c-80337fe276ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a9ced15-c5f8-49a8-87a7-796877ceef68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import numpyro\n",
    "import pprint\n",
    "from itertools import combinations\n",
    "import matplotlib.dates as mdates\n",
    "import ray\n",
    "import time\n",
    "import mlflow\n",
    "\n",
    "from pymc_marketing.metrics import crps\n",
    "from pymc_marketing.mmm import (\n",
    "    MMM,\n",
    "    GeometricAdstock,\n",
    "    LogisticSaturation,\n",
    ")\n",
    "from pymc_marketing.mmm.utils import apply_sklearn_transformer_across_dim\n",
    "from pymc_marketing.prior import Prior\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 7]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "# dropdown widget\n",
    "dbutils.widgets.dropdown(\"quality_type\", \"QDAU\", [\"QDAU\", \"DAU\"])\n",
    "dbutils.widgets.dropdown(\"audience\", \"new_winback\", [\"new_winback\", \"returning\"])\n",
    "\n",
    "# databricks job parameters\n",
    "dbutils.widgets.dropdown(\"intercept_prior\", \"dist=TruncatedNormal_mu=0.5_sigma=0.05_lower=0.4_upper=0.7\", [\"dist=TruncatedNormal_mu=0.5_sigma=0.05_lower=0.4_upper=0.7\", \"dist=TruncatedNormal_mu=0.2_sigma=0.1_lower=0.1\",\"dist=TruncatedNormal_mu=0.3_sigma=0.1_lower=0.1\", \"dist=TruncatedNormal_mu=0.4_sigma=0.1_lower=0.1\", \"dist=TruncatedNormal_mu=0.5_sigma=0.1_lower=0.1\", \"dist=HalfNormal_sigma=1\"])\n",
    "dbutils.widgets.dropdown(\"adstock_alpha_prior\", 'uniform', ['spend_imps_ratio', 'contrib_imps_ratio', 'uniform', 'uniform_beta_3', 'spend_efficiency_ratio'])\n",
    "dbutils.widgets.dropdown(\"gamma_control_prior\", \"dist=Normal_mu=0_sigma=.05_dims=control\", [\"dist=Normal_mu=0_sigma=.025_dims=control\", \"dist=Normal_mu=0_sigma=.03_dims=control\", \"dist=Normal_mu=0_sigma=.05_dims=control\", \"dist=Normal_mu=0_sigma=.1_dims=control\"])\n",
    "dbutils.widgets.dropdown(\"saturation_beta_prior_sigma\", \"prior_imps_share_sigma\", [\"prior_imps_share_sigma\", \"prior_spend_share_sigma\", \"prior_contribution_share_sigma\", \"prior_spend_efficiency_sigma\"])\n",
    "\n",
    "dbutils.widgets.text(\"sample_tuneups\", \"750\")\n",
    "tuneups = int(dbutils.widgets.get(\"sample_tuneups\"))\n",
    "\n",
    "dbutils.widgets.text(\"sample_draws\", \"750\")\n",
    "draws = int(dbutils.widgets.get(\"sample_draws\"))\n",
    "\n",
    "dbutils.widgets.dropdown(\"trend_type\", \"annual\", [\"month\", \"annual\"])\n",
    "trend_type = (dbutils.widgets.get(\"trend_type\"))\n",
    "\n",
    "dbutils.widgets.text(\"chain\", \"4\")\n",
    "chain = int(dbutils.widgets.get(\"chain\"))\n",
    "\n",
    "intercept_prior = dbutils.widgets.get(\"intercept_prior\")\n",
    "adstock_alpha_prior = dbutils.widgets.get(\"adstock_alpha_prior\")\n",
    "gamma_control_prior = dbutils.widgets.get(\"gamma_control_prior\")\n",
    "saturation_beta_prior_sigma = dbutils.widgets.get(\"saturation_beta_prior_sigma\")\n",
    "audience = dbutils.widgets.get(\"audience\")\n",
    "quality_type = dbutils.widgets.get(\"quality_type\")\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07330cd5-ebb5-4e05-a9d4-30b158decd0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "adstock_alpha_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c061f92-8717-4026-a24c-bc44c673cb52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Initiate mlflow logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06280bc0-b1a8-412f-80a6-85c65329318c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"quality_type\", quality_type)\n",
    "mlflow.log_param(\"audience\", audience)\n",
    "mlflow.log_param(\"intercept_prior\", intercept_prior)\n",
    "mlflow.log_param(\"adstock_alpha_prior\", adstock_alpha_prior)\n",
    "mlflow.log_param(\"gamma_control_prior\", gamma_control_prior)\n",
    "mlflow.log_param(\"saturation_beta_prior_sigma\", saturation_beta_prior_sigma)\n",
    "mlflow.log_param(\"trend_type\", trend_type)\n",
    "mlflow.log_param(\"sample_tuneups\", tuneups)\n",
    "mlflow.log_param(\"sample_draws\", draws)\n",
    "mlflow.log_param(\"chain\", chain)\n",
    "\n",
    "import pymc_marketing.mlflow\n",
    "\n",
    "pymc_marketing.mlflow.autolog()\n",
    "\n",
    "run = mlflow.active_run()\n",
    "if run:\n",
    "    print(\"Active run name:\", run.info.run_name)\n",
    "else:\n",
    "    print(\"⚠️ No active MLflow run.\")\n",
    "\n",
    "# Add a description of the experiment\n",
    "description = \"Even more restrictive gamma control (sd=.03)\"\n",
    "mlflow.set_tag(\"mlflow.note.content\", description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da666e92-db7a-4779-b530-6f3751c11d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "From the widgets or job parameters, we can customize the hyperpriors space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c4dfe75-57d6-43b4-8b50-4a1c94bd70a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_prior_config_string(config_str: str) -> Prior:\n",
    "    \"\"\"\n",
    "    Parse a string like 'dist=Normal_mu=0.5_sigma=0.1' into a Prior object.\n",
    "    Supports flat (non-nested) prior definitions.\n",
    "    \"\"\"\n",
    "    if not config_str:\n",
    "        return \"\"\n",
    "\n",
    "    parts = config_str.strip().split(\"_\")\n",
    "    config = {}\n",
    "\n",
    "    for part in parts:\n",
    "        if \"=\" not in part:\n",
    "            raise ValueError(f\"Invalid part '{part}': expected key=value format\")\n",
    "        key, value = part.split(\"=\", 1)\n",
    "        config[key] = value\n",
    "\n",
    "    if \"dist\" not in config:\n",
    "        raise ValueError(\"Missing 'dist' key in prior definition.\")\n",
    "\n",
    "    dist_name = config.pop(\"dist\")\n",
    "    # Convert numeric strings to float\n",
    "    kwargs = {\n",
    "        k: float(v) if v.replace(\".\", \"\", 1).isdigit() or \"e\" in v.lower() else v\n",
    "        for k, v in config.items()\n",
    "    }\n",
    "\n",
    "    return Prior(dist_name, **kwargs)\n",
    "\n",
    "# all these are prior functions, rather than giving a fixed value, we provide range of values like beta, gamma , norman, half normal\n",
    "#Normal: For parameters where we expect values to cluster around a mean.\n",
    "#Half-Normal: For parameters where we want to enforce positivity.\n",
    "#Beta: For parameters which are constrained between 0 and 1.\n",
    "#Gamma: For parameters that are positive and skewed.\n",
    "# in our example we have provided intercept and gamma\n",
    "#The above function is used to parse the intercept_prior (base line of target value) and gamma control prior(control variable that ) string and return a Prior object\n",
    "model_config_param = {\n",
    "    \"intercept\": parse_prior_config_string(intercept_prior),\n",
    "    \"gamma_control\": parse_prior_config_string(gamma_control_prior),\n",
    "}\n",
    "print(model_config_param)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0957e8a1-9109-4f4c-89e1-0e50d180227e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "seed: int = sum(map(ord, \"mmm\"))\n",
    "rng: np.random.Generator = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc7b0a6-78cb-43f7-9ce9-00066168c128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db2922b1-5d14-455d-b21d-37a0208d7e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Read data of daily active users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26b4d2de-6477-42c2-9c7f-62dfaab0b03d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import daily active users\n",
    "dau_df = spark.sql(\"SELECT * FROM datascience.shua.mmm_dau\").toPandas()\n",
    "\n",
    "# Lowercase all column names\n",
    "dau_df.columns = [col.lower() for col in dau_df.columns]\n",
    "\n",
    "# Replace 'winback' with 'new' in the start_type column\n",
    "dau_df[\"start_type\"] = dau_df[\"start_type\"].replace(\"WINBACK\", \"NEW\")\n",
    "\n",
    "# Group by date, start_type, and quality_flag, summing daily_users\n",
    "dau_df = (\n",
    "    dau_df.groupby([\"status_date\", \"start_type\"], as_index=False)[\"daily_users\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"status_date\": \"day_dt\"})\n",
    ")\n",
    "\n",
    "# pivot by start_type\n",
    "dau_df = pd.pivot_table(dau_df, index=\"day_dt\", columns=\"start_type\", values=\"daily_users\").reset_index()\n",
    "\n",
    "# rename columns\n",
    "dau_df.columns = [\"day_dt\", \"y_new_winback_dau\", \"y_returning_dau\"]\n",
    "\n",
    "dau_df.columns = [col.lower() for col in dau_df.columns]\n",
    "dau_df['day_dt'] = pd.to_datetime(dau_df[\"day_dt\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "783d5cd0-9bcf-4592-8f1b-79aef6ca4b42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Read mmm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ffebca-90ae-429a-8eff-1d6dc49b0ca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = spark.sql(\"select * from datascience.shua.mmm_data_wide_042425\").toPandas()\n",
    "raw_df[\"day_dt\"] = pd.to_datetime(raw_df[\"day_dt\"], errors=\"coerce\")\n",
    "# join with dau_df\n",
    "raw_df = pd.merge(\n",
    "    raw_df,\n",
    "    dau_df,\n",
    "    on=\"day_dt\",\n",
    "    how=\"left\",\n",
    ")\n",
    "# rename a column\n",
    "raw_df = raw_df.rename(columns={\"y_new_winback\": \"y_new_winback_qdau\", \"y_returning\": \"y_returning_qdau\"})\n",
    "display(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21eb98a7-05e9-4a18-ab26-f80ca7051384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define important events/anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9296237f-4331-4d13-a147-03c97204eaa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Map the event dictionary of notable events\n",
    "# look for holidays that start on weekends and could have a holiday effect on the following Monday\n",
    "events_dict = {\n",
    "    \"2023-01-01\": \"ny23_2023\",\n",
    "    \"2023-01-16\": \"mlk_2023\",\n",
    "    \"2023-02-12\": \"superbowl_2023\",\n",
    "    \"2023-02-13\": \"day_after_superbowl_2023\",\n",
    "    \"2023-02-20\": \"pres_2023\",\n",
    "    \"2023-03-24\": \"anomaly_3_24_23\",  # roku stitcher bug start\n",
    "    \"2023-03-25\": \"anomaly_3_25_23\",\n",
    "    \"2023-03-26\": \"anomaly_3_26_23\",\n",
    "    \"2023-03-27\": \"anomaly_3_27_23\",\n",
    "    \"2023-03-28\": \"anomaly_3_28_23\",  # roku stitcher bug end\n",
    "    \"2023-05-29\": \"mem_2023\",\n",
    "    \"2023-06-19\": \"juneteenth_2023\",\n",
    "    \"2023-07-04\": \"ind_2023\",\n",
    "    \"2023-09-04\": \"lab_2023\",\n",
    "    \"2023-11-23\": \"thanks_2023\",\n",
    "    \"2023-11-24\": \"blk_fri_2023\",\n",
    "    \"2023-12-17\": \"anomaly_12_17_23\",  # bug\n",
    "    \"2023-12-20\": \"anomaly_12_20_2023\",  # bug\n",
    "    \"2023-12-21\": \"anomaly_12_21_2023\",  # bug\n",
    "    \"2023-12-23\": \"day_b4_xmas_eve_2023\",\n",
    "    \"2023-12-24\": \"xmas_eve_2023\",\n",
    "    \"2023-12-25\": \"xmas_2023\",\n",
    "    \"2023-12-26\": \"xmas_day_after_2023\",\n",
    "    \"2023-12-30\": \"day_b4_nye_2023\",\n",
    "    \"2023-12-31\": \"nye_2023\",\n",
    "    \"2024-01-01\": \"ny24_2024\",\n",
    "    \"2024-01-15\": \"mlk_2024\",\n",
    "    \"2024-02-11\": \"anomaly_2_11_2024\",  # bug\n",
    "    \"2024-02-19\": \"pres_2024\",\n",
    "    \"2024-03-12\": \"anomaly_3_11_2024\",  # bug\n",
    "    \"2024-03-12\": \"anomaly_3_12_2024\",  # bug\n",
    "    \"2024-03-13\": \"anomaly_3_13_2024\",  # bug\n",
    "    \"2024-03-14\": \"anomaly_3_14_2024\",  # bug\n",
    "    \"2024-03-15\": \"anomaly_3_15_2024\",  # bug\n",
    "    \"2024-05-27\": \"mem_2024\",\n",
    "    \"2024-06-19\": \"juneteenth_2024\",\n",
    "    \"2024-07-04\": \"ind_2024\",\n",
    "    \"2024-09-02\": \"lab_2024\",\n",
    "    \"2024-11-28\": \"thanks_2024\",\n",
    "    \"2024-11-29\": \"blk_fri_2024\",\n",
    "    \"2024-12-24\": \"xmas_eve_2024\",\n",
    "    \"2024-12-25\": \"xmas_2024\",\n",
    "    \"2024-12-31\": \"nye_2024\",\n",
    "    \"2024-02-11\": \"superbowl_2024\",\n",
    "    \"2024-02-12\": \"day_after_superbowl_2024\",\n",
    "}\n",
    "\n",
    "# Map the event names to the 'event' column\n",
    "raw_df['event'] = pd.to_datetime(raw_df['day_dt'], errors='coerce').dt.strftime('%Y-%m-%d').map(events_dict)\n",
    "\n",
    "# One-hot encode events\n",
    "event_dummies = pd.get_dummies(raw_df[\"event\"], prefix=\"event_\", prefix_sep=\"\")\n",
    "event_dummies = (event_dummies >= 1).astype(int)\n",
    "daily_df = pd.concat([raw_df, event_dummies], axis=1).sort_values(\"day_dt\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# One-hot encode day of week\n",
    "dow_dummies = pd.get_dummies(daily_df[\"day_dt\"].dt.dayofweek, prefix=\"dow_\", prefix_sep=\"\")\n",
    "dow_dummies = (dow_dummies >= 1).astype(int)\n",
    "daily_df = pd.concat([daily_df, dow_dummies], axis=1)\n",
    "display(daily_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "518cbbca-cbb1-4771-844a-e08d3249ea73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Plot of y_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f5d449c-88d8-4d6d-be0a-c2915996e4a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(20, 10), sharex=True)\n",
    "\n",
    "# Define variables to plot\n",
    "metrics = [(\"y_new_winback_dau\", \"New Winback Daily\"), (\"y_new_winback_qdau\", \"New Winback Quality Daily\"), (\"y_returning_dau\", \"Returning Daily\"), (\"y_returning_qdau\", \"Returning Quality Daily\")]\n",
    "\n",
    "for ax, (col, title) in zip(axes, metrics):\n",
    "    sns.lineplot(data=daily_df, x=\"day_dt\", y=col, color=\"black\", ax=ax)\n",
    "    event_days = daily_df[pd.notna(daily_df[\"event\"])]\n",
    "    ax.plot(event_days[\"day_dt\"], event_days[col], \"ro\", markersize=6, label=\"Event\")\n",
    "    ax.set(title=title, xlabel=\"Month\", ylabel=col)\n",
    "    ax.legend()\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc3cc2ce-2787-46d4-8323-f07f7ba4f896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Add trend control vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32e67dc4-68c6-4dfc-b2af-d6b0cda45190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add annual trend\n",
    "\n",
    "# Extract year\n",
    "daily_df[\"year\"] = daily_df[\"day_dt\"].dt.year\n",
    "\n",
    "# Use groupby cumcount for per-year linear trend\n",
    "daily_df[\"annual_trend_2023\"] = daily_df.groupby(\"year\").cumcount().where(daily_df[\"year\"] == 2023, 0)\n",
    "daily_df[\"annual_trend_2024\"] = daily_df.groupby(\"year\").cumcount().where(daily_df[\"year\"] == 2024, 0)\n",
    "\n",
    "# segmented monthly trend\n",
    "daily_df[\"month\"] = daily_df[\"day_dt\"].dt.to_period(\"M\")\n",
    "\n",
    "# Normalize day of month to [0, 1] scale per month\n",
    "daily_df[\"day_frac\"] = (\n",
    "    daily_df.groupby(\"month\")[\"day_of_month\"]\n",
    "    .transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    ")\n",
    "\n",
    "# One-hot encode month\n",
    "month_dummies = pd.get_dummies(daily_df[\"month\"].astype(str), prefix=\"month\", drop_first=False)\n",
    "\n",
    "# Multiply each dummy by normalized day_frac\n",
    "for col in month_dummies.columns:\n",
    "    daily_df[f\"trend_{col}\"] = month_dummies[col] * daily_df[\"day_frac\"]\n",
    "\n",
    "display(daily_df)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3efa850-d994-4951-8fb6-9f3f5e694db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Specify Variables in the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c84cd69-8bb8-45cc-9307-3ad67580584b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. control variables\n",
    "# We just keep the holidays columns\n",
    "control_columns = [col for col in daily_df.columns if col.startswith(\"event_\") or col.startswith(\"dow_\") or col.startswith(f\"trend_{trend_type}\")]\n",
    "print(\"Control columns:\", control_columns)\n",
    "\n",
    "# 2. media variables\n",
    "# Impressions only for now\n",
    "media_activity = \"imps\" # must be \"imps\" or \"spend\"\n",
    "channel_columns_raw = sorted(\n",
    "    [\n",
    "        col\n",
    "        for col in daily_df.columns\n",
    "        if (\"channel_\" in col) & (f\"_{media_activity}\" in col)\n",
    "    ]\n",
    ")\n",
    "\n",
    "imps_channel_mapping = {\n",
    "    \"channel_app_imps\": \"MACS App\",\n",
    "    \"channel_hmi_audio_imps\": \"HMI Audio\",\n",
    "    \"channel_hmi_display_imps\": \"HMI Display\",\n",
    "    \"channel_hmi_linear_imps\": \"HMI Linear\",\n",
    "    \"channel_hmi_ooh_imps\": \"HMI OOH\",\n",
    "    \"channel_hmi_video_imps\": \"HMI Digital Video\",\n",
    "    \"channel_influencer_imps\": \"Influencer\",\n",
    "    \"channel_macs_programmatic_imps\": \"MACS Programmatic\",\n",
    "    \"channel_macs_social_imps\": \"MACS Social\",\n",
    "    \"channel_oo_display_imps\": \"OO Display\",\n",
    "    \"channel_oo_linear_imps\": \"OO Linear\",\n",
    "    \"channel_oo_video_imps\": \"OO Video\",\n",
    "    \"channel_paid_search_imps\": \"Paid Search\",\n",
    "    \"channel_partnerships_imps\": \"Partnerships\",\n",
    "    \"channel_podcast_one_imps\": \"Podcast One Audio\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "spend_channel_mapping = {\n",
    "    \"channel_app_spend\": \"MACS App\",\n",
    "    \"channel_hmi_audio_spend\": \"HMI Audio\",\n",
    "    \"channel_hmi_display_spend\": \"HMI Display\",\n",
    "    \"channel_hmi_linear_spend\": \"HMI Linear\",\n",
    "    \"channel_hmi_ooh_spend\": \"HMI OOH\",\n",
    "    \"channel_hmi_video_spend\": \"HMI Digital Video\",\n",
    "    \"channel_influencer_spend\": \"Influencer\",\n",
    "    \"channel_macs_programmatic_spend\": \"MACS Programmatic\",\n",
    "    \"channel_macs_social_spend\": \"MACS Social\",\n",
    "    \"channel_oo_display_spend\": \"OO Display\",\n",
    "    \"channel_oo_linear_spend\": \"OO Linear\",\n",
    "    \"channel_oo_video_spend\": \"OO Video\",\n",
    "    \"channel_paid_search_spend\": \"Paid Search\",\n",
    "    \"channel_partnerships_spend\": \"Partnerships\",\n",
    "    \"channel_podcast_one_spend\": \"Podcast One Audio\",\n",
    "}\n",
    "\n",
    "if media_activity == \"imps\":\n",
    "    channel_mapping = imps_channel_mapping\n",
    "    channel_columns = sorted(list(imps_channel_mapping.keys()))\n",
    "    \n",
    "else:\n",
    "    channel_mapping = spend_channel_mapping\n",
    "    channel_columns = sorted(list(spend_channel_mapping.keys()))\n",
    "    \n",
    "channel_names = sorted(list(imps_channel_mapping.values()))\n",
    "sorted_spend_channel_names =  {k: spend_channel_mapping[k] for k in sorted(spend_channel_mapping, key=spend_channel_mapping.get)}\n",
    "\n",
    "print(\"Channel columns:\", channel_names)\n",
    "print(\"Spend columns:\", list(sorted_spend_channel_names.keys()))\n",
    "\n",
    "\n",
    "\n",
    "# 3. y variable\n",
    "y_var = f\"y_{audience}_{quality_type}\".lower()\n",
    "print(\"y_var:\", y_var)\n",
    "\n",
    "data_df = daily_df[[\"day_dt\", y_var, *channel_columns, *control_columns, *sorted_spend_channel_names]]\n",
    "data_df = data_df.rename(columns=channel_mapping)\n",
    "\n",
    "# reorder the channel columns in data_df only\n",
    "sorted_channel_columns = sorted(channel_mapping.values())\n",
    "data_df = data_df[['day_dt', y_var, *sorted_channel_columns, *control_columns, *sorted_spend_channel_names]]\n",
    "\n",
    "# 4. Date column\n",
    "data_df[\"day_dt\"] = pd.to_datetime(data_df[\"day_dt\"])\n",
    "date_column = \"day_dt\"\n",
    "data_df.columns\n",
    "\n",
    "# remove event_0 column\n",
    "if \"event_0\" in data_df.columns:\n",
    "    data_df = data_df.drop(columns=[\"event_0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06a4db6a-c20f-42f1-8211-a174177bc389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Specify Time-Slice-Cross-Validation Strategy\n",
    "\n",
    "The main idea of the time-slice cross validation process is to fit the model on a time slice of the data and then evaluate it on the next time slice. We repeat this process for each time slice of the data. As we want to simulate a production-like environment where we enlarge our training data over time, we make the time-slice size grow over time.\n",
    "\n",
    "Following the strategy of the example notebook, we use the media share of each channel to set the prior standard deviation of the beta parameters. We need to compute this share for EACH training time slice independently.\n",
    "\n",
    "```{admonition} Data Leakage\n",
    ":class: warning\n",
    "\n",
    "It is very important to avoid data leakage when performing time-slice cross validation. This means that the model should not see any training data from the future. This also includes any data pre-processing steps!\n",
    "\n",
    "For example, as mentioned above, we need to compute the media share for each training time slice independently if we want to avoid data leakage. Other sources of data leakage include using a global feature for thr trend component. In our case, we simply use an increasing variable `t` so we are safe as we just increase it by one for each time slice.\n",
    "```\n",
    "\n",
    "We wrap the main steps of the training procedure in a set of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c259dc-db77-44ca-bb37-2de55c62af8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get shared channel names, ensuring consistent ordering\n",
    "channels = sorted(channel_names)\n",
    "\n",
    "# Get imps_shares and spend_shares\n",
    "imps_shares = (\n",
    "    data_df.melt(value_vars=channels, var_name=\"channel\", value_name=\"imps\")\n",
    "    .groupby(\"channel\", as_index=False)\n",
    "    .agg({\"imps\": \"sum\"})\n",
    "    .sort_values(by=\"channel\")\n",
    "    .assign(imps_share=lambda x: x[\"imps\"] / x[\"imps\"].sum())[\"imps_share\"]\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "spend_shares = (\n",
    "    daily_df.melt(value_vars=list(spend_channel_mapping.keys()), var_name=\"channel\", value_name=\"spend\")\n",
    "    .groupby(\"channel\", as_index=False)\n",
    "    .agg({\"spend\": \"sum\"})\n",
    "    .sort_values(by=\"channel\")\n",
    "    .assign(spend_share=lambda x: x[\"spend\"] / x[\"spend\"].sum())[\"spend_share\"]\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "# Bar plot settings\n",
    "x = np.arange(len(channels))  # label locations\n",
    "width = 0.4  # bar width\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width / 2, imps_shares, width, label=\"Imps Share\")\n",
    "plt.bar(x + width / 2, spend_shares, width, label=\"Spend Share\")\n",
    "\n",
    "# Add labels\n",
    "for i in range(len(channels)):\n",
    "    plt.text(x[i] - width / 2, imps_shares[i] + 0.01, str(round(imps_shares[i], 2)), ha=\"center\", va=\"bottom\", size=10)\n",
    "    plt.text(x[i] + width / 2, spend_shares[i] + 0.01, str(round(spend_shares[i], 2)), ha=\"center\", va=\"bottom\", size=10)\n",
    "\n",
    "plt.xticks(x, channels, rotation=45)\n",
    "plt.ylabel(\"Share\")\n",
    "plt.title(\"Impressions vs Spend Share by Channel\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c03b2c7-a5d6-44ff-8612-3d311af0275a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Adstock Alpha Tuning##\n",
    "- Beta Distribution is used\n",
    "- Controls the adstock effect, determining how much the impact of marketing spend decays over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2282079-886f-4db5-ba8f-6f3e0c2d30e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def suggest_adstock_alpha_prior(\n",
    "    contrib_share,\n",
    "    imps_share=1,\n",
    "    min_alpha=1,\n",
    "    max_alpha=5,\n",
    "    efficiency_means_short_memory=True,\n",
    "    spend_efficiency = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Suggests a Beta prior for adstock_alpha based on contribution/impression efficiency.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    contrib_share : float\n",
    "        Fractional contribution share of the channel (0 to 1).\n",
    "    imps_share : float\n",
    "        Fractional impression share of the channel (0 to 1).\n",
    "    min_alpha : float\n",
    "        Lower bound for Beta(alpha, beta).\n",
    "    max_alpha : float\n",
    "        Upper bound for Beta(alpha, beta).\n",
    "    efficiency_means_short_memory : bool\n",
    "        If True, high efficiency implies short memory (low alpha).\n",
    "        If False, high efficiency implies long memory (high alpha).\n",
    "    spend_efficiency : bool\n",
    "        If True, use spend efficiency metrics for adstock prior. \n",
    "        If False, use contribution/impression efficiency for adstock prior.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Prior\n",
    "        A Beta distribution for adstock_alpha.\n",
    "    float\n",
    "        The efficiency ratio used in scaling.\n",
    "    \"\"\"\n",
    "    if imps_share == 0:\n",
    "        return Prior(\"Beta\", alpha=1, beta=1), np.inf  # Uninformative\n",
    "\n",
    "    \n",
    "    if spend_efficiency:\n",
    "        efficiency = contrib_share\n",
    "        scaled_value = (np.tanh(contrib_share) + 1) / 2  # range: [0, 1]\n",
    "        if efficiency_means_short_memory:\n",
    "            # High efficiency → short memory → lower alpha\n",
    "            scaled_alpha = max_alpha - (max_alpha - min_alpha) * scaled_value\n",
    "        else:\n",
    "            # High efficiency → long memory → higher alpha\n",
    "            scaled_alpha = min_alpha + (max_alpha - min_alpha) * scaled_value\n",
    "        beta_param = max(1, max_alpha + min_alpha -  scaled_value)   \n",
    "    else:\n",
    "        # Define effectiveness ratio\n",
    "        efficiency = contrib_share / imps_share\n",
    "        scaled = np.clip(efficiency, 0, 5)\n",
    "        # Smooth transformation: scale α depending on the desired interpretation\n",
    "        scaled_value = (np.tanh(scaled) + 1) / 2  # range: [0, 1]\n",
    "        if efficiency_means_short_memory:\n",
    "                # High efficiency → short memory → lower alpha\n",
    "                scaled_alpha = max_alpha - (max_alpha - min_alpha) * (scaled_value)\n",
    "        else:\n",
    "                # High efficiency → long memory → higher alpha\n",
    "                scaled_alpha = min_alpha + (max_alpha - min_alpha) * (scaled_value)\n",
    "\n",
    "        beta_param = max(1, max_alpha + min_alpha - scaled_value)\n",
    "\n",
    "    return {\"prior\": Prior(\"Beta\", alpha=round(scaled_alpha, 2), beta=round(beta_param, 2)),\n",
    "            \"alpha\": scaled_alpha,\n",
    "            \"beta\": beta_param,\n",
    "            \"ratio\": efficiency}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b20f4c7-0baa-4cbb-9b15-f5d11333e7b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- ######Low values of alpha have little impact and are suitable for channels which have a direct response e.g. paid social performance ads with a direct call to action aimed at prospects who have already visited your website. We can try for display ones\n",
    "- ######Higher values of alpha have a stronger impact and are suitable for channels which have a longer term effect e.g. brand building videos with no direct call to action aimed at a broad range of prospects. We can try for video ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d553a55c-79ef-4832-938f-1460c4df4565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#adstock_alpha_prior = \"spend_efficiency\"\n",
    "#audience = 'new_winback'\n",
    "new_winback_channel_contributions_12_1_23 = {\n",
    "                    \"HMI Audio\": 0.0122,\n",
    "                    \"HMI Digital Video\": 0.0237,\n",
    "                    \"HMI Display\": 0.0595,\n",
    "                    \"HMI Linear\": 0.0327,\n",
    "                    \"HMI OOH\": 0.0717,\n",
    "                    \"Influencer\": 0.0002,\n",
    "                    \"MACS App\": 0.0497,\n",
    "                    \"MACS Programmatic\": 0.0281,\n",
    "                    \"MACS Social\": 0.0541,\n",
    "                    \"OO Display\": 0.0067,\n",
    "                    \"OO Linear\": 0.0828,\n",
    "                    \"OO Video\": 0.1046,\n",
    "                    \"Paid Search\": 0.1387,\n",
    "                    \"Partnerships\": 0.2712,\n",
    "                    \"Podcast One Audio\": 0.0439,\n",
    "                }\n",
    "if adstock_alpha_prior == \"spend_imps_ratio\":\n",
    "    channel_data = {\n",
    "        channel: {\"contrib\": contrib, \"imps\": imps}\n",
    "        for channel, contrib, imps in zip(channels, spend_shares, imps_shares)\n",
    "    }\n",
    "    adstock_alpha_prior_dict = {\n",
    "    channel: suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"prior\"]\n",
    "    for channel, v in channel_data.items()\n",
    "    }\n",
    "    efficiency_prior_dict = {\n",
    "    channel: suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"ratio\"]\n",
    "    for channel, v in channel_data.items()\n",
    "    }\n",
    "\n",
    "    alpha_prior_list = [\n",
    "        suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"alpha\"]\n",
    "        for channel, v in channel_data.items()\n",
    "    ]\n",
    "\n",
    "    beta_prior_list = [\n",
    "        suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"beta\"]\n",
    "        for channel, v in channel_data.items()\n",
    "    ]\n",
    "    pprint.pprint(adstock_alpha_prior_dict)\n",
    "    pprint.pprint(efficiency_prior_dict)\n",
    "    pprint.pprint(alpha_prior_list)\n",
    "    pprint.pprint(beta_prior_list)\n",
    "\n",
    "elif adstock_alpha_prior == \"contrib_imps_ratio\":\n",
    "    channel_data = {\n",
    "        channel: {\"contrib\": contrib, \"imps\": imps}\n",
    "        for channel, contrib, imps in zip(channels, list(new_winback_channel_contributions_12_1_23.values()), imps_shares)\n",
    "    }\n",
    "    adstock_alpha_prior_dict = {\n",
    "    channel: suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"prior\"]\n",
    "    for channel, v in channel_data.items()\n",
    "    }\n",
    "    efficiency_prior_dict = {\n",
    "    channel: suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"ratio\"]\n",
    "    for channel, v in channel_data.items()\n",
    "    }\n",
    "\n",
    "    alpha_prior_list = [\n",
    "        suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"alpha\"]\n",
    "        for channel, v in channel_data.items()\n",
    "    ]\n",
    "\n",
    "    beta_prior_list = [\n",
    "        suggest_adstock_alpha_prior(v[\"contrib\"], v[\"imps\"])[\"beta\"]\n",
    "        for channel, v in channel_data.items()\n",
    "    ]\n",
    "    pprint.pprint(adstock_alpha_prior_dict)\n",
    "    pprint.pprint(efficiency_prior_dict)\n",
    "    pprint.pprint(alpha_prior_list)\n",
    "    pprint.pprint(beta_prior_list)\n",
    "elif adstock_alpha_prior == \"spend_efficiency_ratio\":\n",
    "    if audience == 'returning':\n",
    "                spend_efficiency = {\n",
    "                    \"HMI Digital Display\" : 0.062,\n",
    "                    \"HMI Digital Video\" : 0.097,\n",
    "                    \"HMI Linear\" : 0.011,\n",
    "                    \"HMI OOH\": 0.011,\n",
    "                    \"HMI Terrestrial Audio\" : 0.034,\n",
    "                    \"Influencer\" : 0.539,\n",
    "                    \"MACS App\" : 0.007,\n",
    "                    \"MACS Programmatic\" : 0.005,\n",
    "                    \"MACS Social\" : 0.004,\n",
    "                    \"O&O Display\" : 0.227,\n",
    "                    \"O&O Linear\" : 0.0003,\n",
    "                    \"O&O Video\" : 0.0007,\n",
    "                    \"Paid Search\" : 0.0007,\n",
    "                    \"Partnerships\" : 0.0001,\n",
    "                    \"Podcast One Audio\" : 0.0001,\n",
    "                }    \n",
    "    elif audience == 'new_winback':\n",
    "                spend_efficiency = {\n",
    "                    \"HMI Digital Display\": 0.158,\n",
    "                    \"HMI Digital Video\": 0.028,\n",
    "                    \"HMI Linear\": 0.014,\n",
    "                    \"HMI OOH\": 0.039,\n",
    "                    \"HMI Terrestrial Audio\": 0.038,\n",
    "                    \"Influencer\": 0.070,\n",
    "                    \"MACS App\": 0.058,\n",
    "                    \"MACS Programmatic\": 0.031,\n",
    "                    \"MACS Social\": 0.038,\n",
    "                    \"O&O Display\": 0.278,\n",
    "                    \"O&O Linear\": 0.015,\n",
    "                    \"O&O Video\": 0.074,\n",
    "                    \"Paid Search\": 0.116,\n",
    "                    \"Partnerships\": 0.033,\n",
    "                    \"Podcast One Audio\": 0.012,\n",
    "                }\n",
    "    channel_data = {channel: v for channel, v in spend_efficiency.items()}\n",
    "  \n",
    "    adstock_alpha_prior_dict = {\n",
    "    channel: suggest_adstock_alpha_prior(v, spend_efficiency = True)[\"prior\"]\n",
    "    for channel, v in channel_data.items()\n",
    "    }\n",
    "    efficiency_prior_dict = {\n",
    "    channel: suggest_adstock_alpha_prior(v , spend_efficiency = True)[\"ratio\"]\n",
    "    for channel, v in channel_data.items()\n",
    "    }\n",
    "    alpha_prior_list = [\n",
    "    suggest_adstock_alpha_prior(v, spend_efficiency = True)[\"alpha\"]\n",
    "        for channel, v in channel_data.items()\n",
    "    ]\n",
    "\n",
    "    beta_prior_list = [\n",
    "        suggest_adstock_alpha_prior(v, spend_efficiency = True)[\"beta\"]\n",
    "        for channel, v in channel_data.items()\n",
    "    ]\n",
    "    pprint.pprint(adstock_alpha_prior_dict)\n",
    "    pprint.pprint(efficiency_prior_dict)\n",
    "    pprint.pprint(alpha_prior_list)\n",
    "    pprint.pprint(beta_prior_list)    \n",
    "\n",
    "elif adstock_alpha_prior == \"uniform\": #where we expect values to cluster around mean \n",
    "\n",
    "    adstock_alpha_prior_dict = {\n",
    "    channel: Prior(\"Beta\", alpha=1, beta=1, dims=\"channel\")\n",
    "    for channel in channels\n",
    "    }\n",
    "elif adstock_alpha_prior == \"uniform_beta_3\":\n",
    "\n",
    "    adstock_alpha_prior_dict = {\n",
    "    channel: Prior(\"Beta\", alpha=1, beta=3, dims=\"channel\")\n",
    "    for channel in channels\n",
    "    }    \n",
    "\n",
    "    pprint.pprint(adstock_alpha_prior_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f54d50d-92eb-4e7d-9c05-15e06b8977f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save to JSON file\n",
    "import json\n",
    "artifact_path = \"Adstock Beta Dist Priors By Channel.json\"\n",
    "with open(artifact_path, \"w\") as f:\n",
    "    json.dump({k: (v.to_dict() if hasattr(v, 'to_dict') else v) for k, v in adstock_alpha_prior_dict.items()}, f, indent=4)\n",
    "\n",
    "# Log to MLflow\n",
    "mlflow.log_artifact(artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628ff396-e585-4318-b91b-b8e35abb08ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import beta\n",
    "\n",
    "def plot_adstock_alpha_priors_facet(prior_dict, channels_to_plot=None, n_cols=4):\n",
    "    \"\"\"\n",
    "    Plots Beta distributions for adstock_alpha priors per channel using facet grid layout.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prior_dict : dict\n",
    "        Dictionary of {channel: Prior(\"Beta\", alpha=..., beta=...)}\n",
    "    channels_to_plot : list of str, optional\n",
    "        Subset of channels to visualize. If None, plots all.\n",
    "    n_cols : int\n",
    "        Number of columns in the facet grid.\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, 1, 500)\n",
    "    selected_channels = channels_to_plot or list(prior_dict.keys())\n",
    "    n_channels = len(selected_channels)\n",
    "    n_rows = math.ceil(n_channels / n_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 3 * n_rows), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, channel in enumerate(selected_channels):\n",
    "        prior = prior_dict[channel]\n",
    "        a = prior.parameters.get(\"alpha\")\n",
    "        b = prior.parameters.get(\"beta\")\n",
    "\n",
    "        y = beta.pdf(x, a, b)\n",
    "        ax = axes[i]\n",
    "        ax.plot(x, y, label=f\"α={a:.2f}, β={b:.2f}\")\n",
    "        ax.set_title(channel)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "    # Hide unused subplots if any\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"Beta Priors for Adstock α by Channel\", fontsize=16)\n",
    "    fig.supxlabel(\"adstock_alpha\")\n",
    "    fig.supylabel(\"Density\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_adstock_alpha_priors_facet(adstock_alpha_prior_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15836dc3-95ee-43c5-bfeb-d9379f4cbd9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prior Specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67cae917-24d9-4a6c-9d0a-72216c325eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For each fold, we \n",
    "- compute prior sigma for saturation beta based on media activity shares or the last known media contribution of training set (i.e. 12/1/2023 below).\n",
    "- filter out zero columns\n",
    "- reset the model configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85203745-0801-4ef1-bad1-a0ee9dcfc4cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Saturation beta - The marketing spend coefficient, which measures the direct effect of marketing spend on the target variable (e.g. dau or qdau).\n",
    "The half-normal prior is used as it enforces positivity \n",
    "Prior sigma derives below is used in the Saturation beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "648c7e07-7f0c-4a55-b84c-40c8df076503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''saturation_beta_prior_sigma == \"prior_imps_share_sigma\"\n",
    "print(\"imps\", compute_sigma_from_media_activity(data_df, media_activity=\"imps\"))\n",
    "\n",
    "saturation_beta_prior_sigma == \"prior_spend_share_sigma\"\n",
    "print(\"spend\", compute_sigma_from_media_activity(data_df, media_activity=\"imps\"))\n",
    "\n",
    "saturation_beta_prior_sigma == \"prior_contribution_share_sigma\"\n",
    "print(\"contribution\", [v * len(returning_channel_contributions_12_1_23) for v in returning_channel_contributions_12_1_23.values()])\n",
    "spend_efficiency = {\n",
    "                    \"HMI Digital Display\": 0.158,\n",
    "                    \"HMI Digital Video\": 0.028,\n",
    "                    \"HMI Linear\": 0.014,\n",
    "                    \"HMI OOH\": 0.039,\n",
    "                    \"HMI Terrestrial Audio\": 0.038,\n",
    "                    \"Influencer\": 0.070,\n",
    "                    \"MACS App\": 0.058,\n",
    "                    \"MACS Programmatic\": 0.031,\n",
    "                    \"MACS Social\": 0.038,\n",
    "                    \"O&O Display\": 0.278,\n",
    "                    \"O&O Linear\": 0.015,\n",
    "                    \"O&O Video\": 0.074,\n",
    "                    \"Paid Search\": 0.116,\n",
    "                    \"Partnerships\": 0.033,\n",
    "                    \"Podcast One Audio\": 0.012,\n",
    "                }\n",
    "saturation_beta_prior_sigma == \"prior_spend_efficiency_sigma\"\n",
    "print(\"spend_efficiency\", [v  for v in spend_efficiency.values()])\n",
    "\n",
    "saturation_beta_prior_sigma == \"prior_spend_share_sigma\"\n",
    "print(\"spend\", compute_sigma_from_media_activity(data_df, media_activity=\"imps\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4050d2b-52ab-419f-9d9f-6521b8953bf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_sigma_from_media_activity(\n",
    "    X: pd.DataFrame, \n",
    "    media_activity: str = \"imps\"\n",
    ") -> list[float]:\n",
    "    \"\"\"Compute the prior standard deviation of the beta parameters from the activity share of each channel.\"\"\"\n",
    "\n",
    "    if media_activity == \"imps\":\n",
    "        media_cols = list(channel_names)\n",
    "    elif media_activity == \"spend\":\n",
    "        media_cols = list(sorted_spend_channel_names)\n",
    "\n",
    "    n_channels = len(media_cols)\n",
    "    total_activity_per_channel = data_df[media_cols].sum(axis=0)\n",
    "    activity_share = total_activity_per_channel / total_activity_per_channel.sum()\n",
    "    prior_sigma = n_channels * activity_share.to_numpy()\n",
    "    return prior_sigma.tolist()\n",
    "\n",
    "def get_mmm(X: pd.DataFrame, channel_names: list[str], control_columns: list[str], date_column: str) -> MMM:\n",
    "    \"\"\"Specify the model.\"\"\"\n",
    "    # specify the prior sigma for saturation beta\n",
    "    if saturation_beta_prior_sigma == \"prior_imps_share_sigma\":\n",
    "            prior_sigma = compute_sigma_from_media_activity(data_df, media_activity=\"imps\")\n",
    "    elif saturation_beta_prior_sigma == \"prior_spend_share_sigma\":\n",
    "            prior_sigma = compute_sigma_from_media_activity(data_df, media_activity=\"spend\")      \n",
    "    elif saturation_beta_prior_sigma == \"prior_contribution_share_sigma\":\n",
    "            if audience == 'returning':\n",
    "                returning_channel_contributions_12_1_23 = {\n",
    "                    \"HMI Audio\": 0.0075,\n",
    "                    \"HMI Digital Video\": 0.0231,\n",
    "                    \"HMI Display\": 0.0549,\n",
    "                    \"HMI Linear\": 0.0223,\n",
    "                    \"HMI OOH\": 0.0622,\n",
    "                    \"Influencer\": 0.0003,\n",
    "                    \"MACS App\": 0.0511,\n",
    "                    \"MACS Programmatic\": 0.0342,\n",
    "                    \"MACS Social\": 0.0636,\n",
    "                    \"OO Display\": 0.0043,\n",
    "                    \"OO Linear\": 0.0904,\n",
    "                    \"OO Video\": 0.0960,\n",
    "                    \"Paid Search\": 0.1401,\n",
    "                    \"Partnerships\": 0.2875,\n",
    "                    \"Podcast One Audio\": 0.0803,\n",
    "                }\n",
    "                prior_sigma = [v * len(returning_channel_contributions_12_1_23) for v in returning_channel_contributions_12_1_23.values()]\n",
    "            elif audience == 'new_winback':\n",
    "                new_winback_channel_contributions_12_1_23 = {\n",
    "                    \"HMI Audio\": 0.0122,\n",
    "                    \"HMI Digital Video\": 0.0237,\n",
    "                    \"HMI Display\": 0.0595,\n",
    "                    \"HMI Linear\": 0.0327,\n",
    "                    \"HMI OOH\": 0.0717,\n",
    "                    \"Influencer\": 0.0002,\n",
    "                    \"MACS App\": 0.0497,\n",
    "                    \"MACS Programmatic\": 0.0281,\n",
    "                    \"MACS Social\": 0.0541,\n",
    "                    \"OO Display\": 0.0067,\n",
    "                    \"OO Linear\": 0.0828,\n",
    "                    \"OO Video\": 0.1046,\n",
    "                    \"Paid Search\": 0.1387,\n",
    "                    \"Partnerships\": 0.2712,\n",
    "                    \"Podcast One Audio\": 0.0439,\n",
    "                }\n",
    "                prior_sigma = [v * len(new_winback_channel_contributions_12_1_23) for v in new_winback_channel_contributions_12_1_23.values()]\n",
    "    elif saturation_beta_prior_sigma == \"prior_spend_efficiency_sigma\":\n",
    "            if audience == 'returning':\n",
    "                spend_efficiency = {\n",
    "                    \"HMI Digital Display\" : 0.062,\n",
    "                    \"HMI Digital Video\" : 0.097,\n",
    "                    \"HMI Linear\" : 0.011,\n",
    "                    \"HMI OOH\": 0.011,\n",
    "                    \"HMI Terrestrial Audio\" : 0.034,\n",
    "                    \"Influencer\" : 0.539,\n",
    "                    \"MACS App\" : 0.007,\n",
    "                    \"MACS Programmatic\" : 0.005,\n",
    "                    \"MACS Social\" : 0.004,\n",
    "                    \"O&O Display\" : 0.227,\n",
    "                    \"O&O Linear\" : 0.000,\n",
    "                    \"O&O Video\" : 0.001,\n",
    "                    \"Paid Search\" : 0.001,\n",
    "                    \"Partnerships\" : 0.000,\n",
    "                    \"Podcast One Audio\" : 0.000,\n",
    "                }    \n",
    "                prior_sigma = [v for v in spend_efficiency.values()]\n",
    "            elif audience == 'new_winback':\n",
    "                spend_efficiency = {\n",
    "                    \"HMI Digital Display\": 0.158,\n",
    "                    \"HMI Digital Video\": 0.028,\n",
    "                    \"HMI Linear\": 0.014,\n",
    "                    \"HMI OOH\": 0.039,\n",
    "                    \"HMI Terrestrial Audio\": 0.038,\n",
    "                    \"Influencer\": 0.070,\n",
    "                    \"MACS App\": 0.058,\n",
    "                    \"MACS Programmatic\": 0.031,\n",
    "                    \"MACS Social\": 0.038,\n",
    "                    \"O&O Display\": 0.278,\n",
    "                    \"O&O Linear\": 0.015,\n",
    "                    \"O&O Video\": 0.074,\n",
    "                    \"Paid Search\": 0.116,\n",
    "                    \"Partnerships\": 0.033,\n",
    "                    \"Podcast One Audio\": 0.012,\n",
    "                }\n",
    "                prior_sigma = [v for v in spend_efficiency.values()]\n",
    "    # specify the prior alpha,beta for adstock_alpha beta prior distribution\n",
    "    if adstock_alpha_prior in ['spend_imps_ratio', 'contrib_imps_ratio', 'spend_efficiency_ratio']:\n",
    "        # using using different values of alpha & beta for different ratios by each channels \n",
    "        prior_alpha = alpha_prior_list\n",
    "        prior_beta = beta_prior_list\n",
    "    elif adstock_alpha_prior == 'uniform':\n",
    "        # uniform beta distribution\n",
    "        # passing alpha & beta as 1 for all channels\n",
    "        prior_alpha = [1 for _ in channels]\n",
    "        prior_beta = [1 for _ in channels]\n",
    "    elif adstock_alpha_prior == 'uniform_beta_3':\n",
    "        # uniform beta distribution\n",
    "        prior_alpha = [1 for _ in channels]\n",
    "        prior_beta = [3 for _ in channels]    \n",
    "            \n",
    "    \n",
    "    print(f\"Running prior_sigma using {saturation_beta_prior_sigma}\")\n",
    "    print(f\"Running prior_alpha and prior_beta of adstock using {adstock_alpha_prior}\")\n",
    "\n",
    "    # Filter out control columns that are all zero for this fold\n",
    "    nonzero_control_columns = [col for col in control_columns if X[col].sum() > 0]\n",
    "\n",
    "    ### Prior Specs ####\n",
    "    model_config = model_config_param | {\"saturation_beta\": Prior(\"HalfNormal\", sigma=prior_sigma, dims=\"channel\"),\n",
    "                                         \"adstock_alpha\": Prior(\"Beta\", alpha=prior_alpha, beta=prior_beta, dims=\"channel\")}\n",
    "    \n",
    "    ###################\n",
    "    \n",
    "    return MMM(\n",
    "        adstock=GeometricAdstock(l_max=64),\n",
    "        saturation=LogisticSaturation(),\n",
    "        date_column=date_column,\n",
    "        channel_columns=channel_names, # use channel names \n",
    "        control_columns=nonzero_control_columns,\n",
    "        model_config=model_config,\n",
    "        time_varying_media=True,\n",
    "        time_varying_intercept=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_mmm(\n",
    "    mmm: MMM, X: pd.DataFrame, y: pd.Series, random_seed: np.random.Generator\n",
    ") -> MMM:\n",
    "    \"\"\"Fit the model.\"\"\"\n",
    "    fit_kwargs = {\n",
    "        \"tune\": tuneups,\n",
    "        \"chains\": chain,\n",
    "        \"draws\": draws,\n",
    "        \"nuts_sampler\": \"numpyro\",\n",
    "        \"random_seed\": random_seed,\n",
    "        \"prior_predictive\": True,\n",
    "        \"log_likelihood\": True,\n",
    "        \"target_accept\": target_accept\n",
    "    }\n",
    "    _ = mmm.fit(X, y, progressbar=True, **fit_kwargs)\n",
    "    _ = mmm.sample_posterior_predictive(\n",
    "        X, extend_idata=True, combined=True, progressbar=True, random_seed=random_seed\n",
    "    )\n",
    "    return mmm\n",
    "target_accept = 0.90 #acceptance rate\n",
    "# Log to MLflow\n",
    "mlflow.log_param(\"target_accept\", target_accept) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d07481a-5705-462b-96db-3525c871a0a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For the sake of convenience, we define a data container to store the results of the time-slice cross validation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40a7473-91ae-4cae-964f-9886e1182c8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TimeSliceCrossValidationResult:\n",
    "    \"\"\"Container for the results of the time slice cross validation.\"\"\"\n",
    "\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: pd.Series\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: pd.Series\n",
    "    mmm: MMM\n",
    "    y_pred_test: pd.Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f44610e-8d6b-4a74-8b6e-6608f120985f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Finally, we define the main function that performs the time-slice cross validation step by calling the functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af25155-e9d6-4b0b-9093-16b2bf4d44a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def time_slice_cross_validation_step(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    random_seed: np.random.Generator,\n",
    ") -> TimeSliceCrossValidationResult:\n",
    "    \"\"\"Time-slice cross validation step.\n",
    "\n",
    "    We fit the model on the training data and generate predictions for the test data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: pd.DataFrame\n",
    "        Training data.\n",
    "    y_train: pd.Series\n",
    "        Training target.\n",
    "    X_test: pd.DataFrame\n",
    "        Test data.\n",
    "    y_test: pd.Series\n",
    "        Test target.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TimeSliceCrossValidationResult\n",
    "        Results of the time slice cross validation step.\n",
    "    \"\"\"\n",
    "    mmm = get_mmm(X_train, channel_names=channel_names, control_columns=control_columns, date_column=date_column)\n",
    "    \n",
    "    # Alternative using f-string\n",
    "    print(f\"Configurations for mmm: {pprint.pformat(mmm.model_config)}\")\n",
    "    mmm = fit_mmm(mmm, X_train, y_train, random_seed)\n",
    "\n",
    "    y_pred_test = mmm.sample_posterior_predictive(\n",
    "        X_test,\n",
    "        include_last_observations=True,\n",
    "        original_scale=True,\n",
    "        extend_idata=False,\n",
    "        progressbar=False,\n",
    "        random_seed=random_seed,\n",
    "    )\n",
    "\n",
    "    return TimeSliceCrossValidationResult(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        mmm=mmm,\n",
    "        y_pred_test=y_pred_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf6e0ed9-9378-4ac0-96ee-4dfd6f2360e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Prior distribution Checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc67c8f9-cb28-4390-b6b7-69fde4a68ae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prior_dist_check_cv(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    random_seed: np.random.Generator):\n",
    "\n",
    "    \"\"\"\n",
    "    Plot the prior distribution check\n",
    "    \"\"\"\n",
    "\n",
    "    mmm = get_mmm(X_train, channel_names=channel_names, control_columns=control_columns, date_column=date_column)\n",
    "    prior_predictive = mmm.sample_prior_predictive(X_train, y_train, samples=4_000, extend_idata=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 3.5))  # shortened height\n",
    "\n",
    "    fig = mmm.plot_prior_predictive(original_scale=True, ax=ax)\n",
    "    # set y-axis to be in the range of y_var\n",
    "    ax.legend(loc=\"upper right\", bbox_to_anchor=(0.5, -0.2), ncol=4)\n",
    "    ax.set_title(\"Prior Predictive Sampling - Training Data\")\n",
    "    ax.set(xlabel=\"date\", ylabel=y_var)\n",
    "\n",
    "    # Save and log to MLflow\n",
    "    fig.savefig(f\"Prior Predictive Check - Training Data.png\", bbox_inches=\"tight\")\n",
    "    mlflow.log_artifact(f\"Prior Predictive Check - Training Data.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca34a39a-97c9-4a02-88a7-c2715cf345f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are now ready to run the time-slice cross validation loop 💪!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c919816d-632c-4298-895e-85f986c4ba52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Time-Slice-Cross-Validation Loop\n",
    "\n",
    "Depending on the business requirements, we need to decide the initial number of observations to use for fitting the model (`n_init`) and the forecast horizon (`forecast_horizon`). For this example, we use the first year's observations to fit the model and then predict the next 12 observations (3 months)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4a32e2-bd6b-48d9-89d5-1e2fdde39423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split the data into X and Y\n",
    "\n",
    "x_columns = [date_column] + channel_names + control_columns\n",
    "X = data_df.loc[:, x_columns]\n",
    "y = data_df[y_var]\n",
    "\n",
    "# Define your forecast horizons per month\n",
    "n_init = 365\n",
    "forecast_horizons = {\n",
    "    \"2024-01\": 31,\n",
    "    \"2024-02\": 29,\n",
    "    \"2024-03\": 31,\n",
    "}\n",
    "months = sorted(list(forecast_horizons.keys()))\n",
    "n_iterations = len(months)\n",
    "\n",
    "mlflow.log_param(\"forecast_horizons\", forecast_horizons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc4119e6-8c1f-4989-aa1b-3284e69fdfa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prior Distribution Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f79b282f-ba00-4142-a04f-6f9f12fc6c61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can now generate prior predictive samples to see how the model behaves under the prior specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a9f586b-9142-4fa1-a9a3-359cd5d32e01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "prior_check_results = []\n",
    "split_start = n_init\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    month_str = months[i]\n",
    "    forecast_horizon = forecast_horizons[month_str]\n",
    "    step_size = forecast_horizon\n",
    "\n",
    "    train_test_split = split_start\n",
    "\n",
    "    if train_test_split + forecast_horizon > len(y):\n",
    "        print(f\"⚠️ Skipping iteration {i} ({month_str}) — not enough data\")\n",
    "        continue\n",
    "\n",
    "    X_train = X.iloc[:train_test_split].copy()\n",
    "    y_train = y.iloc[:train_test_split].copy()\n",
    "    X_test = X.iloc[train_test_split : train_test_split + forecast_horizon].copy()\n",
    "    y_test = y.iloc[train_test_split : train_test_split + forecast_horizon].copy()\n",
    "\n",
    "\n",
    "    print(f\"\\n📦 Fold {i} — {month_str}\")\n",
    "    print(f\"Train: {X_train[date_column].min()} to {X_train[date_column].max()}\")\n",
    "    print(f\"Test:  {X_test[date_column].min()} to {X_test[date_column].max()}\")\n",
    "\n",
    "    result = prior_dist_check_cv(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        random_seed=rng,\n",
    "    )\n",
    "\n",
    "    print(result)\n",
    "    prior_check_results.append(result)\n",
    "    split_start += step_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6d87a38-6638-4ce9-9525-4e3aac6d5c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "split_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e987becb-3595-403c-a23a-187ab47a0f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's run it! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5776381-8c59-43ec-95da-64f4760f60fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- intercept — The baseline level of sales or target variable in the absence of any marketing spend or other variables. It sets the starting point for the model.\n",
    "- likelihood — When you increase the focus on the likelihood, the model relies more heavily on the observed data and less on the priors. This means the model will be more data-driven, allowing the observed outcomes to have a stronger influence on the parameter estimates\n",
    "- gamma_control — Control variables that account for external factors, such as macroeconomic conditions, holidays, or other non-marketing variables that might influence sales.\n",
    "- gamma_fourier — Fourier terms used to model seasonality in the data, capturing recurring patterns or cycles in sales.\n",
    "- adstock_alpha — Controls the adstock effect, determining how much the impact of marketing spend decays over time.\n",
    "- saturation_lambda — Defines the steepness of the saturation curve, determining how quickly diminishing returns set in as marketing spend increases. If ( \\lambda ) is small, the media channel saturates quickly, meaning additional spend or impressions beyond a certain point will have little incremental effect. If ( \\lambda ) is large, the media channel can sustain higher levels of spend or impressions before experiencing diminishing returns.\n",
    "- saturation_beta — The marketing spend coefficient, which measures the direct effect of marketing spend on the target variable (e.g. sales). High ( \\beta ): Media effectiveness drops off quickly after reaching the saturation point, indicating that additional spend or impressions beyond a certain level have little incremental effect.Low ( \\beta ): Media effectiveness decreases more gradually, allowing for a smoother decline in returns as spend or impressions increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6bdc28f-d902-4fd3-a48f-21ce189a4660",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "training"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize Ray\n",
    "if not ray.is_initialized():\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Ray remote function for CV step\n",
    "@ray.remote\n",
    "def run_time_slice_cv(X_train, y_train, X_test, y_test, rng, max_retries=2):\n",
    "    import traceback\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            return time_slice_cross_validation_step(\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                random_seed=rng,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                print(f\"❌ FAILED: {e}\")\n",
    "                traceback.print_exc()\n",
    "                return None\n",
    "\n",
    "# Prepare tasks\n",
    "futures = []\n",
    "split_start = n_init\n",
    "split_indices = []\n",
    "#n_iterations is 3 months\n",
    "for i in range(n_iterations):\n",
    "    month_str = months[i]\n",
    "    forecast_horizon = forecast_horizons[month_str]\n",
    "    step_size = forecast_horizon\n",
    "\n",
    "    train_test_split = split_start\n",
    "\n",
    "    if train_test_split + forecast_horizon > len(y):\n",
    "        print(f\"⚠️ Skipping iteration {i} ({month_str}) — not enough data\")\n",
    "        continue\n",
    "\n",
    "    X_train = X.iloc[:train_test_split].copy()\n",
    "    y_train = y.iloc[:train_test_split].copy()\n",
    "    X_test = X.iloc[train_test_split : train_test_split + forecast_horizon].copy()\n",
    "    y_test = y.iloc[train_test_split : train_test_split + forecast_horizon].copy()\n",
    "\n",
    "    print(f\"Forecast horizon: {forecast_horizon} days\")\n",
    "    print(f\"Iteration {i} ({month_str})\")\n",
    "    # print training period\n",
    "    print(f\"Training period: {X_train[date_column].min()} to {X_train[date_column].max()}\")\n",
    "    # print testing period\n",
    "    print(f\"Testing period: {X_test[date_column].min()} to {X_test[date_column].max()}\")\n",
    "\n",
    "\n",
    "    future = run_time_slice_cv.remote(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        rng=rng,\n",
    "    )\n",
    "    futures.append(future)\n",
    "    split_indices.append((i, month_str))\n",
    "\n",
    "    split_start += step_size\n",
    "\n",
    "# Collect results with tqdm\n",
    "results = []\n",
    "for i, future in tqdm(enumerate(futures), total=len(futures), desc=\"TimeSlice CV\"):\n",
    "    result = ray.get(future)\n",
    "    if result is not None:\n",
    "        results.append(result)\n",
    "    else:\n",
    "        print(f\"⚠️ Skipped result at iteration {split_indices[i][0]}: {split_indices[i][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c769466d-60b7-4c41-a6f7-a978fa8bda22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Diagnostics\n",
    "\n",
    "First, we evaluate whether we have any divergences in the model (we can extend the analysis more more model diagnostics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "341e8dd1-25d1-457e-ad8f-9b26f7d26ed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "divergence = sum([result.mmm.idata[\"sample_stats\"][\"diverging\"].sum().item() for result in results])\n",
    "## divergence should be 0 if possible which indicated a good start\n",
    "## play around with tune, draws, chains, target_Accept\n",
    "mlflow.log_param(\"divergence\", divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbc267f6-f381-42be-bcf4-0a4a0b416a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Measure rhat - measuring convergence across chains (Values close to 1 (typically < 1.05) indicate good convergence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "940114de-98c6-4859-8c23-6caac38c9e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TO DO: Log rhat to mlflow\n",
    "r_hats = []\n",
    "for month, result in zip(months, results):\n",
    "    idata = result.mmm.idata\n",
    "\n",
    "    summary = az.summary(\n",
    "        data=idata,\n",
    "        var_names=[\n",
    "            \"intercept\",\n",
    "            \"y_sigma\",\n",
    "            \"saturation_beta\",\n",
    "            \"saturation_lam\",\n",
    "            \"adstock_alpha\",\n",
    "            \"gamma_control\",\n",
    "        ],\n",
    "        kind=\"diagnostics\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n R-hat summary for {month}\")\n",
    "    print(summary[\"r_hat\"].describe())\n",
    "    r_hats.append(summary[\"r_hat\"].mean())\n",
    "\n",
    "mean_r_hat = np.mean(r_hats)\n",
    "mlflow.log_metric(\"mean_r_hat\", mean_r_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9f9bd6c-89d7-4b19-bb8c-c93a3a1959ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lets visualize the posterior distributions of all model parameters from the first cross-validation fold of a PyMC-Marketing MMM model by plotting one histogram per parameter.\n",
    "\n",
    "Ideally posterior distribution should be smooth and unimodal, with all chains showing similar distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca41f30-77a6-466b-92a1-0e6d29cdaa23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First result only\n",
    "month = months[0]\n",
    "result = results[0]\n",
    "idata = result.mmm.idata\n",
    "    \n",
    "\n",
    "# define param_names\n",
    "param_names = list(idata.posterior.data_vars)\n",
    "# define n_params\n",
    "n_params = len(param_names)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_params, figsize=(15, 3 * n_params), sharex=False)\n",
    "\n",
    "if n_params == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "for i, param in enumerate(param_names):\n",
    "        values = idata.posterior[param].values.flatten()\n",
    "        axes[i].hist(values, bins=50, density=True)\n",
    "        axes[i].set_title(param)\n",
    "        axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save and log\n",
    "fig.savefig(\"parameter_posterior_distribution_fold0.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"parameter_posterior_distribution_fold0.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a45c8a1b-01f2-4111-9b34-aa2071c9f8e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Model Trace: Illustrate the value of each parameter over the MCMC sampling process. Any trends or slow drifts may indicate poor mixing or non-convergence, while chains that don’t overlap could suggest they are stuck in different modes of the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ffd5492-2f46-41fc-b82d-5871049ace61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot trace and capture Axes array\n",
    "_ = az.plot_trace(\n",
    "    data=results[0].mmm.fit_result,\n",
    "    var_names=[\n",
    "        \"intercept\",\n",
    "        \"y_sigma\",\n",
    "        \"saturation_beta\",\n",
    "        \"saturation_lam\",\n",
    "        \"adstock_alpha\",\n",
    "        \"gamma_control\",\n",
    "    ],\n",
    "    compact=True,\n",
    "    backend_kwargs={\"figsize\": (12, 10), \"layout\": \"constrained\"},\n",
    ")\n",
    "\n",
    "# Get the current figure object after plot\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(\"Model Trace - Fold 0\", fontsize=16)\n",
    "\n",
    "# Save and log\n",
    "fig.savefig(\"model_trace_fold_0.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"model_trace_fold_0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff1925e-87a9-4789-b57d-10becadfe929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot trace and capture Axes array\n",
    "_ = az.plot_trace(\n",
    "    data=results[-1].mmm.fit_result,\n",
    "    var_names=[\n",
    "        \"intercept\",\n",
    "        \"y_sigma\",\n",
    "        \"saturation_beta\",\n",
    "        \"saturation_lam\",\n",
    "        \"adstock_alpha\",\n",
    "        \"gamma_control\",\n",
    "    ],\n",
    "    compact=True,\n",
    "    backend_kwargs={\"figsize\": (12, 10), \"layout\": \"constrained\"},\n",
    ")\n",
    "\n",
    "# Get the current figure object after plot\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(\"Model Trace - Last Fold\", fontsize=16)\n",
    "\n",
    "# Save and log\n",
    "fig.savefig(\"model_trace_last_fold.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"model_trace_last_fold.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db8dd938-d92a-44f6-a99a-901a59ff05fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Prior vs. Posterior Influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49aa4f9a-d71a-41ed-8f11-e34c0f8e1b60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "idata = results[0].mmm.idata\n",
    "print(idata.groups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d97dc101-95c6-4f66-9218-98c2f6496490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "az.plot_dist_comparison(\n",
    "    data=results[1].mmm.idata,\n",
    "    var_names=[\n",
    "        \"intercept\",\n",
    "        \"y_sigma\",\n",
    "        \"saturation_beta\",\n",
    "        \"saturation_lam\",\n",
    "        \"adstock_alpha\",\n",
    "        \"gamma_control\",\n",
    "    ],\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b20159-53ff-4fc3-9381-e0ad39966ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate Parameter Stability\n",
    "\n",
    "Next, we look at the stability of the model parameters. For a good model, these should not change abruptly over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e0ec9f-9a5f-4c1c-83d0-701a39fbab8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8f52de2-657d-484b-bb8c-c17fcbf20f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "records_df = pd.DataFrame()\n",
    "\n",
    "for month, result in zip(months, results):\n",
    "    idata = result.mmm.idata\n",
    "    intercept = idata.posterior[\"intercept\"]  # Could be scalar or time-varying\n",
    "\n",
    "    # Stack chain and draw into one dimension\n",
    "    intercept_stacked = intercept.stack(sample=(\"chain\", \"draw\"))\n",
    "\n",
    "    # Check if intercept is time-varying (has more than 1 dim after stacking)\n",
    "    non_sample_dims = [dim for dim in intercept_stacked.dims if dim != \"sample\"]\n",
    "\n",
    "    if non_sample_dims:  # Time-varying intercept\n",
    "        time_dim = non_sample_dims[0]\n",
    "        n_days = intercept_stacked.sizes[time_dim]\n",
    "\n",
    "        # Get the date range based on X_train\n",
    "        start_date = pd.to_datetime(result.X_train[date_column].min())\n",
    "        time_index = pd.date_range(start=start_date, periods=n_days, freq=\"D\")\n",
    "\n",
    "        # Reshape to DataFrame: [time x sample]\n",
    "        intercept_df = pd.DataFrame(\n",
    "            intercept_stacked.transpose(time_dim, \"sample\").values,\n",
    "            index=time_index,\n",
    "            columns=intercept_stacked.coords[\"sample\"].values,\n",
    "        )\n",
    "        intercept_df = intercept_df.rename_axis(\"day_dt\").reset_index()\n",
    "        intercept_df[\"iteration\"] = month\n",
    "\n",
    "        # Melt to long format\n",
    "        intercept_long = intercept_df.melt(\n",
    "            id_vars=[\"day_dt\", \"iteration\"], \n",
    "            var_name=\"sample_id\", \n",
    "            value_name=\"intercept\"\n",
    "        )\n",
    "\n",
    "        # Monthly summaries\n",
    "        intercept_long[\"month_of_day_dt\"] = intercept_long[\"day_dt\"].dt.to_period(\"M\")\n",
    "        grouped = intercept_long.groupby(\"month_of_day_dt\")[\"intercept\"]\n",
    "\n",
    "        monthly_summary = grouped.agg(mean=\"mean\", median=\"median\", std=\"std\").reset_index()\n",
    "        monthly_summary[\"lower_ci\"] = grouped.quantile(0.025).values\n",
    "        monthly_summary[\"upper_ci\"] = grouped.quantile(0.975).values\n",
    "        monthly_summary[\"iteration\"] = month\n",
    "\n",
    "        records_df = pd.concat([records_df, monthly_summary], ignore_index=True)\n",
    "\n",
    "    else:  # Scalar intercept\n",
    "        intercept_flat = intercept_stacked.values\n",
    "\n",
    "        summary = {\n",
    "            \"iteration\": month,\n",
    "            \"month_of_day_dt\": pd.Period(result.X_train[date_column].min(), freq=\"M\"),\n",
    "            \"mean\": intercept_flat.mean(),\n",
    "            \"median\": np.median(intercept_flat),\n",
    "            \"std\": intercept_flat.std(),\n",
    "            \"lower_ci\": np.quantile(intercept_flat, 0.025),\n",
    "            \"upper_ci\": np.quantile(intercept_flat, 0.975),\n",
    "        }\n",
    "\n",
    "        records_df = pd.concat([records_df, pd.DataFrame([summary])], ignore_index=True)\n",
    "\n",
    "# Convert month to string for plotting\n",
    "records_df[\"month_of_day_dt\"] = records_df[\"iteration\"].astype(str)\n",
    "\n",
    "display(records_df)\n",
    "\n",
    "# plot the intercepts\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "\n",
    "az.plot_forest(\n",
    "    data=[result.mmm.idata[\"posterior\"] for result in results],\n",
    "    model_names=[f\"{i}\" for i in months],\n",
    "    var_names=[\"intercept\"],\n",
    "    combined=True,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.suptitle(\"Intercept\", fontsize=18, fontweight=\"bold\", y=1.06)\n",
    "fig.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"Intercept Stability.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"Intercept Stability.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e97e857e-5f5a-4eb0-911f-7437ab105df5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Gamma Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46a5df15-5d42-4de5-b87c-a511fc4da0f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 18))\n",
    "\n",
    "az.plot_forest(\n",
    "    data=[result.mmm.idata[\"posterior\"] for result in results],\n",
    "    model_names=[f\"{i}\" for i in months],\n",
    "    var_names=[\"gamma_control\"],\n",
    "    combined=True,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.suptitle(\"Non-Media Coefficients (Gamma Control)\", fontsize=18, fontweight=\"bold\", y=1.06);\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"Non-Media Coefficients Stability.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"Non-Media Coefficients Stability.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c49428ea-b8ee-473c-9951-9fa4cacbebeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Adstock Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40de847-af38-48c5-900c-dcb6ad8852f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "az.plot_forest(\n",
    "    data=[result.mmm.idata[\"posterior\"] for result in results],\n",
    "    model_names=[f\"{i}\" for i in months],\n",
    "    var_names=[\"adstock_alpha\"],\n",
    "    combined=True,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.suptitle(\"Adstock Alpha\", fontsize=18, fontweight=\"bold\", y=1.06);\n",
    "# label x-axis as \"Adstock Alpha\"\n",
    "ax.set_xlabel(\"Alpha (Low=faster decay, High = slower decay)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"Geo Adstock Alpha Stability.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"Geo Adstock Alpha Stability.png\")\n",
    "\n",
    "# double check whether this is theta \n",
    "#A higher ( \\alpha ) results in a slower decay of media effects, leading to higher cumulative adstock values.\n",
    "#A lower ( \\alpha ) results in a faster decay, leading to lower cumulative adstock values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f7983b5-fc47-4107-87f3-9ad30927c8d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Saturation Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edbc6c2d-ef9c-49b3-a5cf-2a18797050bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "az.plot_forest(\n",
    "    data=[result.mmm.idata[\"posterior\"] for result in results],\n",
    "    model_names=[f\"{i}\" for i in months],\n",
    "    var_names=[\"saturation_beta\"],\n",
    "    combined=True,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.suptitle(\"Saturation Beta\", fontsize=18, fontweight=\"bold\", y=1.06);\n",
    "\n",
    "ax.set_xlabel(\"Beta (Low = fast saturation, High = slow saturation)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"Saturation Beta Stability.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"Saturation Beta Stability.png\")\n",
    "\n",
    "\n",
    "#High ( \\beta ): Media effectiveness drops off quickly after reaching the saturation point, indicating that additional spend or impressions beyond a certain level have little incremental effect.\n",
    "# Low ( \\beta ): Media effectiveness decreases more gradually, allowing for a smoother decline in returns as spend or impressions increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8744611d-88c8-40f9-95ac-d53278eb5747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Saturation Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d0b426b-2213-4dba-92c5-8945133bff91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "az.plot_forest(\n",
    "    data=[result.mmm.idata[\"posterior\"] for result in results],\n",
    "    model_names=[f\"{i}\" for i in months],\n",
    "    var_names=[\"saturation_lam\"],\n",
    "    combined=True,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.suptitle(\"Saturation Lambda\", fontsize=18, fontweight=\"bold\", y=1.06);\n",
    "# label x-axis as \"Saturation Lambda\"\n",
    "ax.set_xlabel(\"Lambda (Low = fast saturation, High = slow saturation)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# If ( \\lambda ) is small, the media channel saturates quickly, meaning additional spend or impressions beyond a certain point will have little incremental effect.\n",
    "# If ( \\lambda ) is large, the media channel can sustain higher levels of spend or impressions before experiencing diminishing returns.\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"Saturation Lambda Stability.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"Saturation Lambda Stability.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e9bfcfe-a12b-4631-b09b-c52586444959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The parameters seem to be stable over time. This implies that the estimates ROAS will not change abruptly over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d33de820-22dc-4077-ad8c-73b1a7129815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's look at contribution of each channel to the total spend for each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bfc6291-377a-4205-88cd-0dc29ab9958a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate Out of Sample Predictions\n",
    "\n",
    "Finally, we evaluate the out of sample predictions. To begin with, we can simply plot the posterior predictive distributions for each iteration for both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff1ac059-98b3-4f35-b299-2e6d088532b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=n_iterations,\n",
    "    ncols=1,\n",
    "    figsize=(20, 12),\n",
    "    sharex=True,\n",
    "    sharey=False,\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    ax = axes[i]\n",
    "    result.mmm.plot_posterior_predictive(original_scale=True, ax=ax)\n",
    "\n",
    "    hdi_prob = 0.94\n",
    "    test_hdi = az.hdi(result.y_pred_test[\"y\"].to_numpy().T, hdi_prob=hdi_prob)\n",
    "\n",
    "    ax.fill_between(\n",
    "        result.X_test[date_column],\n",
    "        test_hdi[:, 0],\n",
    "        test_hdi[:, 1],\n",
    "        color=\"C1\",\n",
    "        label=f\"{hdi_prob:.0%} HDI (test)\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax.plot(X[date_column], y, marker=\"o\", color=\"black\")\n",
    "    ax.axvline(result.X_test[date_column].iloc[0], color=\"C2\", linestyle=\"--\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "axes[-1].set(xlim=(X[date_column].iloc[n_init - 9], None))\n",
    "fig.suptitle(\"Posterior Predictive Check\", fontsize=18, fontweight=\"bold\", y=1.02);\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"Posterior Predictive Check.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"Posterior Predictive Check.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac9fe553-b9ee-4e0d-8132-3df037212f6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Overall, the out of sample predictions look very good 🚀!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9909155-75d1-4861-8b0a-7f0434d2380c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can quantify the model performance using the Continuous Ranked Probability Score (CRPS).\n",
    "\n",
    "> *“The CRPS — Continuous Ranked Probability Score — is a score function that compares a single ground truth value to a Cumulative Distribution Function. It can be used as a metric to evaluate a model’s performance when the target variable is continuous and the model predicts the target’s distribution; Examples include Bayesian Regression or Bayesian Time Series models.”*\n",
    "\n",
    "\n",
    "For a nice explanation of the CRPS, check out this [blog post](https://towardsdatascience.com/crps-a-scoring-function-for-bayesian-machine-learning-models-dd55a7a337a8).\n",
    "\n",
    "In PyMC Marketing, we provide the function {func}`crps <pymc_marketing.metrics.crps>` to compute this metric. We can use it to compute the CRPS score for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be9fe39-6092-4514-8987-8ed5d6265469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crps_results_train: list[float] = [\n",
    "    crps(\n",
    "        y_true=result.y_train.to_numpy(),\n",
    "        y_pred=az.extract(\n",
    "            # Scale the predictions back to the original scale\n",
    "            apply_sklearn_transformer_across_dim(\n",
    "                data=result.mmm.idata.posterior_predictive[\"y\"],\n",
    "                func=result.mmm.get_target_transformer().inverse_transform,\n",
    "                dim_name=\"date\",\n",
    "            )\n",
    "        )[\"y\"]\n",
    "        .to_numpy()\n",
    "        .T,\n",
    "    )\n",
    "    for result in results\n",
    "]\n",
    "\n",
    "\n",
    "crps_results_test: list[float] = [\n",
    "    crps(\n",
    "        y_true=result.y_test.to_numpy(),\n",
    "        y_pred=result.y_pred_test[\"y\"].to_numpy().T,\n",
    "    )\n",
    "    for result in results\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=2, ncols=1, figsize=(12, 7), sharex=True, sharey=False, layout=\"constrained\"\n",
    ")\n",
    "\n",
    "ax[0].plot(crps_results_train, marker=\"o\", color=\"C0\", label=\"train\")\n",
    "ax[0].set(ylabel=\"CRPS\", title=\"Train CRPS\")\n",
    "ax[1].plot(crps_results_test, marker=\"o\", color=\"C1\", label=\"test\")\n",
    "ax[1].set(xlabel=\"Iteration\", ylabel=\"CRPS\", title=\"Test CRPS\")\n",
    "fig.suptitle(\"CRPS for each iteration\", fontsize=18, fontweight=\"bold\", y=1.05);\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"CRPS.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"CRPS.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da80d04-d011-4896-9544-42e0d1dd337e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Component Contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24eab786-0312-44cd-bb50-72816b221cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_contributions = []\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    contrib = result.mmm.compute_mean_contributions_over_time(original_scale=True).copy()\n",
    "\n",
    "    # Use the index as day\n",
    "    contrib[\"day_dt\"] = pd.to_datetime(contrib.index)\n",
    "    contrib[\"fold\"] = i + 1\n",
    "\n",
    "    all_contributions.append(contrib)\n",
    "\n",
    "df_all_contributions = pd.concat(all_contributions, ignore_index=True)\n",
    "# make day_dt the index\n",
    "df_all_contributions = df_all_contributions.set_index(\"day_dt\", drop=False)\n",
    "\n",
    "# melt df\n",
    "melt_contrib = df_all_contributions.melt(id_vars=[\"day_dt\", \"fold\"], var_name=\"channel\", value_name=\"mean_contribution\")\n",
    "\n",
    "# aggregate by day_dt, and channel\n",
    "mean_contrib = melt_contrib.groupby([\"day_dt\", \"channel\"])[\"mean_contribution\"].mean().reset_index()\n",
    "display(mean_contrib[mean_contrib['channel'] == 'intercept'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb103f2-11eb-4645-ade3-1f676dba3d7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "compute_grouped_contribution_share"
    }
   },
   "outputs": [],
   "source": [
    "def compute_grouped_contribution_share(\n",
    "    df,\n",
    "    channel_groups,\n",
    "    format=\"long\",\n",
    "    date_col=\"day_dt\",\n",
    "    freq=\"D\",\n",
    "    percentage_share=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes grouped contribution values or percentage shares over time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with columns ['date', 'channel', 'mean_contribution'].\n",
    "    channel_groups : dict\n",
    "        Dictionary of group_name -> list of channel names.\n",
    "    format : str\n",
    "        Output format: 'long' or 'wide'.\n",
    "    date_col : str\n",
    "        Column name for date.\n",
    "    freq : str\n",
    "        Resampling frequency: 'D' = daily, 'W' = weekly, 'M' = monthly, etc.\n",
    "    percentage_share : bool\n",
    "        If True, return percentage share; if False, return original values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Long or wide format DataFrame with contributions or percent shares by group.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # Aggregate contributions per date and channel\n",
    "    grouped = df.groupby([date_col, \"channel\"], as_index=False).agg({\"mean_contribution\": \"mean\"})\n",
    "\n",
    "    # Pivot to wide format\n",
    "    df_pivot = grouped.pivot(index=date_col, columns=\"channel\", values=\"mean_contribution\").fillna(0)\n",
    "\n",
    "    # Resample by frequency\n",
    "    df_resampled = df_pivot.resample(freq).mean()\n",
    "\n",
    "    # Sum contributions within each group\n",
    "    group_contributions = {}\n",
    "    for group_name, channels in channel_groups.items():\n",
    "        valid_channels = [c for c in channels if c in df_resampled.columns]\n",
    "        group_contributions[group_name] = df_resampled[valid_channels].sum(axis=1)\n",
    "\n",
    "    # Build DataFrame of group contributions\n",
    "    df_grouped = pd.DataFrame(group_contributions)\n",
    "    df_grouped[date_col] = df_resampled.index\n",
    "\n",
    "    if percentage_share:\n",
    "        # this takes into account each components (channel contribution, seasonality contrib like dow, trend, events, media contribution), it sum this & calc %\n",
    "        df_grouped_share = df_grouped.set_index(date_col).div(df_grouped.set_index(date_col).sum(axis=1), axis=0) * 100\n",
    "        df_grouped_share = df_grouped_share.reset_index()\n",
    "        value_col = \"percent_share\"\n",
    "        df_out = df_grouped_share\n",
    "    else:\n",
    "        df_out = df_grouped\n",
    "        value_col = \"contribution\"\n",
    "\n",
    "    if format == \"long\":\n",
    "        df_long = df_out.melt(id_vars=[date_col], var_name=\"group\", value_name=value_col)\n",
    "        return df_long.reset_index(drop=True)\n",
    "\n",
    "    elif format == \"wide\":\n",
    "        return df_out.reset_index(drop=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid format. Choose 'long' or 'wide'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f3c93ab-169d-41cf-844f-c9012c4bc42e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "format=\"wide\" \n",
    "date_col = \"day_dt\"\n",
    "freq=\"M\"\n",
    "percentage_share=True\n",
    "\n",
    "channel_groups = {\n",
    "        \"Intercept\": ['intercept'],\n",
    "        \"Day of Week Seasonality\": [col for col in control_columns if col.startswith(\"dow_\")],\n",
    "        \"Trend\": [col for col in control_columns if col.startswith('trend_')],\n",
    "        \"Events\": [col for col in control_columns if col.startswith(\"event_\")],\n",
    "        \"Media\": channel_names\n",
    "    }\n",
    "\n",
    "df = (mean_contrib).copy()\n",
    "df[\"day_dt\"] = pd.to_datetime(df[\"day_dt\"])\n",
    "# Aggregate contributions per date and channel\n",
    "grouped = df.groupby([date_col, \"channel\"], as_index=False).agg({\"mean_contribution\": \"mean\"})\n",
    "# Pivot to wide format\n",
    "df_pivot = grouped.pivot(index=date_col, columns=\"channel\", values=\"mean_contribution\").fillna(0)\n",
    "\n",
    "# Resample by frequency\n",
    "df_resampled = df_pivot.resample(freq).mean()\n",
    "\n",
    "# Sum contributions within each group\n",
    "group_contributions = {}\n",
    "for group_name, channels in channel_groups.items():\n",
    "    valid_channels = [c for c in channels if c in df_resampled.columns]\n",
    "    group_contributions[group_name] = df_resampled[valid_channels].sum(axis=1)\n",
    "\n",
    "\n",
    "# Build DataFrame of group contributions\n",
    "df_grouped = pd.DataFrame(group_contributions)\n",
    "df_grouped[date_col] = df_resampled.index\n",
    "\n",
    "\n",
    "if percentage_share:\n",
    "    # this takes into account each components (channel contribution, seasonality contrib like dow, trend, events, media contribution), it sum this & calc %\n",
    "   df_grouped_share = df_grouped.set_index(date_col).div(df_grouped.set_index(date_col).sum(axis=1), axis=0) * 100\n",
    "   df_grouped_share = df_grouped_share.reset_index()\n",
    "   value_col = \"percent_share\"\n",
    "   df_out = df_grouped_share\n",
    "else:\n",
    "   df_out = df_grouped\n",
    "   value_col = \"contribution\"\n",
    "display(df_out)\n",
    "if format == \"long\":\n",
    "  df_long = df_out.melt(id_vars=[date_col], var_name=\"group\", value_name=value_col)\n",
    "  display(df_long.reset_index(drop=True))\n",
    "\n",
    "elif format == \"wide\":\n",
    "    df_out = df_out.reset_index(drop=True)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid format. Choose 'long' or 'wide'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cf1ba52-c4b4-4b06-bb5a-9b51e63f6261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"Y2hhbm5lbF9ncm91cGluZ3MgPSB7CiAgICAgICAgIkludGVyY2VwdCI6IFsnaW50ZXJjZXB0J10sCiAgICAgICAgIkRheSBvZiBXZWVrIFNlYXNvbmFsaXR5IjogW2NvbCBmb3IgY29sIGluIGNvbnRyb2xfY29sdW1ucyBpZiBjb2wuc3RhcnRzd2l0aCgiZG93XyIpXSwKICAgICAgICAiVHJlbmQiOiBbY29sIGZvciBjb2wgaW4gY29udHJvbF9jb2x1bW5zIGlmIGNvbC5zdGFydHN3aXRoKCd0cmVuZF8nKV0sCiAgICAgICAgIkV2ZW50cyI6IFtjb2wgZm9yIGNvbCBpbiBjb250cm9sX2NvbHVtbnMgaWYgY29sLnN0YXJ0c3dpdGgoImV2ZW50XyIpXSwKICAgICAgICAiTWVkaWEiOiBjaGFubmVsX25hbWVzCiAgICB9CgpkZl9wZXJjZW50X3NoYXJlID0gY29tcHV0ZV9ncm91cGVkX2NvbnRyaWJ1dGlvbl9zaGFyZShtZWFuX2NvbnRyaWIsIGNoYW5uZWxfZ3JvdXBpbmdzLCBmb3JtYXQ9IndpZGUiLCBkYXRlX2NvbCA9ICJkYXlfZHQiLCBmcmVxPSJNIiwgcGVyY2VudGFnZV9zaGFyZT1UcnVlKQoKCiMgU2F2ZSB5b3VyIERhdGFGcmFtZSB0byBhIHRlbXBvcmFyeSBmaWxlCmRmX3BlcmNlbnRfc2hhcmUudG9fY3N2KCJoaWdoX2xldmVsX2NvbnRyaWJ1dGlvbl9wZXJjZW50LmNzdiIsIGluZGV4PUZhbHNlKQoKIyBMb2cgaXQgYXMgYW4gYXJ0aWZhY3QgaW4gdGhlIGN1cnJlbnQgTUxmbG93IHJ1bgptbGZsb3cubG9nX2FydGlmYWN0KCJoaWdoX2xldmVsX2NvbnRyaWJ1dGlvbl9wZXJjZW50LmNzdiIpCgpkaXNwbGF5KGRmX3BlcmNlbnRfc2hhcmUp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewba68d66\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewba68d66\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewba68d66\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewba68d66) SELECT `period`,`group`,SUM(`percent_share`) `percent_share_alias` FROM q GROUP BY `group`,`period` ORDER BY `period` ASC,`group` ASC\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewba68d66\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "PIVOT_TABLE"
         },
         {
          "key": "options",
          "value": {
           "cell": {
            "field": "percent_share",
            "transform": {
             "fn": "sum"
            }
           },
           "columns": [
            {
             "field": "group",
             "sort": "ascending"
            }
           ],
           "rows": [
            {
             "field": "period",
             "sort": "ascending"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "7c45a774-be0d-4443-8b44-0cc6ab197284",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 49.703125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "period",
           "type": "column"
          },
          {
           "column": "group",
           "type": "column"
          }
         ],
         "order_by": [
          {
           "direction": "asc",
           "expression": {
            "identifier": "period",
            "kind": "identifier"
           }
          },
          {
           "direction": "asc",
           "expression": {
            "identifier": "group",
            "kind": "identifier"
           }
          }
         ],
         "selects": [
          {
           "column": "period",
           "type": "column"
          },
          {
           "column": "group",
           "type": "column"
          },
          {
           "alias": "percent_share_alias",
           "args": [
            {
             "column": "percent_share",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "channel_groupings = {\n",
    "        \"Intercept\": ['intercept'],\n",
    "        \"Day of Week Seasonality\": [col for col in control_columns if col.startswith(\"dow_\")],\n",
    "        \"Trend\": [col for col in control_columns if col.startswith('trend_')],\n",
    "        \"Events\": [col for col in control_columns if col.startswith(\"event_\")],\n",
    "        \"Media\": channel_names\n",
    "    }\n",
    "\n",
    "df_percent_share = compute_grouped_contribution_share(mean_contrib, channel_groupings, format=\"wide\", date_col = \"day_dt\", freq=\"M\", percentage_share=True)\n",
    "\n",
    "\n",
    "# Save your DataFrame to a temporary file\n",
    "df_percent_share.to_csv(\"high_level_contribution_percent.csv\", index=False)\n",
    "\n",
    "# Log it as an artifact in the current MLflow run\n",
    "mlflow.log_artifact(\"high_level_contribution_percent.csv\")\n",
    "\n",
    "display(df_percent_share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "744d6c4d-21fc-4222-9339-9162b37512a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"Y2hhbm5lbF9ncm91cGluZ3MgPSB7CiAgICAgICAgIkludGVyY2VwdCI6IFsnaW50ZXJjZXB0J10sCiAgICAgICAgIkRheSBvZiBXZWVrIFNlYXNvbmFsaXR5IjogW2NvbCBmb3IgY29sIGluIGNvbnRyb2xfY29sdW1ucyBpZiBjb2wuc3RhcnRzd2l0aCgiZG93XyIpXSwKICAgICAgICAiVHJlbmQiOiBbY29sIGZvciBjb2wgaW4gY29udHJvbF9jb2x1bW5zIGlmIGNvbC5zdGFydHN3aXRoKCd0cmVuZF8nKV0sCiAgICAgICAgIkV2ZW50cyI6IFtjb2wgZm9yIGNvbCBpbiBjb250cm9sX2NvbHVtbnMgaWYgY29sLnN0YXJ0c3dpdGgoImV2ZW50XyIpXSwKICAgICAgICAiTWVkaWEiOiBjaGFubmVsX25hbWVzCiAgICB9CgpoaWdoX2xldmVsX2NvbnRyaWIgPSBjb21wdXRlX2dyb3VwZWRfY29udHJpYnV0aW9uX3NoYXJlKG1lYW5fY29udHJpYiwgY2hhbm5lbF9ncm91cGluZ3MsIGZvcm1hdD0id2lkZSIsIGRhdGVfY29sID0gImRheV9kdCIsIGZyZXE9IkQiLCBwZXJjZW50YWdlX3NoYXJlPUZhbHNlKQoKCiMgU2F2ZSB5b3VyIERhdGFGcmFtZSB0byBhIHRlbXBvcmFyeSBmaWxlCmhpZ2hfbGV2ZWxfY29udHJpYi50b19jc3YoImhpZ2hfbGV2ZWxfY29udHJpYnV0aW9ucy5jc3YiLCBpbmRleD1GYWxzZSkKCiMgTG9nIGl0IGFzIGFuIGFydGlmYWN0IGluIHRoZSBjdXJyZW50IE1MZmxvdyBydW4KbWxmbG93LmxvZ19hcnRpZmFjdCgiaGlnaF9sZXZlbF9jb250cmlidXRpb25zLmNzdiIpCgpkaXNwbGF5KGhpZ2hfbGV2ZWxfY29udHJpYik=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView2bcdff5\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView2bcdff5\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView2bcdff5\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView2bcdff5) SELECT `day_dt`,SUM(`Trend`) `column_d914cc5c725` FROM q GROUP BY `day_dt`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView2bcdff5\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "day_dt",
             "id": "column_d914cc5c722"
            },
            "y": [
             {
              "column": "Trend",
              "id": "column_d914cc5c725",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_d914cc5c725": {
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "1669c2aa-a130-491a-8f28-b3a584b5681a",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 49.71484375,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "day_dt",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "day_dt",
           "type": "column"
          },
          {
           "alias": "column_d914cc5c725",
           "args": [
            {
             "column": "Trend",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "channel_groupings = {\n",
    "        \"Intercept\": ['intercept'],\n",
    "        \"Day of Week Seasonality\": [col for col in control_columns if col.startswith(\"dow_\")],\n",
    "        \"Trend\": [col for col in control_columns if col.startswith('trend_')],\n",
    "        \"Events\": [col for col in control_columns if col.startswith(\"event_\")],\n",
    "        \"Media\": channel_names\n",
    "    }\n",
    "\n",
    "high_level_contrib = compute_grouped_contribution_share(mean_contrib, channel_groupings, format=\"wide\", date_col = \"day_dt\", freq=\"D\", percentage_share=False)\n",
    "\n",
    "\n",
    "# Save your DataFrame to a temporary file\n",
    "high_level_contrib.to_csv(\"high_level_contributions.csv\", index=False)\n",
    "\n",
    "# Log it as an artifact in the current MLflow run\n",
    "mlflow.log_artifact(\"high_level_contributions.csv\")\n",
    "\n",
    "display(high_level_contrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eea14164-f279-47a6-881b-705aad9ab9ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "channel_groupings = {\n",
    "    \"Base\": (\n",
    "        ['intercept']\n",
    "        + [col for col in control_columns if col.startswith(\"dow_\")]\n",
    "        + [col for col in control_columns if col.startswith(f\"trend_\")]\n",
    "        + [col for col in control_columns if col.startswith(\"event_\")]\n",
    "    ),\n",
    "    \"Partnerships\": [\"Partnerships\"],\n",
    "    \"Paid Search\": [\"Paid Search\"],\n",
    "    \"OO Video\": [\"OO Video\"],\n",
    "    \"OO Linear\": [\"OO Linear\"],\n",
    "    \"Podcast One Audio\": [\"Podcast One Audio\"],\n",
    "    \"MACS Social\": [\"MACS Social\"],\n",
    "    \"HMI OOH\": [\"HMI OOH\"],\n",
    "    \"HMI Display\": [\"HMI Display\"],\n",
    "    \"MACS App\": [\"MACS App\"],\n",
    "    \"MACS Programmatic\": [\"MACS Programmatic\"],\n",
    "    \"HMI Digital Video\": [\"HMI Digital Video\"],\n",
    "    \"HMI Linear\": [\"HMI Linear\"],\n",
    "    \"HMI Audio\": [\"HMI Audio\"],\n",
    "    \"OO Display\": [\"OO Display\"],\n",
    "    \"Influencer\": [\"Influencer\"],\n",
    "}\n",
    "\n",
    "media_percent_share = compute_grouped_contribution_share(mean_contrib, channel_groupings, format=\"wide\", date_col = \"day_dt\", freq=\"M\")\n",
    "\n",
    "# Save your DataFrame to a temporary file\n",
    "media_percent_share.to_csv(\"media_contributions.csv\", index=False)\n",
    "\n",
    "# Log it as an artifact in the current MLflow run\n",
    "mlflow.log_artifact(\"media_contributions.csv\")\n",
    "\n",
    "display(media_percent_share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5dc5e15-9a28-484c-a76b-eacd7ee7fb82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate mean of each remaining column\n",
    "avg_series = media_percent_share.drop('day_dt', axis=1).mean()\n",
    "\n",
    "# Create DataFrame from the Series\n",
    "avg_df = avg_series.reset_index()\n",
    "avg_df.columns = ['channel_names', 'average']\n",
    "\n",
    "display(avg_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c59446e-4e40-490c-aa63-7daafdf7123d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "exclude_cols = [\"Base\", \"day_dt\"]\n",
    "contribution_cols = [col for col in media_percent_share.columns if col not in exclude_cols]\n",
    "\n",
    "# Compute mean contributions and get top 3\n",
    "top3 = media_percent_share[contribution_cols].mean().sort_values(ascending=False).head(3)\n",
    "print(top3)\n",
    "\n",
    "\n",
    "# Log each as MLflow param with rank\n",
    "\n",
    "for i, (channel, contrib) in enumerate(top3.items(), start=1):\n",
    "    rank_key = f\"Rank {i} channel\"\n",
    "    value = {channel: round(contrib, 6)}\n",
    "    mlflow.log_param(rank_key, value)  # convert dict to JSON string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06d45bc8-7178-4c2e-802f-371e535be51e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_area_contributions(df, date_col=\"day_dt\", title=\"Component Contributions Over Time\"):\n",
    "    \"\"\"\n",
    "    Plots an area chart of component contributions over time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with datetime column and float columns per component.\n",
    "    date_col : str\n",
    "        Name of the datetime column.\n",
    "    title : str\n",
    "        Title of the plot.\n",
    "    \"\"\"\n",
    "    # Sort by date to ensure correct plotting order\n",
    "    df = df.sort_values(date_col)\n",
    "\n",
    "    # Set datetime column as index\n",
    "    df_plot = df.set_index(date_col)\n",
    "\n",
    "    # Plot area chart\n",
    "    ax = df_plot.plot.area(figsize=(14, 6), alpha=0.8)\n",
    "\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylabel(\"Contribution\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.grid(True, which=\"major\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_area_contributions(media_percent_share, date_col=\"day_dt\", title=\"Monthly Component Contribution Shares\")\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"CRPS.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"CRPS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6b5dad-fd4e-479d-adfe-5fbace593907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for any negative coefficients, replace with 0\n",
    "high_level_contrib_positive = high_level_contrib.copy()\n",
    "high_level_contrib_positive['Trend'] = high_level_contrib['Trend'].clip(lower=0)\n",
    "high_level_contrib_positive['Events'] = high_level_contrib['Events'].clip(lower=0)\n",
    "high_level_contrib_positive['Day of Week Seasonality'] = high_level_contrib['Day of Week Seasonality'].clip(lower=0)\n",
    "\n",
    "plot_area_contributions(high_level_contrib_positive, title=\"Monthly Component Contributions\")\n",
    "\n",
    "# Save and log to MLflow\n",
    "fig.savefig(\"CRPS.png\", bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"CRPS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa124d2c-d256-4724-ac0c-e3ea87286796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_share = media_percent_share[\"Base\"].loc[media_percent_share[\"day_dt\"].dt.year == 2024].mean()\n",
    "media_share = 100 - base_share\n",
    "\n",
    "print(\"Base Contribution Share:\", base_share)\n",
    "print(\"Media Contribution Share:\", media_share)\n",
    "\n",
    "mlflow.log_metrics({\"base_share\": round(base_share,2), \"media_share\": round(media_share,2)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8e2127-ebff-4b6e-ab7e-b5c86cef09b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5c177c-488e-4a80-8fe4-cef23ffcf229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_out_sample_metrics_cv(results, metric=\"mape\"):\n",
    "    \"\"\"\n",
    "    Computes MAPE (Mean Absolute Percentage Error) across all folds of a\n",
    "    cross-validated PyMC-Marketing MMM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : list of TimeSliceCrossValidationResult\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        MAPE across all folds.\n",
    "    \"\"\"\n",
    "\n",
    "    if metric == \"mape\":\n",
    "      all_y_true = []\n",
    "      all_y_pred = []\n",
    "\n",
    "      for result in results:\n",
    "            y_true = result.y_test.values.flatten()\n",
    "            y_pred_samples = result.y_pred_test[\"y\"].to_numpy()  # shape: (T, S)\n",
    "\n",
    "            y_pred_mean = y_pred_samples.T.mean(axis=0)\n",
    "\n",
    "            all_y_true.extend(y_true)\n",
    "            all_y_pred.extend(y_pred_mean)\n",
    "\n",
    "      # Convert to numpy arrays\n",
    "      all_y_true = np.array(all_y_true)\n",
    "      all_y_pred = np.array(all_y_pred)\n",
    "\n",
    "      mape = np.mean(np.abs((all_y_true - all_y_pred) / all_y_true)) * 100\n",
    "      return mape\n",
    "    elif metric == \"coverage\":\n",
    "      total = 0\n",
    "      covered = 0\n",
    "\n",
    "      for result in results:\n",
    "          y_true = result.y_test.values.flatten()\n",
    "\n",
    "          # shape: (T, S), need (S, T) for az.hdi\n",
    "          y_pred_samples = result.y_pred_test[\"y\"].to_numpy().T  # shape: (samples, time)\n",
    "          hdi = az.hdi(y_pred_samples, hdi_prob=hdi_prob)  # shape: (T, 2)\n",
    "\n",
    "          lower, upper = hdi[:, 0], hdi[:, 1]\n",
    "          is_covered = (y_true >= lower) & (y_true <= upper)\n",
    "\n",
    "          covered += is_covered.sum()\n",
    "          total += len(y_true)\n",
    "\n",
    "      coverage_rate = covered / total\n",
    "      return coverage_rate\n",
    "\n",
    "    \n",
    "    else:\n",
    "      raise ValueError(\"metric must be either mape, coverage, or pseudo_r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f0b1fa-d635-4498-a768-467588322342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mape = compute_out_sample_metrics_cv(results, metric=\"mape\")\n",
    "coverage = compute_out_sample_metrics_cv(results, metric=\"coverage\")\n",
    "\n",
    "print(\"MAPE for out-of-sample:\", mape)\n",
    "print(\"coverage for out-of-sample:\", coverage)\n",
    "\n",
    "mlflow.log_metric(\"mape\", round(mape,2))\n",
    "mlflow.log_metric(\"coverage\", round(coverage,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99cba08-5b84-4a66-829a-6780cc3acf6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Posterior fit per fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19e7b07-c76e-43c1-965b-588f5dab3760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for month, result in zip(months,results):\n",
    "\n",
    "  fig = result.mmm.plot_posterior_predictive(original_scale=True, figsize=(10, 3))\n",
    "  # Get Axes and set x-axis limits\n",
    "  ax = fig.axes[0]  # assumes there's one main plot\n",
    "  ax.set_xlim([pd.to_datetime('2023-12'), pd.to_datetime(\"2024-04\")])\n",
    "  ax.set_title(f\"Posterior Predictions - {month}\")\n",
    "  print(month, result.X_test[\"day_dt\"].min(), result.X_test[\"day_dt\"].max())\n",
    "\n",
    "  # Save and log to MLflow\n",
    "  fig.savefig(f\"Posterior Predictions Over Time - {month}\", bbox_inches=\"tight\")\n",
    "  mlflow.log_artifact(f\"Posterior Predictions Over Time - {month}.png\")\n",
    "\n",
    "  # TO DO: troubleshoot why the last month isn't showing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da32bc8d-7244-4424-8e72-5b512b4d7a0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mape_over_time(results):\n",
    "    \"\"\"\n",
    "    Plot MAPE over time across all test folds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : list of TimeSliceCrossValidationResult\n",
    "    \"\"\"\n",
    "    all_fold_mape = []\n",
    "\n",
    "    for result in results:\n",
    "        y_true = result.y_test.values.flatten()\n",
    "        y_pred = result.y_pred_test[\"y\"].to_numpy().T.mean(axis=0)\n",
    "        dates = result.X_test[\"day_dt\"].values\n",
    "\n",
    "        # Avoid div-by-zero\n",
    "        mask = y_true != 0\n",
    "        y_true = y_true[mask]\n",
    "        y_pred = y_pred[mask]\n",
    "        dates = dates[mask]\n",
    "\n",
    "        mape = np.abs((y_true - y_pred) / y_true)\n",
    "\n",
    "        fold_df = pd.DataFrame({\n",
    "            \"day_dt\": pd.to_datetime(dates),\n",
    "            \"mape\": mape\n",
    "        })\n",
    "        all_fold_mape.append(fold_df)\n",
    "\n",
    "    # Combine all folds and average if needed\n",
    "    mape_df = pd.concat(all_fold_mape)\n",
    "    mape_daily = mape_df.groupby(\"day_dt\").mean().reset_index()\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(mape_daily[\"day_dt\"], mape_daily[\"mape\"] * 100, marker=\"o\")\n",
    "    ax.set_ylabel(\"MAPE (%)\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_title(\"MAPE Over Time Across Folds\")\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and log to MLflow\n",
    "    fig.savefig(\"MAPE Over Time Across Folds - Out Sample\", bbox_inches=\"tight\")\n",
    "    mlflow.log_artifact(\"MAPE Over Time Across Folds - Out Sample.png\")\n",
    "\n",
    "plot_mape_over_time(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c97ea61-6e5d-42cf-b08c-912f3973f154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymc_marketing.mmm.transformers import geometric_adstock, logistic_saturation\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "def data_generator(start_date, periods, channels, spend_scalar, adstock_alphas, saturation_lamdas, betas, freq=\"W\"):\n",
    "    '''\n",
    "    Generates a synthetic dataset for a MMM with trend, seasonality, and channel-specific contributions.\n",
    "\n",
    "    Args:\n",
    "        start_date (str or pd.Timestamp): The start date for the generated time series data.\n",
    "        periods (int): The number of time periods (e.g., days, weeks) to generate data for.\n",
    "        channels (list of str): A list of channel names for which the model will generate spend and sales data.\n",
    "        spend_scalar (list of float): Scalars that adjust the raw spend for each channel to a desired scale.\n",
    "        adstock_alphas (list of float): The adstock decay factors for each channel, determining how much past spend influences the current period.\n",
    "        saturation_lamdas (list of float): Lambda values for the logistic saturation function, controlling the saturation effect on each channel.\n",
    "        betas (list of float): The coefficients for each channel, representing the contribution of each channel's impact on sales.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the generated time series data, including demand, sales, and channel-specific metrics.\n",
    "    '''\n",
    "    \n",
    "    # 0. Create time dimension\n",
    "    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n",
    "    df = pd.DataFrame({'date': date_range})\n",
    "    \n",
    "    # 1. Add trend component with some growth\n",
    "    df[\"trend\"]= (np.linspace(start=0.0, stop=20, num=periods) + 5) ** (1 / 8) - 1\n",
    "    \n",
    "    # 2. Add seasonal component with oscillation around 0\n",
    "    df[\"seasonality\"] = df[\"seasonality\"] = 0.1 * np.sin(2 * np.pi * df.index / 52)\n",
    "    \n",
    "    # 3. Multiply trend and seasonality to create overall demand with noise\n",
    "    df[\"demand\"] = df[\"trend\"] * (1 + df[\"seasonality\"]) + np.random.normal(loc=0, scale=0.10, size=periods)\n",
    "    df[\"demand\"] = df[\"demand\"] * 1000\n",
    "    \n",
    "    # 4. Create proxy for demand, which is able to follow demand but has some noise added\n",
    "    df[\"demand_proxy\"] = np.abs(df[\"demand\"]* np.random.normal(loc=1, scale=0.10, size=periods))\n",
    "    \n",
    "    # 5. Initialize sales based on demand\n",
    "    df[\"sales\"] = df[\"demand\"]\n",
    "    # 6. Loop through each channel and add channel-specific contribution\n",
    "    for i, channel in enumerate(channels):\n",
    "        \n",
    "        # Create raw channel spend, following demand with some random noise added\n",
    "        df[f\"{channel}_spend_raw\"] = df[\"demand\"] * spend_scalar[i]\n",
    "        df[f\"{channel}_spend_raw\"] = np.abs(df[f\"{channel}_spend_raw\"] * np.random.normal(loc=1, scale=0.30, size=periods))\n",
    "               \n",
    "        # Scale channel spend\n",
    "        channel_transformer = MaxAbsScaler().fit(df[f\"{channel}_spend_raw\"].values.reshape(-1, 1))\n",
    "        df[f\"{channel}_spend\"] = channel_transformer .transform(df[f\"{channel}_spend_raw\"].values.reshape(-1, 1))\n",
    "    \n",
    "   \n",
    "        # Apply adstock transformation\n",
    "        df[f\"{channel}_adstock\"] = geometric_adstock(\n",
    "            x=df[f\"{channel}_spend\"].to_numpy(),\n",
    "            alpha=adstock_alphas[i],\n",
    "            l_max=8, normalize=True\n",
    "        ).eval().flatten()\n",
    "        \n",
    "        # Apply saturation transformation\n",
    "        df[f\"{channel}_saturated\"] = logistic_saturation(\n",
    "            x=df[f\"{channel}_adstock\"].to_numpy(),\n",
    "            lam=saturation_lamdas[i]\n",
    "        ).eval()\n",
    "        \n",
    "        # Calculate contribution to sales\n",
    "        df[f\"{channel}_sales\"] = df[f\"{channel}_saturated\"] * betas[i]\n",
    "        \n",
    "        # Add the channel-specific contribution to sales\n",
    "        df[\"sales\"] += df[f\"{channel}_sales\"]\n",
    "\n",
    "    return df\n",
    "  \n",
    "np.random.seed(10)\n",
    "\n",
    "# Set parameters for data generator\n",
    "start_date = \"2023-03-01\"\n",
    "periods = 52 * 3\n",
    "channels = [\"tv\", \"social\", \"search\"]\n",
    "adstock_alphas = [0.50, 0.25, 0.05]\n",
    "saturation_lamdas = [1.5, 2.5, 3.5]\n",
    "betas = [350, 150, 50]\n",
    "spend_scalars = [10, 15, 20]\n",
    "\n",
    "df = data_generator(start_date, periods, channels, spend_scalars, adstock_alphas, saturation_lamdas, betas)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b72b65-9581-4e1c-900e-f0258eaeaefa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymc_marketing.mmm.transformers import geometric_adstock, logistic_saturation\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "def data_generator(start_date, periods, channels, spend_scalar, adstock_alphas, saturation_lamdas,freq=\"D\"):\n",
    "    '''\n",
    "    Generates a synthetic dataset for a MMM with trend, seasonality, and channel-specific contributions.\n",
    "\n",
    "    Args:\n",
    "        start_date (str or pd.Timestamp): The start date for the generated time series data.\n",
    "        periods (int): The number of time periods (e.g., days, weeks) to generate data for.\n",
    "        channels (list of str): A list of channel names for which the model will generate spend and sales data.\n",
    "        spend_scalar (list of float): Scalars that adjust the raw spend for each channel to a desired scale.\n",
    "        adstock_alphas (list of float): The adstock decay factors for each channel, determining how much past spend influences the current period.\n",
    "        saturation_lamdas (list of float): Lambda values for the logistic saturation function, controlling the saturation effect on each channel.\n",
    "        betas (list of float): The coefficients for each channel, representing the contribution of each channel's impact on sales.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the generated time series data, including demand, sales, and channel-specific metrics.\n",
    "    '''\n",
    "    \n",
    "    # 0. Create time dimension\n",
    "    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n",
    "    df = pd.DataFrame({'date': date_range})\n",
    "    \n",
    "    # 1. Add trend component with some growth\n",
    "    df[\"spend\"]= 266610\n",
    "    \n",
    "    # 2. Add seasonal component with oscillation around 0\n",
    "    df[\"total_efficiency\"] = 50630\n",
    "    \n",
    "    # 3. Multiply trend and seasonality to create overall demand with noise\n",
    "    df[\"beta\"] = df[\"total_efficiency\"]/df[\"spend\"]\n",
    "    \n",
    "    # 4. Create proxy for demand, which is able to follow demand but has some noise added\n",
    "    #df[\"demand_proxy\"] = np.abs(df[\"demand\"]* np.random.normal(loc=1, scale=0.10, size=periods))\n",
    "    \n",
    "    # 5. Initialize sales based on demand\n",
    "    #df[\"sales\"] = df[\"demand\"]\n",
    "    \n",
    "    # 6. Loop through each channel and add channel-specific contribution\n",
    "    for i, channel in enumerate(channels):\n",
    "        \n",
    "        # Create raw channel spend, following demand with some random noise added\n",
    "        df[f\"{channel}_spend_raw\"] = df[\"spend\"] * spend_scalar[i]\n",
    "        df[f\"{channel}_spend_raw\"] = np.abs(df[f\"{channel}_spend_raw\"] * np.random.normal(loc=1, scale=0.30, size=periods))\n",
    "               \n",
    "        # Scale channel spend\n",
    "        channel_transformer = MaxAbsScaler().fit(df[f\"{channel}_spend_raw\"].values.reshape(-1, 1))\n",
    "        df[f\"{channel}_spend\"] = channel_transformer .transform(df[f\"{channel}_spend_raw\"].values.reshape(-1, 1))\n",
    "    \n",
    "   \n",
    "        # Apply adstock transformation\n",
    "        df[f\"{channel}_adstock\"] = geometric_adstock(\n",
    "            x=df[\"spend\"].to_numpy(),\n",
    "            alpha=adstock_alphas[i],\n",
    "            l_max=8, normalize=True\n",
    "        ).eval().flatten()\n",
    "        \n",
    "        # Apply saturation transformation\n",
    "        df[f\"{channel}_saturated\"] = logistic_saturation(\n",
    "            x=df[f\"{channel}_adstock\"].to_numpy(),\n",
    "            lam=saturation_lamdas[i]\n",
    "        ).eval()\n",
    "        \n",
    "        # Calculate contribution to sales\n",
    "        df[f\"{channel}_users\"] = df[f\"{channel}_saturated\"] * (df[\"total_efficiency\"]/df[\"spend\"])\n",
    "        \n",
    "        # Add the channel-specific contribution to sales\n",
    "        df[\"users\"] = df[f\"{channel}_users\"]\n",
    "    return df\n",
    "  \n",
    "np.random.seed(10)\n",
    "\n",
    "# Set parameters for data generator\n",
    "start_date = \"2023-03-01\"\n",
    "periods = 1\n",
    "channels = [\"display\"]\n",
    "adstock_alphas = [0.26]\n",
    "saturation_lamdas = [0.89]\n",
    "spend_scalars = [1]\n",
    "\n",
    "df = data_generator(start_date, periods, channels, spend_scalars, adstock_alphas, saturation_lamdas)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da89c07-3809-48a1-a440-3c88f222b96e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the inputs\n",
    "spend = np.array([266610])\n",
    "impressions = np.array([36933436])\n",
    "beta = 0.267\n",
    "saturation_lambda = 0.7\n",
    "adstock_theta = 0.3\n",
    "\n",
    "# Step 1: Adstock Transformation\n",
    "def adstock_transformation(spend, theta):\n",
    "    adstocked_spend = np.zeros_like(spend)\n",
    "    for t in range(len(spend)):\n",
    "        adstocked_spend[t] = spend[t]\n",
    "    if t > 0:\n",
    "        adstocked_spend[t] += adstocked_spend[t-1] * theta\n",
    "    return adstocked_spend\n",
    "\n",
    "adstocked_spend = adstock_transformation(spend, adstock_theta)\n",
    "\n",
    "# Step 2: Saturation Function\n",
    "def saturation_function(adstocked_spend, saturation_lambda):\n",
    "    saturated_spend = 1 - np.exp(-saturation_lambda * adstocked_spend)\n",
    "    return saturated_spend\n",
    "\n",
    "saturated_spend = saturation_function(adstocked_spend, saturation_lambda)\n",
    "\n",
    "# Step 3: Sales Contribution\n",
    "sales_contribution = beta * saturated_spend\n",
    "\n",
    "# Step 4: Total Sales\n",
    "total_sales = np.sum(sales_contribution)\n",
    "\n",
    "# Step 5: Media Impact Proportion\n",
    "media_impact_proportion = sales_contribution / total_sales\n",
    "\n",
    "# Step 6: Users Generated\n",
    "users_generated = impressions * media_impact_proportion\n",
    "\n",
    "# Step 7: Total Users Generated\n",
    "total_users_generated = np.sum(users_generated)\n",
    "\n",
    "# Output\n",
    "print(\"Spend:\", spend)\n",
    "print(\"Impressions:\", impressions)\n",
    "print(\"Adstocked Spend:\", adstocked_spend)\n",
    "print(\"Saturated Spend:\", saturated_spend)\n",
    "print(\"Sales Contribution:\", sales_contribution)\n",
    "print(\"Total Sales:\", total_sales)\n",
    "print(\"Media Impact Proportion:\", media_impact_proportion)\n",
    "print(\"Users Generated:\", users_generated)\n",
    "print(\"Total Users Generated:\", total_users_generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b4134f-1482-4cfe-8715-cd5d740de549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92fde0f3-94a3-423b-bab3-9457cd3cc7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Bayesian R^2"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "audience",
      "width": 194
     },
     {
      "breakBefore": false,
      "name": "quality_type",
      "width": 194
     },
     {
      "breakBefore": false,
      "name": "intercept_prior",
      "width": 403
     },
     {
      "breakBefore": false,
      "name": "gamma_control_prior",
      "width": 403
     },
     {
      "breakBefore": false,
      "name": "adstock_alpha_prior",
      "width": 298
     },
     {
      "breakBefore": false,
      "name": "saturation_beta_prior_sigma",
      "width": 298
     },
     {
      "breakBefore": false,
      "name": "sample_tuneups",
      "width": 194
     },
     {
      "breakBefore": false,
      "name": "sample_draws",
      "width": 194
     }
    ]
   },
   "notebookName": "mmm_cross_validation",
   "widgets": {
    "adstock_alpha_prior": {
     "currentValue": "contrib_imps_ratio",
     "nuid": "5bf1af6a-e2b3-4652-be6c-570ce793f2e6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "uniform",
      "label": null,
      "name": "adstock_alpha_prior",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "spend_imps_ratio",
        "contrib_imps_ratio",
        "uniform",
        "uniform_beta_3",
        "spend_efficiency_ratio"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "uniform",
      "label": null,
      "name": "adstock_alpha_prior",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "spend_imps_ratio",
        "contrib_imps_ratio",
        "uniform",
        "uniform_beta_3",
        "spend_efficiency_ratio"
       ]
      }
     }
    },
    "audience": {
     "currentValue": "new_winback",
     "nuid": "02f45e64-cdc0-4a37-909d-7939857efbe3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "new_winback",
      "label": null,
      "name": "audience",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "new_winback",
        "returning"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "new_winback",
      "label": null,
      "name": "audience",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "new_winback",
        "returning"
       ]
      }
     }
    },
    "chain": {
     "currentValue": "4",
     "nuid": "5a694747-668d-47c2-8b9f-14e8ca6cbbf1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "4",
      "label": null,
      "name": "chain",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "4",
      "label": null,
      "name": "chain",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "gamma_control_prior": {
     "currentValue": "dist=Normal_mu=0_sigma=.03_dims=control",
     "nuid": "e1da57f3-9992-4b97-80b2-45515f31e9c4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dist=Normal_mu=0_sigma=.05_dims=control",
      "label": null,
      "name": "gamma_control_prior",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dist=Normal_mu=0_sigma=.025_dims=control",
        "dist=Normal_mu=0_sigma=.03_dims=control",
        "dist=Normal_mu=0_sigma=.05_dims=control",
        "dist=Normal_mu=0_sigma=.1_dims=control"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dist=Normal_mu=0_sigma=.05_dims=control",
      "label": null,
      "name": "gamma_control_prior",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dist=Normal_mu=0_sigma=.025_dims=control",
        "dist=Normal_mu=0_sigma=.03_dims=control",
        "dist=Normal_mu=0_sigma=.05_dims=control",
        "dist=Normal_mu=0_sigma=.1_dims=control"
       ]
      }
     }
    },
    "intercept_prior": {
     "currentValue": "dist=TruncatedNormal_mu=0.2_sigma=0.1_lower=0.1",
     "nuid": "8470323a-473a-4fb5-9706-fb81218cae86",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dist=TruncatedNormal_mu=0.5_sigma=0.05_lower=0.4_upper=0.7",
      "label": null,
      "name": "intercept_prior",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dist=TruncatedNormal_mu=0.5_sigma=0.05_lower=0.4_upper=0.7",
        "dist=TruncatedNormal_mu=0.2_sigma=0.1_lower=0.1",
        "dist=TruncatedNormal_mu=0.3_sigma=0.1_lower=0.1",
        "dist=TruncatedNormal_mu=0.4_sigma=0.1_lower=0.1",
        "dist=TruncatedNormal_mu=0.5_sigma=0.1_lower=0.1",
        "dist=HalfNormal_sigma=1"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dist=TruncatedNormal_mu=0.5_sigma=0.05_lower=0.4_upper=0.7",
      "label": null,
      "name": "intercept_prior",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dist=TruncatedNormal_mu=0.5_sigma=0.05_lower=0.4_upper=0.7",
        "dist=TruncatedNormal_mu=0.2_sigma=0.1_lower=0.1",
        "dist=TruncatedNormal_mu=0.3_sigma=0.1_lower=0.1",
        "dist=TruncatedNormal_mu=0.4_sigma=0.1_lower=0.1",
        "dist=TruncatedNormal_mu=0.5_sigma=0.1_lower=0.1",
        "dist=HalfNormal_sigma=1"
       ]
      }
     }
    },
    "quality_type": {
     "currentValue": "DAU",
     "nuid": "b902097a-aced-4b0a-8d6c-0d156206f6cb",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "QDAU",
      "label": null,
      "name": "quality_type",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "QDAU",
        "DAU"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "QDAU",
      "label": null,
      "name": "quality_type",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "QDAU",
        "DAU"
       ]
      }
     }
    },
    "sample_draws": {
     "currentValue": "250",
     "nuid": "5426be00-0749-4a8b-8ae6-9c0858ce63ec",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "750",
      "label": null,
      "name": "sample_draws",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "750",
      "label": null,
      "name": "sample_draws",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "sample_tuneups": {
     "currentValue": "250",
     "nuid": "a65a02aa-735c-4d48-ba36-4c7f6c684f25",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "750",
      "label": null,
      "name": "sample_tuneups",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "750",
      "label": null,
      "name": "sample_tuneups",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "saturation_beta_prior_sigma": {
     "currentValue": "prior_spend_efficiency_sigma",
     "nuid": "44a963f3-e04b-4a87-88d5-5e856dccd5a6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "prior_imps_share_sigma",
      "label": null,
      "name": "saturation_beta_prior_sigma",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "prior_imps_share_sigma",
        "prior_spend_share_sigma",
        "prior_contribution_share_sigma",
        "prior_spend_efficiency_sigma"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "prior_imps_share_sigma",
      "label": null,
      "name": "saturation_beta_prior_sigma",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "prior_imps_share_sigma",
        "prior_spend_share_sigma",
        "prior_contribution_share_sigma",
        "prior_spend_efficiency_sigma"
       ]
      }
     }
    },
    "trend_type": {
     "currentValue": "month",
     "nuid": "f6fabaf3-7e94-4804-ac86-479baa42aeab",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "annual",
      "label": null,
      "name": "trend_type",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "month",
        "annual"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "annual",
      "label": null,
      "name": "trend_type",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "month",
        "annual"
       ]
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
